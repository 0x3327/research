[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "documents/research/posts/ERFC-57.gfm.html",
    "href": "documents/research/posts/ERFC-57.gfm.html",
    "title": "Solidity++ (S++)",
    "section": "",
    "text": "Executive Summary\nWriting efficient code in modern languages is mostly reflected in writing efficient algorithms and business logic. Writing efficient code in Solidity is mostly reflected in thinking as an assembler, moving focus from logic implementation to memory optimisations and low level hacks. Highly optimised code may become unreadable and unmaintainable with possible bugs hidden between the lines of bit level operations. To solve this problem, this project proposes multiple automatic optimisation techniques that will be all summed up into a transpiler and Solidity language extensions called Solidity++.\n\n\nIntroduction\nJavascript introduced classes in ES6 which are very useful construct for writing clean and readable code. Browser, however, could not understand ES6 code so the code needed to be transpiled into ES5. Programmers could use classes to write logic and transpiler took care of transforming the code into an optimised ES5. The same parallel could be made with Solidity. Programmers should be focused on writing business logic in Solidity-like S++ code and the code should be automatically transpiled into efficient Solidity code. S++ should not drastically modify basic Solidity syntax, but introduce new annotations and helper functions that would enable efficient transpiling to pure Solidity code.\nThere are multiple ways to improve the smart contract code on bytecode level. This project will focus on improvements on Solidity code level but the end product - optimised Solidity code will be compiled into bytecode and further optimisations on bytecode level can be performed.\nOptimisations on code level include variable declaration ordering (state variables, local variables and structs), which correlates with the order of memory block stacking and directly influences the costs of storing the data. Some variables might occupy more space than needed to store values in ranges that require less bits than the closest Solidity data type. The most extreme example is boolean data type which stores true/false values, that could be stored in 1 bit of memory, but really use an entire byte. Accessing storage structs requires more operations than accessing a local variable, which becomes obvious problem when it is done in loops. Any fixed sized data type, like, int64, bytes32 and so on, are always cheaper than dynamic data types, such as string, and those dynamic types should be replaced with fixed ones wherever is possible (wherever the value range of the variable is known). Cost reduction can even be achieved by deleting variables - freeing blockchain space. Execution of a code that performs delete command on some variable/mapping value/… rewards executor with a refund of up to 50% of transaction cost, depending on the amount of freed space. There were some attempts to remove this feature (https://eips.ethereum.org/EIPS/eip-3298) but so far it is still valid and exploitable. Usage of lazy evaluation is, an in other languages, preferred way of saving execution steps, which for Solidity directly equals saved money.\n\nExisting solutions\nThere are optimisations on loop level [1] that do not include variable level optimisations but recognise code patterns that are classified into several categories that can be automatically optimised. Also, there are papers [2] that uncover additional space for optimisation, such as removing conditions that always equal the same value or values in the loops that never change. There are also blog advices for writing optimised smart contracts, such as [3], that can and will be referenced when implementing automatisation methods.\n\n\n\nGoals & Methodology\nSo far, there is no tool or IDE that either automates these optimisations or includes multiple improvements in one software. The goal of this project is to evaluate feasibility of implementing such tool and Solidity language extensions that could allow its easier implementation. End PoC toll planned as a result of this project focuses on space optimisation techniques and includes: * Implementing parser for Solidity language that enables language semantic and syntax analysis * Implementing optimisation methods: * Variable declaration reordering * Struct values declaration reordering * Defining data type extensions for storing custom-sized data (i.e. integers with values between 20 and 40) and implementing language parser extensions * Implementing generator for optimised code\n\n\nResults & Discussion\nProto-parser for Solidity code, which includes basic PoC constructs was implemented, as well as variable declaration reordering to enable more efficient space usage; The plan is to extend the parser and to include other optimisation techniques listed in the previous segment.\n\nVariable declaration reordering\nVariable declaration reordering is a problem synonymous to binning problem, where the goal is to pack objects of a certain size into bins of fixed size in any order using the least number of bins. The problem is a known optimisation problem which comes from NP (non-deterministically polynomial) class and has no known efficient solution. The approach for implementing variable declaration reordering was using branch-and-bound algorithm which resulted in decent reordering time of less than 10 seconds for up to 16 variables in one block. The reordering was performed per block. The same algorithm will be performed on structs.\n\n\nData type extensions\nTo enable extensions of data types there are two steps: * Defining language extensions * Map variables to bits of memory blocks and create getters and setters\nNew data types will be mapped to memory blocks, variables of 256 bits, on bit level using bit masks. Packing of those values will be optimised using the previously defined variable stacking algorithm\nProposed language extension of integer data types consists of using the existing base type (int, uint) followed by value range given in brackets. Example custom type uint<10, 1030> represents unsigned integer values between 12 and 98. The optimiser would determine the closest known data type that covers the range (in this case uint16). The real number of bits required to store value 1030 is 11 and the closest data type is 16 bits long which means that 5 bits are redundant. First level of optimisation may be using 11 bits mapped to a memory block of 256 bits and implementing getters and setters to interact with the value, which is casted to uint16 when used. But there is another hidden optimisation that can further improve space management. Notice that the upper limit of the interval is indeed 1030 which required 11 bits to store but the lower limit is 10 which means that total number of values is 1030 - 10 = 1020 and that many values can be stored in 10 bits of memory, so the optimisation will take into account the overall interval range and generate getters / setters that will include offsets before storing/loading values.\nBoolean type optimisation will be straight forward and, while it will keep the type name, it will be mapped to only one bit of memory.\nString data types would be extended with indicator of the number of characters as string<10> will represent fixed sized string that will be mapped to bytes.\n\n\nCode generation\nThe parser generates three-like structure of the Solidity code. Every statement or declaration is stored as an object with parameters related to the nature of the statement/declaration. For example, declaration uint256 private var1 is stored as VariableDeclaration object with attributes type=uint256, modifier=private identifier=var1. Each object contains information to reconstruct itself back to string from. This type of organisation enables easy traversal through code, easy recognition of higher-level constructs, such are infinite loops, and easy reconstruction to Solidity code (in this case - optimised code).\n\n\n\nConclusion\nThe feasibility of the proposed project is confirmed by implementing PoC prototype, the optimisation evaluation on the existing smart contracts is yet to come but even the simple reordering of variables that reduces the number of used memory blocks clearly shows benefits of money savings (saved memory) and time savings (eliminated thinking about such generic problem that can and should be automatised). The borderline is - it makes no harm to use it, it can only be an improvement.\nThe value of this project is assumed, but still has to be confirmed. That is why the next step of this research should be further assessment by target user group, which should include developers that are actively using Solidity in their everyday work and domain experts. # Appendices\n\n\nBibliography\n[1] B Mariano, Y. Chen, Y. Feng Demystifying Loops in Smart Contracts https://fredfeng.github.io/papers/ase20-consul.pdf\n[2] T.Brandstaetter Optimisation of Solidity Smart Contracts https://repositum.tuwien.at/bitstream/20.500.12708/1428/2/Brandstaetter%20Tamara%20-%202020%20-%20Optimization%20of%20solidity%20smart%20contracts.pdf\n[3] https://mudit.blog/solidity-gas-optimization-tips/"
  },
  {
    "objectID": "documents/research/posts/ERFC-38.gfm.html#executive-summary",
    "href": "documents/research/posts/ERFC-38.gfm.html#executive-summary",
    "title": "Building on Filecoin and Filecoin Proofs",
    "section": "Executive Summary",
    "text": "Executive Summary\nWhen looking to build dApps that utilize decentralized storage Filecoin seems like the best option even with its flaws like : absent proof of deletion for the client, absent encryption, impossible modifying of the stored data and the durability problem of Filecoin’s Proof of Replication. This research gives an overview of competitors in decentralized storage solution field with a focus on Filecoin protocol. It also shows how its competitor Storj differs from Filecoin and the current grant opportunities for potential building on it. Considering the results of the research Filecoin seems to be the best option considering the popularity and the size of its ecosystem. Currently there is no interest for building on and with Filecoin and this is a purely explorative work without experiments, in order to test the researcher’s methodology and the approach to research. However, it proposes a question about a potential way of improving Filecoin, or creating both safer protocol for the user and cheaper for the storage miners."
  },
  {
    "objectID": "documents/research/posts/ERFC-38.gfm.html#goals-methodology",
    "href": "documents/research/posts/ERFC-38.gfm.html#goals-methodology",
    "title": "Building on Filecoin and Filecoin Proofs",
    "section": "Goals & Methodology",
    "text": "Goals & Methodology\nThe aim of this research is to explore Filecoin protocol and show its fallacies as much as writer is possible with a goal of inspiring building applications that utilize Filecoin, new protocols or improving Filecoin via building for it. The paper will compile the list of all fallacies and possibilities for improvement for Filecoin, opportunities for building on it, and the current state of decentralized storage market mainly comparing Storj and Filecoin."
  },
  {
    "objectID": "documents/research/posts/ERFC-38.gfm.html#introduction",
    "href": "documents/research/posts/ERFC-38.gfm.html#introduction",
    "title": "Building on Filecoin and Filecoin Proofs",
    "section": "Introduction",
    "text": "Introduction\nUnlike a centralized server operated by a single company, decentralized storage systems consist of a peer-to-peer network of user-operators who hold a portion of the overall data. The platforms can store any data sent by the user, with some platforms more focusing on encryption. There is no official data disclosing the types of data stored. This research will be covering contract-based persistence platforms with a focus on Filecoin.\nContract-based persistence means that data cannot be replicated by every node and stored forever, and instead must be upkept with contract agreements. These are agreements made with multiple nodes that have promised to hold a piece of data for a period of time. They must be refunded or renewed whenever they run out to keep the data persisted. Platforms with contract-based persistence currently present on the market are:\n\nFilecoin\nSkynet\nStorj\n0Chain\n\nFilecoin is a peer-to-peer network that stores files with built in economic incentives to ensure files are stored reliably over time. Users pay to store their files on storage providers. Storage providers are computers responsible for storing files and proving they have stored the files correctly over time. Available storage and the price of it is not controlled by any company. Anyone who wants to store their files or get paid for storing other users files can join Filecoin is written in its documentation, but is that really the case? It will be further explored in later sections. Filecoin’s native currency is FIL. Storage providers earn units of FIL for storing user’s data. Its blockchain records transactions along with proofs from storage providers that they are storing files correctly.\nCurrently Filecoin stores over 40.0453 PiB of users data over 1,848,292 deals.1\nWhen users want to store their files on Filecoin they use terminal or different guis that have been built by developers to choose between cost, redundancy and speed and they select the storage provider whose storage offer is best suited for their needs. Applications that implement filecoin negotiate storage with storage providers. There is no need for different API for each provider.2\n“While interacting with IPFS does not require using Filecoin, all Filecoin nodes are IPFS nodes under the hood, and (with some manual configuration) can connect to and fetch IPLD-formatted data from other IPFS nodes using libp2p. However, Filecoin nodes don’t join or participate in the public IPFS DHT. IPFS alone does not include a built-in mechanism to incentivize the storage of data for other people. This is the challenge Filecoin aims to solve. Filecoin is built on IPFS to create a distributed storage marketplace for long-term storage.”3"
  },
  {
    "objectID": "documents/research/posts/ERFC-38.gfm.html#filecoins-proof-system",
    "href": "documents/research/posts/ERFC-38.gfm.html#filecoins-proof-system",
    "title": "Building on Filecoin and Filecoin Proofs",
    "section": "Filecoin’s Proof System",
    "text": "Filecoin’s Proof System\nFilecoin uses Proof of Replication (PoRep) and Proof of Spacetime (poSt).\nIn a Proof of Replication, a storage miner proves that they are storing a physically unique copy, or replica, of the data. Proof of Replication happens just once, at the time the data is first stored by the miner. As the storage miner receives each piece of client data they place it into a sector, fundamental unit of storage in Filecoin. Sectors can contain pieces from multiple deals and clients. Steps in PoRep:\n\nProof of Replication\n\nFilling sectors and generating the Commd\n\nOnce a sector is full a Commitment of Data (CommD) is created, representing the root node of all the piece CIDs contained in the sector.\n\nSealing sectors and producing the Commitment of Replication\nSector data is encoded through a sequence of graph and hashing processes to create a unique replica. The root hash of the merkle tree of the resulting replica is called CommRLast. CommRLast is then hashed together with the CommC(another merkle root output from PoRep). This generates the CommR (Commitment of Replication) which is then recorded on Filecoin’s Blockchain. CommR, last is saved privately by the miner for future use in Proof of Spacetime but is not saved to the chain.\nEncoding process is slow and computationally heavy. Filecoin doesn’t provide encryption by default so users must encrypt data before adding it to the Filecoin network. This is the first issue encountered with Filecoin : Filecoin is optimized for public data and doesn’t yet support access controls.\nThe CommR offers clients the proof that the miner is storing a physically unique copy of the client’s data. If a client stores the same data with multiple storage miners, or makes multiple storage deals for the same data with the same miner, each deal will yield a different CommR. The sealing process also compresses the Proof of Replication using zk-SNARKs to keep the chain smaller so that it can be stored by all members of the Filecoin network for verification purposes.\nUnlike PoRep which is run once to prove that a miner stored a physically unique copy of the data at the time the sector was sealed. PoSt is run repeteadly to prove that the miners are continuing to dedicate storage space to that same data over time.4\nProof of Spacetime\nPoSt builds on several elements created during PoRep: the replica, the private CommRLast and public Commr. PoSt then selects some leaf nodes of the encoded replica and runs merkle inclusion proofs on them to prove that the miner has the specific bytes that indicate that he still holds the clients data. The miner then uses the privately stored CommRLast to prove that they know of a root for the replica which both agrees with the inclusion proofs and can be used to derive the CommR. As the final step PoSt compresses these proofs into a zk-Snark.\nIf the miners fail the Proof of Spacetime at any point they will lose their staked collateral. Aside for this fine, there is no other incentive to keep the miners storing the data. That becomes a problem if client’s storing private data or data of great significance.5"
  },
  {
    "objectID": "documents/research/posts/ERFC-38.gfm.html#results-discussion",
    "href": "documents/research/posts/ERFC-38.gfm.html#results-discussion",
    "title": "Building on Filecoin and Filecoin Proofs",
    "section": "Results & Discussion",
    "text": "Results & Discussion\n\nCritique From the users perspective:\nAs of today the price of 1 Filecoin token (FIL) is $22.05. The users can store a gigabyte of data for as little as 0.0000006157315278020465 FIL (0.01% the cost of Amazon S3) which means the user can store 100GB of data for $0.00135.6 From the price perspective the upside of Filecoin is it’s cheap storage but there are various downsides such as:\n\nAccesibility: if the user is not tech-savvy there is a big barrier to entry even with GUIs currently available. They are often not simple to install and it is hard to get them to work. Hovewer there are various Apps that make that somewhat easier like ChainSafe Files and Web3.Storage.When it comes to being a storage miner there minimal needs for hardware are:\n\n\n8+ core CPU\n128 GiB of RAM atleast\nA strong GPU for SNARK computations\n1TiB NVMe-based disk space for cache storage is recommended\nThis data shows that there needs to be a significant investment on storage miners part which is nothing out of the ordinary but significantly reduces the accessibility to the average person, ofcourse assuming the person wants to be a storage miner.\n\n\nAs the price of FIL tokens fluctuate the price of storage fluctuates as well. There is also a risk if any extra FIL is left in the customers wallet after the storage contract then token could potentially drop/rise in value, not to mention the fees of converting fiat into cryptocurrency.\nThere is no built in encryption. Users need to encrypt their data on their own. Encryption/decryption of files cost compute resources (RAM, CPU), and therefore money. Most end users would prefer the implementation of this functionality to be handled, optionally, by the Filecoin web, cli or desktop client software they choose to make use of. This problem is seemingly taken care of with apps like ChainSafe Files.7\nChainSafe Files is an online platform to store, view, and share files. Its main focus is data privacy of the users and self-reliance. When it comes to self-reliance ChainSafe Files makes sure that the users can access the stored files even if the Files platform becomes unavailable. It also offers authentication flow using a decentralized login provider called tKey, by Torus. tKey is a private key generator that can link keys to social accounts among other functions. The Files’ backend is built on top of ChainSafe Storage, and any file that is uploaded to Files is also pinned by a node on its infrastructure (each file has an CID) thus making the data retrieval possible even in the case of ChainSafe files app outage. In the case of retrieval users still have to decrypt the data, since all the data stored by ChainSafe Files is encrypted by default.8\nIf the storage provider doesn’t respect his end of the deal he will be penalized and lose his staked FIL. Unless negotiating a great number of deals for the same data and storing a lot of copies in Filecoin, there is no guarantee that the data will be safely stored. These deals for the same data increase the cost of the service if the user wants to have somewhat durable data. Filecoin tries to mittigate this by having storage miners put 100+ FIL in collateral which also lowers the accessibility to the average person to become a storage miner. Currently most of the storage miners are located in China.\nFilecoin storage is cold storage. There is no way to modify data. If the user needs to change data , new data must be written.9\nIf the user issues a deletion command there is no guarantee that the client performs the operation. There is no way yet to Construct a formal Proof of Deletion.\n\n\n\nThe issue with Replication and Filecoin\nWhen decentralized storage network is utilized any storage node could go offline thus the stored data would be at risk of getting lost. To achieve a somewhat reliable storage many decentralized providers use replication, which means the only way to keep the users data reliably besides penalizing storage miners is to store multiple copies of it. Replication is not good for the network expansion factor. If Filecoin wants more durability for its data it needs more copies. For every increase of durability (storing or repairing the data) another multiple of the data size in bandwith is needed. Eg. If the durability level requires a replication strategy that makes 10 copies of the data this yields and expansion factor of 1000%. This data needs to be stored on the network, using bandwith in the process. The more replication the bigger the bandwith usage. Hovewer, if the node goes offline , only one of the storage nodes is needed to bring a new replacement node in, which again means that the 100% of the replicated data must be transferred. Excessive expansion factors produce an inefficient allocation of resources.10\nAnother issue with replication is churn (nodes joining and leaving the network). Quoting Patrick Gerbes and John Gleeson: “Using replication in a high-churn environment is not only impractical, but inevitably doomed to fail. Distributed storage systems have mechanisms to repair data by replacing the pieces that become unavailable due to node churn. However, in distributed cloud storage systems, file repair incurs a cost for the bandwidth utilized during the repair process. Regardless of whether file pieces are simply replicated, or whether erasure coding is used to recreate missing pieces, the file repair process requires pieces to be downloaded from available nodes and uploaded to other uncorrelated and available nodes.”11\nCurrently the circulating supply of FIL token is 162,302,978.00 FIL. The potential circulating FIL could reach 1.977 Billion tokens if the network hits a Yottabyte of storage capacity in under 20 years which is brave considering the current data stored in data centers today is less then a Zettabyte and a Yottabite is 1000 times larger. The 770 Million of which is for baseline minting.\n330 million FIL tokens are released on a 6 year half-life based on time. A 6 year half-life means that 97% of these tokens will be released in aproximately 30 years. This amount is amount is minted to provide counter pressure to shocks.\nAnother 300 million FIL is held back in reserve to incentivize future types of mining. How they are released is up to the Filecoin community.12\n\nFigure 1: Maximum and Minimum Minting from Storage Mining.\n\nFigure 2: Network Storage Baseline for Max Baseline Minting on Log Scale.\nAs of today 29,180,207.338966275 FIL has been slashed.This means that if a network participant misbehaves, part of their FIL collateral or potential FIL rewards is confiscated and burned. FIL is also slashed for various other reasons.\nConsidering the token release we can expect total supply of FIL token to be almost doubled in the next five years. If we look the rate of slashing so far the dilluting process will exceed the tokens burned. This makes specullating Filecoin token price risky, both for the investors and users.13"
  },
  {
    "objectID": "documents/research/posts/ERFC-38.gfm.html#filecoin-and-storj-comparission",
    "href": "documents/research/posts/ERFC-38.gfm.html#filecoin-and-storj-comparission",
    "title": "Building on Filecoin and Filecoin Proofs",
    "section": "Filecoin and Storj comparission",
    "text": "Filecoin and Storj comparission\nWe will compare Filecoin and Storj by: 1. Consensus Algorithms\nFilecoin’s consesus mechanism has been covered in the paragraphs above.\nStorj does not have its own chain: the platform is built on Ethereum(currently built on PoW). The reason they decided to build on the Ethereum network is because of simplicity of using it as a method exchange. From the start Storj was never intented to be a true decentralized storage network.\n\nBlock time\nFilecoin’s block time is thirty seconds on average.\nStorj is the only “decentralized storage” solution to not employ their own chain. Storj decided to use the Ethereum network due to the easy deployment of a coin on it. Because of this decision, the block time is twelve seconds and needs to deal with the consequences of sharing blockchain with other projects.\nEnforcement of data retention\nDue to the PoSt mentioned in the paragraphs above the entire network is designed around data retention. In the case of miner failing to keep his promise , the only backup of the user’s data was irreversibly lost since Filecoin doesn’t use redundancy which is also mentioned in the paragraphs above.\nIn Storj the actual enforcement of data isn’t clearly documented. Each Satellite (the interface between the storage operators and the clients) does the enforcement of data retention all on their own. Each Satellite has its own subset of storage nodes that it knows have a good reputation and it trusts, it uses these hosts to upload data to. Then it, at regular intervals, checks random data segments with “Berlekamp-Welch” error correction to make sure that the data is still there. If they fail to prove they store the data the reputation of that host is changed and data migrates to a new host. There is no chain-level enforcement for data retention.\nContent distribution\nIn Filecoin data has to be sealed to be counted as a provable storage to the chain and because it is computation heavy it isn’t practical for the miners to serve data. 1 MiB file can take 5 to 10 minutes to unseal and 32 GiB file takes 3 hours on minimum hardware requirements mentioned above. To battle this Filecoin introduced a method to store cached and unsealed versions of data while storing the same data as sealed in order to provide proofs. This leads to the issue of miner storing the double amount of data while also posing the problem that unless the data is frequently accessed the miners will not store it because it isn’t profitable for them. That makes creating Google drive equivalent on Filecoin not practical because the data is not frequent enough to makes sense to cache while also being low latency (because of slow sealing and unsealing).\nIn Storj’s case data is being accessible only through the S3 gateway of centralized data data Satellites. Users can transform any data to public data and can send anyone a link to that data.\nSector size\nSector size is the minimum amount of data that can be sent to host and paid for. Filecoin has a fixed 32 GiB sector size. For each 256b stored 2b are proofs. Which means 1% of storage paid for is for proof. Also everything sent must be in a .car file which can be computationally heavy on the client-side.\nIn Storj’s case the sector size is not really clear. Current object fee is $0.0000022 per file stored. Which means that if there is a large amount of small files stored that would bring extra costs to the user.\nDecentralization\nBecause of the Filecoin’s Hardware requirements for hosts that means not everyone can run a storage node. On the other networks basically anyone can run a storage node( most minimal requirements are 2 cores 8GB of RAM and some storage) but in Filecoin only people with sufficient funds for initial investments can run hosts which reduces the spread of the network. Which makes us question the actuall decentralization of the network.\nStorj isn’t decentralized. The blockchain nature of the Storj coin is only designed for for efficient transacting not decentralized enforcement of data retention which means that Storj is actually a distributed storage provider but not fully decentralized.14"
  },
  {
    "objectID": "documents/research/posts/ERFC-38.gfm.html#filecoin-grants",
    "href": "documents/research/posts/ERFC-38.gfm.html#filecoin-grants",
    "title": "Building on Filecoin and Filecoin Proofs",
    "section": "Filecoin Grants",
    "text": "Filecoin Grants\nFilecoin offers various types of grants for building with Filecoin.\n\nNext Step Microgrants\nFilecoin offers grants of 5000$ in FIL to support taking the next step when the initial prototype is created. Their purpose is financing projects in the early stage. Acceptance criteria is simple. Projects must meet these criteria:\n\nApplicant has already built something with Filecoin (or closely related technologies such as IPLD, libp2p, or frameworks or services such as NFT.storage, Textile Powergate, etc.), independently or as part of a course or hackathon.\nApplicant must provide clear description of the Next Step after grant support\nThe project can be completed within 3 months.\nProject must be open-sourced.\nThe applicant must complete weekly updates and a grant report upon conclusion.\n\nProjects that qualify for Microgrants: 1. Projects that publish data or files to IPFS 2. Projects that don’t use IPFS directly 3. Projects that save data or retrieve data from the Filecoin Network 4. Non-coding projects, videos, tutorials etc\nThese grants are offered on the quarterly basis.\n\n\nOpen Grants\nFilecoin’s focus areas currently are: 1. Core development - core protocol research, specification and implementation work 2. Application Development - applications that utilize Filecoin as a decentralized storage layer 3. Developer tools and libraries - tools and libraries for protocol developers and application developers 4. Integration and adoption - integration into existing app or projects with significant usage 5. Technical design - improvement proposals for the core storage protocol 6. Documentation 7. Community building 8. Metaverse - experiences, applications, communities, tooling, standards, infrastructure, et cetera15 9. Research that explores Filecoin and decentralized storage16"
  },
  {
    "objectID": "documents/research/posts/ERFC-38.gfm.html#conclusion",
    "href": "documents/research/posts/ERFC-38.gfm.html#conclusion",
    "title": "Building on Filecoin and Filecoin Proofs",
    "section": "Conclusion",
    "text": "Conclusion\nWhen it comes to decentralized storage solutions Filecoin draws all the attention compared to its competitors like Storj and Sia. Storj solves Filecoin’s replication problem, on the expense of decentralization but the internet search statistics still shows that Filecoin is the main contender in Decentralized storage.\n\nFigure 3. Filecoin (blue) vs Storj (red) search interest in the past 12 months.\nEven with unclear tokenomics a nd problematic durability to the writer of this research, building with Filecoin seems like the best solution when building dApps that utilize decentralized storage because of its low cost and grant opportunities, even though storage miners are somewhat centralized. If there is a way to improve current storage miner’s/clients experience on Filecoin both in cost and ease of use that would surely be a great grant opportunity. Also there could potentially be a way to draw them to a protocol that is a cheaper alternative. Ofcourse, these are all assumptions, since much of the data on these protocol is unclear and not available."
  },
  {
    "objectID": "documents/research/posts/ERFC-37.gfm.html#ronin-sidechain",
    "href": "documents/research/posts/ERFC-37.gfm.html#ronin-sidechain",
    "title": "Gaming DAPPs - Play to Earn model",
    "section": "Ronin sidechain",
    "text": "Ronin sidechain\nThe game was developed on Ethereum in 2018 by the Sky Mavis company located in Vietnam. Due to high Ethereum fees, the game’s creators moved to their own EVM compatible sidechain called Ronin which uses Proof of Authority with validators being chosen by the company5 .\nTo start playing a player needs to :\n\ncreate their own Ronin wallet\ncreate their user account on the Axie Marketplace and connect their wallet to it\ntransfer some amount of ETH and buy at least 3 Axies with a floor price of ~42 dollars per Axie\ninstall a PC or Mobile app and login with their user account\n\nDisregarding the difficulty of the onboarding process, the whole point of a blockchain game is to have the players take actions on the blockchain. Some arguments could be made that at the time the game was developed this was needed but with the introduction of Ronin it is unclear why the whole game was not ported to it.\nThe only actions that are taken on the Ronin sidechain are dedicated to trading - buying, minting and gifting of Axies. There are no fees on Ronin, but the number of transactions per wallet address per day is limited. Deploying on Ronin, also, requires company’s permission so the development of new games and the whole Ronin ecosystem is slowed down."
  },
  {
    "objectID": "documents/research/posts/ERFC-37.gfm.html#scholarships",
    "href": "documents/research/posts/ERFC-37.gfm.html#scholarships",
    "title": "Gaming DAPPs - Play to Earn model",
    "section": "Scholarships",
    "text": "Scholarships\nMarket decides on the price of Axies and with a high entry cost of starting with the game, a new model of onboarding has emerged. Newcomers (scholars) can “rent out” the Axies for a certain period of time and negotiate with the owners (managers) the terms of the profit distribution. This is not a official in-game feature and is enabled by having the managers controlling the Ronin wallet and scholars controlling the user account associated with it. This leads to a bad position for the scholar as the profits are claimed by the manager and then the scholar’s share is sent to the their wallet’s address. Scholars are essentially at the mercy of the managers as their earnings and the scholarship itself can be revoked at any time.\nMore fairer way of enabling Scholarships, would be to have a smart contract that would have complete control of the Axies and through which the scholars would make in-game actions. The profits would go to the contract’s address that would perform a fair split. The terms of the agreement (the minimum amount of profits to be earned by the scholar) and the scholarship’s time period would be embedded in the contract. If both parties agree, the terms could be changed later on."
  },
  {
    "objectID": "documents/research/posts/ERFC-37.gfm.html#game-breakdown",
    "href": "documents/research/posts/ERFC-37.gfm.html#game-breakdown",
    "title": "Gaming DAPPs - Play to Earn model",
    "section": "Game Breakdown",
    "text": "Game Breakdown\nThe game itself is organized in 1v1 matches where players do not have any influence against who they will get matched. Matching is done by the server based on players’ Match Making Rankings (MMR) which is determined by the win/lose ratio of those players. All of this could be implemented in a smart contract which would keep track of all of the players’ MMRs and update them after each match. Players could start a new match or join an existing one if the absolute difference in their MMRs is under a certain threshold. The benefits of this approach is that they could also choose what match they will join or challenge a specific player.\nWinner of each match gets some amount of game’s “Smooth Love Potion” (SLP) tokens. That amount is dependent on the MMR of that player (the higher the MMR the more SLP tokens they will win). SLP is an inflationary ERC20 token that gets minted after each match. Even though there are SLP burning mechanisms through some in-game actions, most players opt to cash out their winnings so this might not be a viable economic model. One alternative would be to have both players stake some amount of tokens in a match with the winner taking the sum of those stakes. The problem is that it introduces betting connotations and games go to extreme lengths in order to not be considered a betting game as it potentially introduces regulation.\nInside the game, there is also “Axie Infinity Shard” (AXS) token. AXS is an ERC20 token which has a fixed total supply. The company behind the game has roughly 20% of the total AXS supply and small amounts of AXS is distributed to the top players of the month.\nThe game makes heavy use of Axies which have certain characteristics. Players do not completely own Axies as it was discovered that the company can freeze them, making them useless. Once an Axie is frozen, it cannot be used in the game nor it can be traded on the Axie Marketplace. This possess a major concern as the players’ assets are constantly under a threat of those players being banned from the game by the Sky Mavis company. Full list of violations that will result in a ban can be seen here6 .\nEach Axie has one of the 9 classes, 4 statistics and 4 cards associated with it. Classes are grouped into three groups that form a “rock paper scissor” relationship. Meaning that, group G1 does 15% extra damage to group G2 but takes 15% extra damage when attacked by group G3. Statistics determine the Health, Skill, Speed and Morale of an Axie. This statistics affect the matches and their state transitions as the Speed for example determines the order of attacks. Cards can have positive or negative effects on an Axie as well additional effects that affect the players. Four cards associated with an Axie are added to the player’s deck when that Axie is used inside a match. More information about Axies is provided in Appendix A and the way new Axies are created is provided in the Breeding section of the Whitepaper7 ."
  },
  {
    "objectID": "documents/research/posts/ERFC-37.gfm.html#matches",
    "href": "documents/research/posts/ERFC-37.gfm.html#matches",
    "title": "Gaming DAPPs - Play to Earn model",
    "section": "Matches",
    "text": "Matches\nOne match consists out of:\n\nBoth players choosing a team of 3 Axies and deciding on their positions on the field (creating team formations)\nRounds being played out until one of the players doesn’t have a standing Axie\n\n\n\n\n\nAxieMatch\n\n\nPicture 1 : Axie Infinity Match View\n\n\nTeam formations\nThe positioning of an Axie matters because it determines what Axie will take the damage from the enemy. Each player has 5 rows where an Axie can be positioned. The closest row to the enemy lines will be attacked first. The Axies cannot change their position and they stay where they initially were until they get knocked out. Choosing of a team formation would require two transactions per player. Those transactions would be organized in the commit-reveal scheme so that the players wouldn’t have an advantage against the opponent that naively sent the transaction revealing their team and their positions.\n\n\nRounds\nEach Round is carried out in the following order:\n\nPlayers randomly draw 3 cards* from their own decks (consisting of 24 Cards**) and decide on what cards they will play\nCards are revealed and their affects are applied to Axies\nBattle of the Round takes place\nIf one of the players doesn’t have a standing Axie then the match is over\n\n* Exception is that in the first Round, players draw 6 cards.\n** There are 3 Axies per team, each Axie adds two copies for each of the 4 cards. So in total there are 3*2*4 = 24 cards in one player’s deck.\n\n\nDrawing of Cards\nOnce the teams have been revealed both players know each other decks. However, they should not know what cards the opponent has in their hands and so they should not know what cards they have yet to draw.\nOne scheme that could be applied is the following:\n\nHave both players choose their secret “random” number and commit to it with a hash\nHave the game’s contract ask for a random number from Chainlink’s VRF8\nOnce the randomness has been fulfilled, both players know what is the order of cards they will be drawing as the seed for shuffling their decks will be some function of their own number and the received number from Chainlink’s VRF\n\nThe drawing order of cards in the deck is now known only to the players. During the match, each player can play any card inside their deck and until a match ends, players trust each other that the played card was in the opponent’s hand. However, when the match ends both players would need to reveal their secret numbers as the contract needs to verify if they honored the drawing order and if the card they played at a certain moment was one of the cards they were holding. The first discrepancy would end the verification process and the cheater would be penalized.\nOne additional subproblem is that after each round the remaining cards inside the deck should be reshuffled. This could be done with requesting another random number which would help form a seed for random shuffling of the remaining cards. The verification process would need to be modified to support this.\nNote: after all of the cards have been drawn, the deck resets.\n\n\nReveal of the Cards\nThe reveal of the cards would also need two transactions per player so that one player wouldn’t just wait for the opponent to reveal their cards and then change their strategy accordingly.\nOnce cards have been revealed, before the battle takes place, the effects of the played Cards are applied. In total there are 3 constant effects (Attack damage, Defensive points and the cost of playing that Card) and 19 additional effects that a Card may have. For example, one of those additional effects are an increase/decrease in one of the Axie statistics. When multiple Cards that affect the same Axie are played, their effects are accumulated. More information about cards is provided in Appendix A .\n\n\nBattle of the Round\nBattle rules:\n\nthe order of attacks is determined by the highest Speed statistic (if there is a draw then it is decided by the lowest Health and then by lowest ID of an Axie)\nclosest row with a standing Axie to the enemy lines will take the damage\nif two Axies are inside the row taking the damage there’s a 50/50 chance which Axie will absorb the damage\n\nAll of this logic could be kept inside a contract. One of that things that would need to be taken into account is that Axies attack in the pre-defined order. So during a Round, an Axie that has not yet attacked could be knocked out of the game and so the Cards associated with it, that were played in the Round, should lose their effects. Also, when an Axie gets knocked out, all of their cards should be removed from the deck."
  },
  {
    "objectID": "documents/research/posts/ERFC-37.gfm.html#summary-of-proposed-matched-progression",
    "href": "documents/research/posts/ERFC-37.gfm.html#summary-of-proposed-matched-progression",
    "title": "Gaming DAPPs - Play to Earn model",
    "section": "Summary of proposed matched progression",
    "text": "Summary of proposed matched progression\n\nInital proposal\n\n\n\n\n\n\n\n\n\n\n\nStep\nPhase\nDescription\nNumber of transactions\nDone by\n\n\n\n\n1\nMatch Set Up\nStarting of a Match\n1\nPlayer 1\n\n\n2\nMatch Set Up\nJoining a Match\n1\nPlayer 2\n\n\n3\nMatch Set Up\nCommitting to a team formation and setting of a secret number\n2*1\nPlayer 1 and Player 2\n\n\n4\nMatch Set Up\nRevealing team formations\n2*1\nPlayer 1 and Player 2\n\n\n5\nRound\nCommitting to Cards that will be played in this Round\n2*1\nPlayer 1 and Player 2\n\n\n6\nRound\nRevealing Cards of the Round\n2*1\nPlayer 1 and Player 2\n\n\n7\nMatch Wrap Up\nRevealing secret numbers after the match has ended\n2*1\nPlayer 1 and Player 2\n\n\n\n\nTotal = (1+1+2+2) + N*(2+2) + (2) = 8 + 4*N ; where N is the number of rounds in the match\nEven with just 3 rounds the number of transactions per player would be 16, so this is a problem.\n\n\nImproved Match Progression\nSomething that could be done is the introduction of “fair play” where for one round’s commit stage, one player would transmit both of the commit messages to the chain after which the other player would transmit both reveal messages.\nIf the player transmitting commitment messages decides to not transmit them, then the match is stuck. However, the game can be structured in a way such that it is in the interest of both players to resolve the match regardless of the outcome. This could be done by staking some amount of tokens that the players will receive at the end of the match.\nIf the player transmitting the reveal messages realizes he will lose and decides to not transmit them, then the first player could transmit just their own reveal message that would start a countdown for the other player to transmit their reveal message. If the countdown is reached then the player that didn’t act in the spirit of fair play is penalized in some way.\n\n\n\n\n\n\n\n\n\n\n\nStep\nPhase\nDescription\nNumber of transactions\nDone by\n\n\n\n\n1\nMatch Set Up\nStarting of a Match\n1\nPlayer 1\n\n\n2\nMatch Set Up\nJoining a Match\n1\nPlayer 2\n\n\n3\nMatch Set Up\nTransmitting commitments to the team formations and setting of a secret number\n1\nPlayer 1\n\n\n4\nMatch Set Up\nTransmitting the Reveal of team formations\n1\nPlayer 2\n\n\n5\nRound\nTransmitting Commitment Messages\n1\nPlayer 1\n\n\n6\nRound\nTransmitting Reveal Messages\n1\nPlayer 2\n\n\n7\nMatch Wrap Up\nRevealing secret numbers after the match has ended\n1\nPlayer 1\n\n\n\n\nThis scheme would give the total number of transactions of:\nTotal = (1+1+2) + N*(2) + (1) = 5 + 2*N ; where N is still the number of rounds in the match\nWith the number of rounds equal to 3, the number of transactions per player is less or equal to 6.\nThis is an improvement but maybe it could be brought down further. Also, signing 6 transactions per match would still break the flow of the game, so the players would have to have a substantial financial interest in continuing to play the current match and the game itself."
  },
  {
    "objectID": "documents/research/posts/ERFC-37.gfm.html#appendix-a",
    "href": "documents/research/posts/ERFC-37.gfm.html#appendix-a",
    "title": "Gaming DAPPs - Play to Earn model",
    "section": "Appendix A",
    "text": "Appendix A\nEach Axie has a fixed set of attributes - a class, 4 stats and 4 cards.\n\n\n\n\nAxie\n\n\nPicture 2: Axie characteristics\n\n\nAxie Classes\n\n9 of them (Plant, Reptile, Dusk, Beast, Bug, Mech, Aqua, Bird, Dawn)\n6 main classes (Plant, Beast, Bug, Reptile, Aqua, Bird, Dawn)\n3 remaining are “secret” (and generally weaker in the game)\nto make the strengths of classes balanced, they are grouped in 3 groups, forming a “rock paper scissor” relationship\n\n\n\nAxie Stats\n\ncan be divided into “base” and “additional” Stats\ntotal of 140 points is distributed between 4 of the base Stats (with each class having its own base distribution)\n\nHealth — amount of damage Axie can take before getting knocked out\nSpeed — affects the order in which Axies attack in a match (higher Speeds attack first)\nSkill — increases damage dealt when the Axie performs multiple cards/moves (a.k.a. combo)\nMorale — increases chance to land a critical hit, as well as entering “last stand” which allows them to attack a few more times before getting knocked out\n\n\n\nBase Stats Distribution\n\n\n\nClass\nHealth\nSpeed\nSkill\nMorale\n\n\n\n\nAqua\n39\n39\n35\n27\n\n\nBeast\n31\n35\n31\n43\n\n\nBirds\n27\n43\n35\n35\n\n\nBug\n35\n31\n35\n39\n\n\nPlant\n61\n31\n31\n41\n\n\nReptile\n39\n35\n31\n35\n\n\n\n\n\nadditional Stats depend on Axie’s Body Parts\n\n\n\nAxie Body Parts\n\nthere are 6 of them (Eyes, Ears, Horns, Mouth, Back, Tail)\nonly Horns, Mouth, Back, Tail have an associated card with it (one card per body part)\nIn total, there are:\n\n4*6 types of Mouth\n6*6 types of Horns\n6*6 types of Back\n6*6 types of Tail\n\ninside groups of those types each of the 6 main classes is equally represented\nif the class of an Axie matches with the type of a body part, Axie’s stats will increase\n\n\nAdditional Stats Increase\n\n\n\nClass/Type\nHealth\nSpeed\nSkill\nMorale\n\n\n\n\nAqua\n+1\n+3\n0\n+0\n\n\nBeast\n0\n+1\n0\n+3\n\n\nBirds\n0\n+3\n0\n+1\n\n\nBug\n+1\n0\n0\n+3\n\n\nPlant\n+3\n0\n0\n+1\n\n\nReptile\n+3\n+1\n0\n0\n\n\n\n\n\nAn Axie is a ‘pure breed’ when its class and all of its body parts are of the same type.\n\n\n\nAxie Abilities (Cards)\n\nthere are 132 cards in total (first divided by 6 main classes and then by the body parts)\neach Axie has 4 cards associated with it\neach Card has\n\nan amount of Energy it costs to play it\nattack points - damage it does to the enemy Axie\ndefensive points - forms a shield that takes the damage instead of Axie’s Health\npotentially buffs/debuffs\nadditional effects (draw another card, steal some Energy from the opponent,…)\n\ncards realize a “combo” when 2 or more of them are played (for the same Axie)\ncards realize a “chain” when 2 or more Axies use 2 or more cards that are from the same class\n\n\n\nAxie Card Buffs/Debuffs (Card Effects)\n\nthere are 3 buffs (positive effects on the Axie) and 16 debuffs (negative effects on the enemy Axie)\nthey affect the Axie for one or more turns\nthey are “stackable” - their effects are accumulated\n\n\nList of Buffs\n\n\n\nName\nDescription\n\n\n\n\nAttack+\nIncrease next attack by 20%\n\n\nMorale+\nIncrease Moral by 20% for the following round\n\n\nSpeed+\nIncrease Speed by 20% for the following round.\n\n\n\nList of Debuffs\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nAroma\nTarget priority changes to this Axie until the next round.\n\n\nAttack-\nDecrease next attack by 20%.\n\n\nChill\nAffected Axie cannot enter Last Stand\n\n\nFear\nAffected Axie will miss their next attack\n\n\nFragile\nAffected Axie’s shield will take double damage from the next incoming attack\n\n\nJinx\nAffected Axie cannot land critical hits\n\n\nLethal\nNext incoming attack is a guaranteed critical strike\n\n\nMorale-\nDecrease Morale by 20% for the following round\n\n\nPoison\nAffected Axie will lose 2 HP for every card used\n\n\nSleep\nNext incoming attack will ignore shields\n\n\nSpeed-\nDecrease Speed by 20% for the following round\n\n\nStench\nAffected Axie will lose target priority for the following round\n\n\nStun\nAffected Axie’s first attack will miss.Next incoming attack will ignore shields\n\n\nCannot Be Healed\nThis Axie cannot be healed or recover health. This Debuff cannot be removed"
  },
  {
    "objectID": "documents/research/posts/ERFC-39.gfm.html#quick-recap-ecdsa",
    "href": "documents/research/posts/ERFC-39.gfm.html#quick-recap-ecdsa",
    "title": "BLS vs Schnorr vs ECDSA digital signatures",
    "section": "Quick recap: ECDSA",
    "text": "Quick recap: ECDSA\nModern cryptography is founded on the idea that the key that you use to encrypt your data can be made public while the key that is used to to decrypt your data can be kept private. As such, these systems are known as public-key cryptographic systems.\nECDSA stands for Elliptic Curve Digital Signature Algorithm. Elliptic curve cryptography is a form of public key cryptography which is based on the algebraic structure of elliptic curves over finite fields. Used by both Bitcoin and Ethereum.\nElliptic curve: secp256k1\n\nsecp256k1\nThe elliptic curve domain parameters over Fp associated with a Koblitz curve secp256k1 are specified by the sextuple T = (p, a, b, G, n, h) where the finite field Fp is defined by:\n\np = 0xFFFFFFFF FFFFFFFF FFFFFFFF FFFFFFFF FFFFFFFF FFFFFFFF FFFFFFFE FFFFFC2F = 2^256 − 2^32 − 2^9 − 2^8 − 2^7 − 2^6 − 2^4 − 1\n\nThe curve E: y^2 = x^3 + ax + b over Fp is defined by:\n\na = 0x00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000\nb = 0x00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000007\n\nThe base point G in compressed form is:\n\n0x02 79BE667E F9DCBBAC 55A06295 CE870B07 029BFCDB 2DCE28D9 59F2815B 16F81798\n\nand in uncompressed form is:\n\n0x04 79BE667E F9DCBBAC 55A06295 CE870B07 029BFCDB 2DCE28D9 59F2815B 16F81798 483ADA77 26A3C465 5DA4FBFC 0E1108A8 FD17B448 A6855419 9C47D08F FB10D4B8\n\n\n\n\nsecp256k1 curve\n\n\n\n\nPrivate and public keys\nPrivate keys are generated as random 256 bits or 64 random hex characters or 32 random bytes. The public key is derived from the private key using ECDSA. Public key is a point on secp256k1 elliptic curve, generated by formula K = k * G where K is public key, k is private key, G is the constant point on secp256k1 elliptic curve and * is the multiplication operator on secp256k1 elliptic curve. There is no inverse, “/” operator, therefore the relationship between k and K is fixed, but can only be calculated in one direction, from k to K. A private key can be converted into a public key, but a public key cannot be converted back into a private key, because the math only works one way. The multiplication of k * G is equivalent to repeated addition, so G + G + G + …​ + G, repeated k times.\n\n\nSigning and verification\nTo sign and verify ECDSA signature using OpenSSL, do next\n# Generate private key\nopenssl genpkey -algorithm rsa -out privatni.pem\n\n# Generate public key out of private key\nopenssl rsa -pubout -in privatni.pem -out javni.pem\n\n# Test message for signing\necho \"Test\" > message.txt\n\n# Sign the message (with Bitcoin's hashing alghorithm)\nopenssl dgst -sha256 -sign privatni.pem -out signature.bin message.txt\n\n# Verification\nopenssl dgst -sha256 -verify javni.pem -signature signature.bin message.txt\n\n\n\necdsa signing\n\n\n\n\nECDSA verification in Solidity\nimport \"@openzeppelin/contracts/utils/cryptography/ECDSA.sol\";\n\ncontract Example {\n  address public admin;\n\n  constructor() {\n    admin = msg.sender;\n  }\n\n  function verify(bytes32 _digest, bytes calldata _signature) public view returns(bool) {\n    return admin == ECDSA.recover(_digest, _signature);\n  }\n}\n\n\nECDSA verification in Javascript/Typescript\nimport { SignerWithAddress } from '@nomiclabs/hardhat-ethers/signers';\nimport { utils } from 'ethers';\nimport { expect } from 'chai';\n\n(async () => {\n    let admin: SignerWithAddress;\n\n    [admin] = await ethers.getSigners();\n\n    const message: string = 'Hello World';\n    const msgHash: string = utils.hashMessage(message);\n    const digest: Uint8Array = utils.arrayify(msgHash);\n\n    const signature: string = await admin.signMessage(message);\n\n    const address: string = utils.recoverAddress(digest, signature);\n    expect(address).to.equal(admin.address);\n})();"
  },
  {
    "objectID": "documents/research/posts/ERFC-39.gfm.html#bls",
    "href": "documents/research/posts/ERFC-39.gfm.html#bls",
    "title": "BLS vs Schnorr vs ECDSA digital signatures",
    "section": "BLS",
    "text": "BLS\nBLS stands for Boneh-Lynn-Shacham, it’s a signature scheme that is based on bi-linear pairings. A pairing, defined as e(,), is a bilinear map of 2 groups G1 and G2 in some other group, GT. e(,) takes as arguments points in G1 and G2.\nPairings that verifies a signature looks like this:\ne(g1, sig) = e(P, H(m))\n\n# or in expanded form like this\ne(g1, pk*H(m)) = e(pk*g1, H(m)) = e(g1, pk*H(m))\nH(m) is hashing a message to a point on an elliptic curve.\nBLS consists of:\n\nKeyGen — choose a random α. Given generator g1, P=α*g1\nSign — σ = α*H(m) ∈ G2 (in the case of ETH 2.0)\nVerify(P,m, σ) — if e(g1, σ) = e(P, H(m)) return true.\n\nElliptic curve: BLS12-381\n\nBLS12-381\nBLS12-381 is a pairing-friendly elliptic curve construction that is optimal for zk-SNARKs at the 128-bit security level.\nBarreto-Naehrig (BN) curves are a class of pairing-friendly elliptic curve constructions built over a base field Fp of order r, where r≈p. It is possible to construct a new BN curve that targets 128-bit security by selecting a curve closer to p≈2^(384). However, the larger group order r impairs the performance of multi-exponentiation, fast fourier transforms and other cryptographic operations.\nBarreto-Lynn-Scott (BLS) curves are a slightly older class of pairing-friendly curves which now appear to be more useful for targeting this security level. Current research suggests that with p≈2^(384), and with an embedding degree of 12, these curves target the 128-bit security level.\n\n\nPrivate and public keys\nThe private/secret key (to be used for signing) is just a randomly chosen number between 1 and r−1 inclusive. We’ll call it pk.\nThe corresponding public key is P=[pk]g1, where g1 is the chosen generator of G1. That is, g1 multiplied by pk, which is g1 added to itself pk times.\nThe discrete logarithm problem means that it is unfeasible to recover pk given the public key P.\n\n\nSigning\nOne can sign the message by calculating the signature σ=[pk]H(m). That is, by multiplying the hash point by our secret key. But what is H?\nTo calculate a digital signature over a message, we first need to transform an arbitrary message (byte string) to a point on the G2 curve. The initial implementation in Eth2 was “hash-and-check”:\n\nHash your message to an integer modulo q\nCheck if there is a point on the curve with this x-coordinate. If not, add one and repeat\nOnce you have a point on the curve multiply it by the G2 cofactor to convert it into a point in G2.\n\n\n\n\nbls signing\n\n\n\n\nVerification\nGiven a message m, a signature σ, and a public key P, we want to verify that it was signed with the pk corresponding to P. The signature is valid if, and only if, e(g1,σ)=e(P,H(m)).\n\n\n\nbls verification\n\n\n\n\nAggregation\nA really neat property of BLS signatures is that they can be aggregated, so that we need only two pairings to verify a single message signed by n parties, or n - 1 pairings to verify n different messages signed by n parties, rather than 2n pairings you might naively expect to need. Pairings are expensive to compute, so this is very attractive.\nTo aggregate signatures we just have to add up the G2 points they correspond to: σagg=σ1+σ2+...+σn. We also aggregate the corresponding G1 public key points Pagg=P1+P2+...+Pn.\nNow the magic of pairings means that we can just verify that e(g1,σagg)=e(Pagg,H(m)) to verify all the signatures together with just two pairings.\n\n\nBLS verification in Solidity\nBelow shows an example Solidity function that verifies a single signature. EIP-197 defined a pairing precompile contract at address 0x8 and requires input to a multiple of 192. This assembly code calls the precompile contract at address 0x8 with inputs.\n  // Negated genarator of G2\n  uint256 constant nG2x1 = 11559732032986387107991004021392285783925812861821192530917403151452391805634;\n  uint256 constant nG2x0 = 10857046999023057135944570762232829481370756359578518086990519993285655852781;\n  uint256 constant nG2y1 = 17805874995975841540914202342111839520379459829704422454583296818431106115052;\n  uint256 constant nG2y0 = 13392588948715843804641432497768002650278120570034223513918757245338268106653;\n\n\nfunction verifySingle(\n    uint256[2] memory signature, \\\\ small signature\n    uint256[4] memory pubkey, \\\\ big public key: 96 bytes\n    uint256[2] memory message\n) public view returns (bool) {\n    uint256[12] memory input = [\n        signature[0],\n        signature[1],\n        nG2x1,\n        nG2x0,\n        nG2y1,\n        nG2y0,\n        message[0],\n        message[1],\n        pubkey[1],\n        pubkey[0],\n        pubkey[3],\n        pubkey[2]\n    ];\n    uint256[1] memory out;\n    bool success;\n\n    assembly {\n        success := staticcall(sub(gas(), 2000), 8, input, 384, out, 0x20)\n        switch success\n            case 0 {\n                invalid()\n            }\n    }\n\n    require(success, \"\");\n    return out[0] != 0;\n}\n\n\nBLS verification in Javascript/Typescript\nconst bls = require('@noble/bls12-381');\n\n(async () => {\n    // keys, messages & other inputs can be Uint8Arrays or hex strings\n    const privateKey =\n        '67d53f170b908cabb9eb326c3c337762d59289a8fec79f7bc9254b584b73265c';\n    const message = '64726e3da8';\n    const publicKey = bls.getPublicKey(privateKey);\n    const signature = await bls.sign(message, privateKey);\n    const isValid = await bls.verify(signature, message, publicKey);\n    console.log({ publicKey, signature, isValid });\n\n    // Sign 1 msg with 3 keys\n    const privateKeys = [\n        '18f020b98eb798752a50ed0563b079c125b0db5dd0b1060d1c1b47d4a193e1e4',\n        'ed69a8c50cf8c9836be3b67c7eeff416612d45ba39a5c099d48fa668bf558c9c',\n        '16ae669f3be7a2121e17d0c68c05a8f3d6bef21ec0f2315f1d7aec12484e4cf5',\n    ];\n    const messages = ['d2', '0d98', '05caf3'];\n    const publicKeys = privateKeys.map(bls.getPublicKey);\n    const signatures2 = await Promise.all(\n        privateKeys.map((p) => bls.sign(message, p))\n    );\n    const aggPubKey2 = bls.aggregatePublicKeys(publicKeys);\n    const aggSignature2 = bls.aggregateSignatures(signatures2);\n    const isValid2 = await bls.verify(aggSignature2, message, aggPubKey2);\n    console.log({ signatures2, aggSignature2, isValid2 });\n})();"
  },
  {
    "objectID": "documents/research/posts/ERFC-39.gfm.html#schnorr",
    "href": "documents/research/posts/ERFC-39.gfm.html#schnorr",
    "title": "BLS vs Schnorr vs ECDSA digital signatures",
    "section": "Schnorr",
    "text": "Schnorr\nSchnorr signatures are generated slightly differently than ECDSA. Instead of two scalars (r,s) we use a point R and a scalar s. Similar to ECDSA, R is a random point on elliptic curve (R = k×G). Second part of the signature is calculated slightly differently: s = k + hash(P,R,m) ⋅ pk. Here pk is your private key, P = pk×G is your public key, m is the message. Then one can verify this signature by checking that s×G = R + hash(P,R,m)×P.\n\n\n\nschnorr signing\n\n\nThis equation is linear, so equations can be added and subtracted with each other and still stay valid. This brings us to several nice features of Schnorr signatures that we can use.\n\nBatch validation\nTo verify a block in Bitcoin blockchain we need to make sure that all signatures in the block are valid. If one of them is not valid we don’t care which one - we just reject the whole block and that’s it.\nWith ECDSA every signature has to be verified separately. Meaning that if we have 1000 signatures in the block we will need to compute 1000 inversions and 2000 point multiplications. In total ~3000 heavy operations.\nWith Schnorr signatures we can add up all the signature verification equations and save some computational power. In total for a block with 1000 transactions we need to verify that\n(s1+s2+…+s1000)×G=(R1+…+R1000)+(hash(P1,R1,m1)×P1+ hash(P2,R2,m2)×P2+…+hash(P1000,R1000,m1000)×P1000)\nHere we have a bunch of point additions (almost free in sense of computational power) and 1001 point multiplication. This is already a factor of 3 improvement - we need to compute roughly one heavy operation per signature.\n\n\n\nbatch validation\n\n\n\n\nKey aggregation\nWe want to keep our bitcoins safe, so we might want to use at least two different private keys to control bitcoins. One we will use on a laptop or a phone and another one - on a hardware wallet / cold wallet. So when one of them is compromised we still have control over our bitcoins.\nCurrently it is implemented via 2-of-2 multisig script. This requires two separate signatures to be included in the transaction.\nWith Schnorr signatures we can use a pair of private keys (pk1,pk2) and generate a shared signature corresponding to a shared public key P=P1+P2=pk1×G+pk2×G. To generate this signature we need to choose a random number on every device (k1,k2), generate a random point Ri=ki×G, add them up to calculate a common hash(P,R1+R2,m) and then get s1 and s2 from every device (si = ki + hash(P,R,m) ⋅ pki). Then we can add up these signatures and use a pair (R, s) = (R1+R2, s1+s2) as our signature for shared public key P. No one else won’t be able to say if it is an aggregated signature or not - it looks exactly the same as a normal Schnorr signature."
  },
  {
    "objectID": "documents/research/posts/ERFC-40.gfm.html#goals",
    "href": "documents/research/posts/ERFC-40.gfm.html#goals",
    "title": "Access NFT Tools - Overview And Space For Improvements",
    "section": "Goals",
    "text": "Goals\n\nIdentifying content gating tools (that works).\nUnderstanding how they work.\nRecognizing use cases that they are used for, the problems they aim to solve.\n\nIt is essential to overview current solutions in the field we aim to dive into. Both, short-term, for this research and experiment, ut also long-term, for being able to actively track all the innovation in this field from now on.\n\nVerify our presumption about how current solutions are used to gate Telegram and Discord communities: Through using centralized services and storing the user’s private information (wallet address and username), existing solutions periodically verify the user’s possession of specific NFT.\n\nThe main goal of this research is to verify our presumption that there is privacy concern and space for tech improvement within the Access NFT tool niche. If it is accurate, and if the current solution keeps track and connection between private users’ data, we can conclude that there are reasons to move forward with this idea and start working on technical specifications and research how it can be developed."
  },
  {
    "objectID": "documents/research/posts/ERFC-40.gfm.html#methodology",
    "href": "documents/research/posts/ERFC-40.gfm.html#methodology",
    "title": "Access NFT Tools - Overview And Space For Improvements",
    "section": "Methodology",
    "text": "Methodology\n\nTrying out and using existing solutions. We aimed to try each solution available on the market as we believe it is the only way to understand how it works and what problems it solves.\nConsult and discuss with team and community members via contact forms or community servers We want to confirm that our understanding of how a specific tool works and what lies under the hood is correct. Hence, the best way is to receive confirmation from the people building and using it, especially when those tools are not open-sourced.\nAnalyze other reviews There are blog posts, websites, and apps that have already analyzed and gathered information about the current state of Access NFT tools. Thus, we do not need to do all of this work again, as we can benefit from their insights and overviews. Still, this does not mean that we will take it for granted. We want to do our research as well.\n\n*We were not looking under the hood (going through the code). Most of the solutions are not open-sourced. Also, we could understand how they work by consulting with the community and team members behind these solutions."
  },
  {
    "objectID": "documents/research/posts/ERFC-40.gfm.html#collab.land",
    "href": "documents/research/posts/ERFC-40.gfm.html#collab.land",
    "title": "Access NFT Tools - Overview And Space For Improvements",
    "section": "Collab.Land",
    "text": "Collab.Land\nCollab.Land is a sovereign ruler and tool that is used by almost all Discord and Telegram Access communities.\nThe Collab.Land documentation is scarce and it is focused on explaining how to connect their bot rather than explaining how it works. Also, we could not find any other useful information and as their solution was not open-sourced, we reached out directly to them looking for answers that will help us test our hypothesis.\nBased on the answers we received, Collab.Land does store connections between users personal information and their wallet address.\nHowever, before we could conclude that our hypothesis was correct, we had to research Swordy-bot."
  },
  {
    "objectID": "documents/research/posts/ERFC-40.gfm.html#swordy-bot",
    "href": "documents/research/posts/ERFC-40.gfm.html#swordy-bot",
    "title": "Access NFT Tools - Overview And Space For Improvements",
    "section": "Swordy Bot",
    "text": "Swordy Bot\nSwordy-bot is a Discord bot used to verify and grant access to a specific Discord channel (server) if a user has required token(s).\nCompared to Collab.Land, Swordy-bot is built on top of decentralized Unlock Protocol. However, as well as Collab.Land it does store and keep a connection between user’s personal information (Discord id) and wallet address in their centralized database. We came to that conclusion (again) based on the information provided by their team members. .\nAnother common thing with Collab.Land is that Sowrdy-bot is not open-sourced as well.\nWorth mentioning is that, even though Collab.Land is sovereign ruler, Swordy-bot is used as a gate keeper by more than 100 Discord communities.\nFinally, we could confirm that our presumption was correct based on the fact that both of these solutions do store and keep track of the user’s wallet address and username centrally.\nThereafter we went a step further and analyzed other solutions used for content gating. All of them can be divided into 3 categories:\n\nProtocols\nPlatforms\nApps/Tools\n\nAlso each of them is either centralized or decentralized.\nWe have already covered Swordy-bot and Collab.land which are centralized tools."
  },
  {
    "objectID": "documents/research/posts/ERFC-40.gfm.html#unlock-protocol-decentralized-protocol",
    "href": "documents/research/posts/ERFC-40.gfm.html#unlock-protocol-decentralized-protocol",
    "title": "Access NFT Tools - Overview And Space For Improvements",
    "section": "Unlock Protocol [Decentralized Protocol]",
    "text": "Unlock Protocol [Decentralized Protocol]\nUnlock Protocol is an open-source protocol, not a centralized platform, used to create an underlying infrastructure for token gating content (communities, application, websites pages, and sections, images, videos, etc.)\nIt enables:\n\nuser (admin) to deploy a set of smart contracts (on Mainnet, xDAI, Polygon, BSC, or Optimism) and define gating details (number of keys, key price and key duration).\nregular users to purchase key (an NFT) and access content.\n\nUP is the underlying layer that allows other tools to build on top of it and utilize its functionalities. Bellow are listed use cases enabled through community-developed integrations (apps and plugins) build on top of Unlock Protocol:\n\nSwordy-bot - gating Discord communities (servers and channels).\n\nDiscourse plugin - gated content on Discourse.\nWP and Webflow plugins - gated website pages or section.\nDurap module - gating content on Durap.\nSlack plugin - gated Slack servers.\nShopify app - allows merchants to offer special memberships to their customers.\n\nPlugins and other integration tools are the ones that connect blockchain with specific apps (Discord, Slack, etc.). Hence, they are centralized solutions that monitor what’s happening on a blockchain (whether the wallet address still holds an NFT) and inform apps about that. Something like reverse oracles.\nAll in all, Unlock Protocol is a customized set of smart contracts with the following functionalities:\n\nMinting and sending NFTs (locks) to users.\nCollecting and withdrawing crypto on behalf of admin.\n\nAlso, if needed, it can support more traditional (web2) authentication methods by storing private keys on behalf of a user. Also, UP support CC payments.\nYou are right if you think that all of the above may be done without UP. However, it is much easier and faster to implement all of these functionalities by using Unlock Protocol than developing all of the smart contracts and features from scratch."
  },
  {
    "objectID": "documents/research/posts/ERFC-40.gfm.html#whale-room-centralized-platform",
    "href": "documents/research/posts/ERFC-40.gfm.html#whale-room-centralized-platform",
    "title": "Access NFT Tools - Overview And Space For Improvements",
    "section": "Whale room [Centralized Platform]",
    "text": "Whale room [Centralized Platform]\nWhale room is a centralized platform that enables users to create their chat rooms within the platform and gate them by setting up the access requirements (required tokens). Also, it supports token mining and distribution (to the community members).\nWhale room is an alternative for using Discord (or Telegram) in combination with Collab.Land (or Swordy-bot) as it offers both, chat rooms (as Telegram and Discord) and token gating functionality (as Collab.Land and Swordy-bot)."
  },
  {
    "objectID": "documents/research/posts/ERFC-40.gfm.html#mintgate-centralized-tool-platform",
    "href": "documents/research/posts/ERFC-40.gfm.html#mintgate-centralized-tool-platform",
    "title": "Access NFT Tools - Overview And Space For Improvements",
    "section": "MintGate [Centralized Tool & Platform]",
    "text": "MintGate [Centralized Tool & Platform]\nMintGate is a tool that allows users to hide content (URL) and present it only to the ones that possess specific NFT. Users can use MintGate to deploy new NFTs required to access content, but it also supports using other NFTs, minted in the other way.\nIn addition, MintGate recently created a centralized platform that offers the creation of your store and gating it with MintGate technology."
  },
  {
    "objectID": "documents/research/posts/ERFC-40.gfm.html#guild-centralized-platform",
    "href": "documents/research/posts/ERFC-40.gfm.html#guild-centralized-platform",
    "title": "Access NFT Tools - Overview And Space For Improvements",
    "section": "Guild [Centralized Platform]",
    "text": "Guild [Centralized Platform]\nIn combination with its Medusa bot, Guild gate the access to Discord private channels rather than the whole discord server. Compared to other solutions, it supports more complex requirements logic by combining the following requirement options: possession of an NFT, amount of an ERC20 token, and opportunity to whitelist wallet addresses.\nBesides this feature and difference, it offers the same options as using some of the previously mentioned bots combined with Discord or Telegram (that you also need if you want to use Guild)."
  },
  {
    "objectID": "documents/research/posts/ERFC-42.gfm.html",
    "href": "documents/research/posts/ERFC-42.gfm.html",
    "title": "Analysis of smart contract fuzzers",
    "section": "",
    "text": "Executive Summary\nWe are witness of the accelerated development of smart contracts, and more and more valuable assets are contained in them. For this reason, it is very important to write secure smart contracts. However, since people write smart contracts, and people make mistakes, it is desirable to have a tool that will be able to point out potential problems in the code. Fuzz testing is a technique that with early entry stresses our program to reveal errors. We will get to know with fuzing tools through this research.\n\n\nIntroduction\nSmart contracts often contain valuable assets, whether in the form of tokens or Ether. Smart contract’s source code is publicly available, every execution happens on a public network. If smart contracts have vulnerabilities that can lead to catastrophic damages, that is potentially measured in millions of dollars. For example, at 2017 attack on Parity Wallet cost ~30 million dollars1 , at the 2016 DAO Hack cost ~150 million dollars2 , at 2020 Harvest Finance was attacked using flash loans and stole ~30 million dollars3 . To prevent disasters like this, it is important to find any vulnerabilities in smart contracts before deployments. Some of the common vulnerabilities are integer overflow/underflow, race conditions, and also we could have logic mistakes that are hard to detect. Security is essential while developing smart contracts. There are some known hacker attacks and good practices to follow.4\nHence, in the development of smart contracts, testing is one of the most important techniques that require special time aside. Mostly we write unit tests, but unit tests are specific to one use case, and often some edge cases are not covered. There is a big research interest in developing testing tools, especially ones that are able to automatically detect as many problems in the code as possible, one such technique is fuzz testing.\n\n\nGoals & Methodology\nThere are many automatic bug-finding tools, and the purpose of this research is to introduce you to smart contracts fuzz testing. Fuzzing is a well-known technique in the security community it generates random, or invalid data as inputs to reveal bugs in the program, in one word it stress the program and causes unexpected behavior or crashes.\nConsidering the research interest in this area, a number of tools have been developed. Some popular open source tools are ContractFuzzer, ContraMaster, ILF, sFuzz, Smartian, and Echidna. Comparison of these tools is not easy task, the main reason is that each tool covers some specific set of bug classes.\nEchidna is a property-based tool, develop with the aim to check if the contract violates some user-defined invariants, while other tool tries to find crashes. Company Trails of Bits extended fuzz technique to the EVM by developing the Echidna tool. Echidna can test both Solidity and Vyper smart contracts, it is written in Haskell, and main design goals are:\n\nEasy to use and configure\nGood contract coverage\nFast and quickly results\n\nContraMaster have been developed to detect irregular transactions due to various types of adversarial exploits, detects 3 classes of bug: Reentrancy, Exception Disorder, Gasless Send and Integer overflow/underflow.\nContractFuzzer detects 7 classes of bug: Reentrancy, Exception Disorder, Gasless Send, Timestamp Dependency, Block Number Dependency, DelegateCall and Freezing Ether Contract.\nsFuzz detects all classes of bug as Contract Fuzzer plus Integer overflow/underflow. It has an extendable architecture which allows to easily support new bug classes as well. Also, sFuzz is effective in achieving high code coverage\nSmartian detects 13 classes of bug: Assertion Failure, Arbitrary Write Block state Dependency, Control Hijack, Ether Leak, Integer Bug, Mishandled Exception, Multiple Send, Reentrancy, Suicidal Contract, Transaction Origin Use, Freezing Ether, Requirement Violation.\nThe way of generating inputs is different, ContractFuzzer and Echidna generate test cases based on a set of predefined parameter values, and fail to cover deeper paths that expose some vulnerabilities. sFuzz has guided input generation based on a genetic algorithm to iteratively improve its branch coverage. ILF generates input based on AI, using neural networks.\nAll in all, only Smartian, ILF, and Echidna at the end show the path how we could reproduce the bug. As Smartian covers more bug classes than ILF, shown in bellow Figure5 , the focus in this research will be on Smartian and Echidna.\n\n\n\nResults & Discussion\nAll examples are run on MacOS Big Sur, version 11.6, processor 2,6 GHz 6-Core Intel Core i7 and 16GB of memory. As there is no upper bound on how long Echidna can run, but the goal is to find a bug in up to 5 minutes.6 Configuration for Smartian test timeout is set up to 5 minutes.\nLet’s first show and discuss few motivating smart contract examples:\ncontract MotivationExample {\n    function f(int256 a, int256 b, int256 c) public pure returns (int256) {\n        int256 d = b + c;\n        if (d < 1) {\n            if (b < 3) {\n                return 1;\n            }\n            if (a == 42) {\n                assert(false);\n                return 2;\n            }\n            return 3;\n        } else {\n            if (c < 42) {\n                return 4;\n            }\n            return 5;\n        }\n    }\n}\nVery fast both Smartian and Echidna find assertion failure in above smart contract, results with counterexample and information how to reproduce transaction are show in next Figures(Figure1 and Figure2):\n\nFigure1 : Smartian replayable test case\n\nFigure2 : Echidna replayable test case\nThe next two examples have some more complex math:\ncontract MotivationExample {\n    bool private value_found;\n\n    function f(uint256 a, uint256 b, uint256 c, uint256 d) public {\n        require(a == 42);\n        require(b == 129);\n        require(c == d+333);\n        value_found = true;\n        assert(value_found == false);\n    }\n}\nAbove one, the inputs must meet three requirements, and to the equality. Smartian and Echidna test it, and at bellow Figures(Figure3 and Figure4) are results:\n\nFigure3 : Smartian replayable test case\n\nFigure4 : Echidna failed to find assertion\nSmartian quickly found assertion failure and counterexample, while Echidna failed to find one. Take a look at hardest motivation example and result from fuzzers:\ncontract MotivationExample {\n    uint256 private stateA;\n    uint256 private stateB;\n    uint256 CONST = 32;\n\n    function f(uint256 x) public {\n      stateA = x;\n    }\n\n    function g(uint256 y) public{\n      if (stateA % CONST == 1) {\n        stateB = y - 10;\n      }\n    }\n\n    function h() public view {\n      if (stateB == 62) { \n        bug(); \n      }\n    }\n\n    function bug() private pure {\n      assert(false);\n    }\n}\n\nFigure5 : Smartian replayable test case\n\nFigure6 : Echidna failed to find assertion\nAgain, Smartian quickly finds assertion failure, while Echidna fails. The reason for failure is due to the way it generates inputs. Echidna is not smart to go in-depth when making input seeds and figure out the values in the deeper branches.\nIndeed, there is a way for Echidna to find assertion failure in the above examples, solution is in the configuration file.\nEchidna has a YAML configuration file, with configurable parameters, that can be turned on or off during the test. If config.yaml is not listed, the default YAML configuration file is called.7 Some of configuration parameters enable to blacklist function, compute maximum gas usage, the maximum number of transactions sequences to generate, number of test sequences to run, prefix for boolean functions that are properties to be checked, contract deployer address. Also it is possible to define set of addresses transactions originate from along with default balance for addresses. In case we have a complex contract, and we need to initialize the blockchain with some data -> tool Etheno helps here8 , after Etheno finishes the initialization JSON file is created, that is set as initialization inside configuration file. Additionally, if our contract uses some framework, for example, Hardhat or Truffle, Echidna then use crytic compile, and build directory of the framework is sent through crytic arguments inside Echidna configuration file.\nBack to the above example, to find assertion failure, it is enough inside configuration file to set corpus directory. After first run, inside the corpus directory we could see the generated input for contract properties, now is enough to modify the input to use suitable parameters that will cause assertion failure.\nAlthough Smartian beat Echidna in the above examples, the logical question, that arises, is what are the advantages of Echidna and why would we use Echidna rather than Smartian?\nEchidna’s advantage are invariants, Invariants are Solidity functions that can represent any incorrect state that contract can have, each invariant must be:\n\nPublic method that has no argument\nReturn true if it is successful\n\nor:\n\nPublic method that can have an argument\nUse assert in function\n\n\nFigure7 : Architecture of Echdina\nArchitecture9 is divided into preprocessing and fuzzing campaigns. In the preprocessing step, the static analyzer tool Slither10 is used with the purpose to find useful constants and functions for effective testing. In the fuzzing campaign step, using contract ABI(Application binary interface) the random transactions are generated, and also any previous transactions from the corpus are included. In case the vulnerability is detected, a counterexample is automatically minimized to the smallest and simplest sequence of transactions that cause failure.\nRunning Echidna:\n$ echidna-test contract.sol --constract TEST --config config.yaml,\nor if Truffle or Hardhat is used:\n$ echidna-test . contract.sol --constract TEST --config config.yaml\nEchidna can be run from the docker, the official image of the current 2.0.0 version is trailofbits/echidna. Inside docker, default version of solidity compiler is 0.5.7, so if we want to test contracts in another version we need to install solc-select. If the preferred method is to not worry about how to install additional tools, there is a docker image trailofbits/eth-security-toolbox, but currently there ecidna version is 1.7.2.\n\n\nConclusion\nAlthough Smartian is better at generating input, what should be noted is that Smartian still doesn’t have support for solc 0.8.x or greater. All examples from the section Results & Discussion have been tested with solc 0.4.25. For an experiment, if you take IB.sol from Smartian benchmark examples,11 and adapt it to work with solidity version 0.8.9, Smartian will fail to find Integer Bug, but Echidna will find it. If you take a look at comparison of fuzz tools in Goals & Methodology that in the previous version Echidna was not able to found these bugs, as the integer overflow/underflow is one of the features in Echidna 2.0.0 for solc 0.8.x or greater. In Appendix at Figure11 and Figure12 is shown Smartian output, and in Figure13 is shown Echidna output.\nIn general, Echidna and Smartian together cover bug classes: Assertion Failure and Integer Bug. Some comparison examples between Echidna 2.0.0, Smartian solc 0.4.25 and Smartian solc 0.8.9 are in research_examples.\nEchidna Assertion allows us to manage what property should test along with the input range value of testing property arguments, in contrary using explicit property we are not sure which function will be checked and which arguments should be used to call test property, explicit property check all method that is not private or internal.\nWhen asked which tool of the two to use, the simple answer is both. Both tools are promising. They cover different classes of problems and the use of both tools for testing smart contracts reduces the chance that our contract has potential flaws. Echidna is under active development, so it is reasonable that it is buggy. But, they work hard to reply & fix each issue.\nThe Echidna coverage report is a little bit confused, one is shown at Figure8. Some suggestion is to add some styled report(example) and the legend table of symbol meaning:\n\n\n\n\n\n\n\nLine marker\nMeaning\n\n\n\n\n’*’\nIf an execution ended with a STOP\n\n\nr\nIf an execution ended with a REVERT\n\n\no\nIf an execution ended with an out-of-gas error\n\n\ne\nIf an execution ended with any other error (zero division, assertion failure, etc)\n\n\n\n\nFigure8 : Echidna coverage\nUI for both tools should be improved, it would be nice to display emitted events. And, Smartian output could color counterexample and found bugs. The current UI is shown in the bellow Figures.\n\nFigure9\n\nSmartian output\n\n\n\nFigure10 : Echidna output\nIt would be nice to have integration with Remix IDE, which will help with debugging.\n\n\nAppendices\n\nFigure11 : Smartian found Integer Bug with 0.4.25\n\nFigure12 : Smartian not found Integer Bug with 0.8.9\n\nFigure13 : Echidna found Integer Bug with 0.8.9\n\n\nBibliography\n\n\n\n\n\nReferences\n\n‘Building-Secure-Contracts/Workflow.md at Master  Crytic/Building-Secure-Contracts’, GitHub <https://github.com/crytic/building-secure-contracts> [accessed 14 February 2022]\n\n\nChoi, Jaeseung, Doyeon Kim, Soomin Kim, Gustavo Grieco, Alex Groce, and Sang Kil Cha, ‘SMARTIAN: Enhancing Smart Contract Fuzzing with Static and Dynamic Data-Flow Analyses’, in 2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE) (Melbourne, Australia: IEEE, 2021), pp. 227–39 <https://doi.org/10.1109/ASE51524.2021.9678888>\n\n\n‘Etheno’ (Crytic, 2022) <https://github.com/crytic/etheno> [accessed 28 February 2022]\n\n\nFoxley, William, ‘Harvest Finance: $24m Attack Triggers $570m ’Bank Run’ in Latest DeFi Exploit’, 2020 <https://www.coindesk.com/tech/2020/10/26/harvest-finance-24m-attack-triggers-570m-bank-run-in-latest-defi-exploit/> [accessed 11 February 2022]\n\n\nGrieco, Gustavo, Will Song, Artur Cygan, Josselin Feist, and Alex Groce, ‘Echidna: Effective, Usable, and Fast Fuzzing for Smart Contracts’, in ISSTA 2020, 2020\n\n\nSiegel, David, ‘Understanding The DAO Attack’, 2016 <https://www.coindesk.com/learn/2016/06/25/understanding-the-dao-attack/> [accessed 11 February 2022]\n\n\n‘Slither, the Solidity Source Analyzer’ (Crytic, 2022) <https://github.com/crytic/slither> [accessed 11 February 2022]\n\n\n‘The Parity Wallet Hack Explained’, OpenZeppelin Blog, 2017 <https://blog.openzeppelin.com/on-the-parity-wallet-multisig-hack-405a8c12e8f7/> [accessed 11 February 2022]\n\nFootnotes\n\n\n‘The Parity Wallet Hack Explained’, OpenZeppelin Blog, 2017 <<https://blog.openzeppelin.com/on-the-parity-wallet-multisig-hack-405a8c12e8f7/>> [accessed 11 February 2022].↩︎\nDavid Siegel, ‘Understanding The DAO Attack’, 2016 <<https://www.coindesk.com/learn/2016/06/25/understanding-the-dao-attack/>> [accessed 11 February 2022].↩︎\nWilliam Foxley, ‘Harvest Finance: $24m Attack Triggers $570m ’Bank Run’ in Latest DeFi Exploit’, 2020 <<https://www.coindesk.com/tech/2020/10/26/harvest-finance-24m-attack-triggers-570m-bank-run-in-latest-defi-exploit/>> [accessed 11 February 2022].↩︎\n‘Building-Secure-Contracts/Workflow.md at Master  Crytic/Building-Secure-Contracts’, GitHub <<https://github.com/crytic/building-secure-contracts>> [accessed 14 February 2022].↩︎\nJaeseung Choi and others, ‘SMARTIAN: Enhancing Smart Contract Fuzzing with Static and Dynamic Data-Flow Analyses’, in 2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE) (Melbourne, Australia: IEEE, 2021), pp. 227–39 <https://doi.org/[10.1109/ASE51524.2021.9678888](https://doi.org/10.1109/ASE51524.2021.9678888)>.↩︎\nGustavo Grieco and others, ‘Echidna: Effective, Usable, and Fast Fuzzing for Smart Contracts’, in ISSTA 2020, 2020.↩︎\nCryticEchidnaEthereum?↩︎\n‘Etheno’ (Crytic, 2022) <<https://github.com/crytic/etheno>> [accessed 28 February 2022].↩︎\nGrieco and others.↩︎\n‘Slither, the Solidity Source Analyzer’ (Crytic, 2022) <<https://github.com/crytic/slither>> [accessed 11 February 2022].↩︎\nSmartian2022?↩︎"
  },
  {
    "objectID": "documents/research/index.html",
    "href": "documents/research/index.html",
    "title": "Research",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n         \n          File Name\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nRoyalty Contract Standardization - RCS\n\n\nThis research examines the possibility of hardcoding the royalty logic for the NFT royalty payments. It also explores how are royalties for NFT creators/artists taken care of by two largest NFT marketplaces OpenSea and LooksRare. It examines the EIP-2981 which aims to solve the royalty implementation problem. Small experiment is conducted with a goal to modify the transfer function from the ERC-721 standard.\n\n\n\n\n\n\n5/12/2022\n\n\nAleksandar Damjanovic\n\n\n9 min\n\n\nERFC-171.gfm.md\n\n\n\n\n\n\n\n\nDeveloping with Ape\n\n\nThis research examines Ape The Ethereum Development Framework For Python Developers. It examines its plugin system and ease of use. As a conclusion Web3 Tech Radar location is suggested.\n\n\n\n\n\n\n4/30/2022\n\n\nAleksandar Damjanovic\n\n\n4 min\n\n\nERFC-172.gfm.md\n\n\n\n\n\n\n\n\nOWT - Omni Web Token\n\n\nJSON Web Tokens, or JWT, are standard for authorizing clients on the Web. Some JWT payloads have a specific structure, such as OAuth(2) tokens. Others, however, are designed specifically for particular apps. Authorization on the Blockchain is done mainly through explicitly whitelisting addresses that are allowed to perform specific actions. Whitelisting introduces high costs when the number of whitelisted addresses is large. A good example is ICO whitelisting, where hundreds or even thousands of participants need to get whitelisted. This research aims to find an efficient, more cost-effective solution for authorizing users on the Blockchain using a system of authorization tokens issued and received off-chain, without the Blockchain transaction fees, and valid on-chain and off-chain. In addition, the token structure should be transferrable off-chain as a JWT token, compatible with the OAuth2 standard, and reusable in both Web 2.0 and Web 3.0 worlds.\n\n\n\n\n\n\n4/6/2022\n\n\nAleksandar Veljković\n\n\n24 min\n\n\nERFC-147.gfm.md\n\n\n\n\n\n\n\n\nBlacklisting Platform based on untransferable NFTs\n\n\nThe main topic of this research is exploring the possibility and the need to create the “Authority” protocol that would handle blacklisting based on community voting and assessment. This protocol would mint the untransferable NFT to the blacklisted address. Creating this solutions (both the platform and the NFT) would not be challenging but the need for these solutions is questionable to the author. This paper explored the current blacklist cases and the possibility of creating the said NFTs. The Tether case is in the main focus since the solutions are similar and simple. The main takeaway is that creating this solution is not necessary per se, but the issue is open for discussion.\n\n\n\n\n\n\n3/31/2022\n\n\nAleksandar Damjanovic\n\n\n7 min\n\n\nERFC-154.gfm.md\n\n\n\n\n\n\n\n\nNFT that is bound by time\n\n\nNFTs have a unique ID and belong to a single wallet. Two standards define what an NFT is and should do: ERC721 and ERC1155, aiming to distinguish each token to be unique. The development of NFT is still in the early stage, and this research shows how NFTs can change their properties. We go through some existing solutions where some events fundamentally affect the NFT, changing its state, properties, or value. We give an overview of those solutions with a desire to cover how they work under the hood and notice potential problems. In the end, are presented possible use cases that open a new door into the NFT word.\n\n\n\n\n\n\n3/14/2022\n\n\nMarija Mijailovic\n\n\n12 min\n\n\nERFC-101.gfm.md\n\n\n\n\n\n\n\n\nDetect NFT Wash Trading\n\n\nWith the monthly trading volume of the Non-Fungible Token (NFT) marketplace OpenSea reaching 5 billion dollars in January 2022[^1] it is clear that NFTs are gaining popularity and with that grows the importance of having a transparent trading activity.\n\n\n\n\n\n\n3/12/2022\n\n\nMilos Bojinovic\n\n\n14 min\n\n\nERFC-90.gfm.md\n\n\n\n\n\n\n\n\nCrypto Insurance - Current state, problems and possibilities of creating new products\n\n\nIn this paper we covered 5 of the main players in DeFi insurance market in order to determine the products offered, the problems with these products, the way the claims are handled and the possibilty of creating new insurance protocols. Initially we were not familiar with this field and the effort needed for creating these products, so we conducted this explorative research.\n\n\n\n\n\n\n3/9/2022\n\n\nAleksandar Damjanovic\n\n\n38 min\n\n\nERFC-91.gfm.md\n\n\n\n\n\n\n\n\nTransaction splitting\n\n\nMany companies sell goods with a business model based on donating some percent of their profit to charity. The problem is that some of these companies don’t transparently perform these donations. If, for example, the handmade wristwatch company claims that it donates 90 percent of earnings to the charity, the buyers can’t be sure whether they indeed donated the funds to charity or just bought a very expensive product. They can only believe the company itself. Blockchain technology eliminates the need for trust.\n\n\n\n\n\n\n3/8/2022\n\n\nAndrej Rakic\n\n\n5 min\n\n\nERFC-105.gfm.md\n\n\n\n\n\n\n\n\nBLS vs Schnorr vs ECDSA digital signatures\n\n\nBitcoin and Ethereum both use elliptic-curve cryptography for generating keys and signing transactions. The algorithm they both use is called Elliptic Curve Digital Signature Algorithm (ECDSA), which represents a secure way of signing a message (a transaction for example) using Elliptic Curve Cryptography (ECC). This is a comparative analysis of BLS, Schnorr, and ECDSA signatures. The goal is to understand why are Ethereum and Bitcoin migrating from ECDSA, why BLS and Schnorr are superior to ECDSA and what are the key differences between them, how to use these signatures in the development and which Elliptic curves they are using and why.\n\n\n\n\n\n\n3/4/2022\n\n\nAndrej Rakic\n\n\n12 min\n\n\nERFC-39.gfm.md\n\n\n\n\n\n\n\n\nROTOKEN\n\n\nOrganisations may include non-founder members with specific roles (treasurer, secretary, board members…) with higher permission levels, compared to the regular members. Members that have permission levels which include control over organisation finances represent a high risk for the organisation as there is a high risk of losing money if malicious members perform illegal transfers. That problem is especially present in the world of cryptocurrency where transfers cannot be revoked, once transaction is executed there is no rollback. The use of multisig might be an obvious solution, but the high overhead cost of writting multiple signatures on blockchain, even if the transaction is valid, may not be acceptable in many situations where there is a high frequency of transactions.\n\n\n\n\n\n\n3/3/2022\n\n\nAleksandar Veljković\n\n\n9 min\n\n\nERFC-103.gfm.md\n\n\n\n\n\n\n\n\nBuilding on Filecoin and Filecoin Proofs\n\n\nWhen looking to build dApps that utilize decentralized storage Filecoin seems like the best option even with its flaws like : absent proof of deletion for the client, absent encryption, impossible modifying of the stored data and the durability problem of Filecoin’s Proof of Replication. This research gives an overview of competitors in decentralized storage solution field with a focus on Filecoin protocol. It also shows how its competitor Storj differs from Filecoin and the current grant opportunities for potential building on it. Considering the results of the research Filecoin seems to be the best option considering the popularity and the size of its ecosystem. Currently there is no interest for building on and with Filecoin and this is a purely explorative work without experiments, in order to test the researcher’s methodology and the approach to research. However, it proposes a question about a potential way of improving Filecoin, or creating both safer protocol for the user and cheaper for the storage miners.\n\n\n\n\n\n\n2/25/2022\n\n\nAleksandar Damjanovic\n\n\n19 min\n\n\nERFC-38.gfm.md\n\n\n\n\n\n\n\n\nSolidity++ (S++)\n\n\nWriting efficient code in modern languages is mostly reflected in writing efficient algorithms and business logic. Writing efficient code in Solidity is mostly reflected in thinking as an assembler, moving focus from logic implementation to memory optimisations and low level hacks. Highly optimised code may become unreadable and unmaintainable with possible bugs hidden between the lines of bit level operations. To solve this problem, this project proposes multiple automatic optimisation techniques that will be all summed up into a transpiler and Solidity language extensions called Solidity++.\n\n\n\n\n\n\n2/24/2022\n\n\nAleksandar Veljković\n\n\n6 min\n\n\nERFC-57.gfm.md\n\n\n\n\n\n\n\n\nGaming DAPPs - Play to Earn model\n\n\nBlockchain games, also called Gaming DAPPs, are an emerging area in the Web3 space. With their gameplay, Gaming DAPPs currently cannot compete with the 3D AAA games and they mostly resemble 2D Hyper Casual Mobile games. Developers often need to make compromises in separating on-chain activity from the actions that are taken off-chain and so the line is blurred between what is truly a blockchain game and what is not. However, strong advantage of these games is that they almost always offer some form of an economic incentive to the players with an opportunity to “own” part of the game in order to influence the game’s further development.\n\n\n\n\n\n\n2/24/2022\n\n\nMilos Bojinovic\n\n\n32 min\n\n\nERFC-37.gfm.md\n\n\n\n\n\n\n\n\nAccess NFT Tools - Overview And Space For Improvements\n\n\nNFT Access tokens are primarily used to join private Discord and Telegram communities. Still, a significantly larger opportunity lies in many online and offline communities and events that might utilize this technology.[^1]\n\n\n\n\n\n\n2/21/2022\n\n\nMilos Novitovic\n\n\n10 min\n\n\nERFC-40.gfm.md\n\n\n\n\n\n\n\n\nAnalysis of smart contract fuzzers\n\n\nWe are witness of the accelerated development of smart contracts, and more and more valuable assets are contained in them. For this reason, it is very important to write secure smart contracts. However, since people write smart contracts, and people make mistakes, it is desirable to have a tool that will be able to point out potential problems in the code. Fuzz testing is a technique that with early entry stresses our program to reveal errors. We will get to know with fuzing tools through this research.\n\n\n\n\n\n\n2/17/2022\n\n\nMarija Mijailovic\n\n\n14 min\n\n\nERFC-42.gfm.md\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "documents/research/posts/ERFC-105.gfm.html",
    "href": "documents/research/posts/ERFC-105.gfm.html",
    "title": "Transaction splitting",
    "section": "",
    "text": "Executive Summary\nMany companies sell goods with a business model based on donating some percent of their profit to charity. The problem is that some of these companies don’t transparently perform these donations. If, for example, the handmade wristwatch company claims that it donates 90 percent of earnings to the charity, the buyers can’t be sure whether they indeed donated the funds to charity or just bought a very expensive product. They can only believe the company itself. Blockchain technology eliminates the need for trust.\n\n\nIntroduction\nThis paper tends to explain and show how the need for trust can be eliminated when it comes to donations and setting up business models in general. Idea is to have a platform that will allow you to set up a use case in which you want to allow users to buy a product from you where a specific percentage of the purchase will go to a different wallet(s).\n\nX% of purchase transparently goes to verified charity wallet\na split transaction between different co-creators or marketplace and author\n\nWe can develop a fully transparent set of smart contracts and protocols on some general-purpose blockchain and introduce to sellers and customers a brand new way for selling goods and doing business in general.\nAdditionally, if there are too many different wallet recipients, the splitting can be done through merkle tree claim model.\n\n\nGoals & Methodology\n\nGoals\n\nHow to make charity donations and spending transparent\nEliminate the need for trusting the brand, and provide customers a way to easily verify their spendings\nSplit input transaction to several different wallets by desired percentage\n\n\n\nMethodology\n\nWrite a set of smart contracts in Solidity for this purpose.\n\nPros:\n\nPortable to any EVM general purpose blockchain\nNative support for cryptocurrencies\nPossibility for including ERC-20 tokens, as well\nTooling (Hardhat, OpenZeppelin, etc.)\n\nCons:\n\nLack of native percentage operator in Solidity, one need to decide up to which decimal the result is reliable\nTransaction costs\n\n\nIntegrate it with Bizzswap\n\nPros:\n\nOut of the box solution for paying/swapping coins and tokens\n\nCons:\n\nThird-party dependency\n\n\nTesting\n\nUnit tests\nIntegration tests\nEnd to end tests\nStatic analasys\nCode coverage\nGas usage\nFuzz testing (optional)\nFormal verification (optional)\nAudit\nDeployment to testnet\n\n\nThe beauty of the end result is that anyone can port a frontend app to this decentralized protocol and incorporate it in their business models, already existing platforms, etc.\n\n\n\nResults & Discussion\nTransaction can be split among several parties in Solidity like this:\n\nStep 1)\nSmart contract should have the ability to receive native coin or any other token. There is a need for a receive() or fallback() function, depends on the implementation of the smart contract.\nreceive() is called if msg.data is empty, otherwise fallback() is called.\n    /**\n    Which function is called, fallback() or receive()?\n\n           send Ether\n               |\n         msg.data is empty?\n              / \\\n            yes  no\n            /     \\\nreceive() exists?  fallback()\n         /   \\\n        yes   no\n        /      \\\n    receive()   fallback()\n    */\n\n    // Function to receive Ether. msg.data must be empty\n    receive() external payable {}\n\n    // Fallback function is called when msg.data is not empty\n    fallback() external payable {}\n\n\nStep 2)\nSmart contract then needs to split the received amount among other wallets by defined percentage. Sending funds is trivial and we won’t focus on that. The most challenging part is proper ratio calculations in a language with no native support for decimal arithmetics.\nIn Solidity, one must assume that all numbers have 18 decimal precision. For example:\n\n1 is 1000000000000000000,\n0.5 is 500000000000000000, and\n100 is 100000000000000000000\n\nSince there are no native language support for percentage arithmetics in Solidity, devs are using Basis Points as a unit of measurement equal to 1/100th of 1 percent. This metric is commonly used for loans and bonds to signify percentage changes or yield spreads in financial instruments, especially when the difference in material interest rates is less than one percent.\n\n0.01% = 1 BPS\n0.05% = 5 BPS\n0.1% = 10 BPS\n0.5% = 50 BPS\n1% = 100 BPS\n10% = 1 000 BPS\n100% = 10 000 BPS\n\nPossible implementation:\n    function calculateFee(uint256 _amount) public pure returns(uint256) {\n        // for example 1.85% is fee\n\n        return _amount * 185 / 10000;\n    }\n\n\n\nConclusion\nThere are no blockers for implementing this idea in Solidity. The tooling is stable, the math is not complex and can be handled by the language, and the blockchain technology itself is capable for storing this type of applications."
  },
  {
    "objectID": "documents/research/posts/ERFC-90.gfm.html#collecting-and-parsing-of-data",
    "href": "documents/research/posts/ERFC-90.gfm.html#collecting-and-parsing-of-data",
    "title": "Detect NFT Wash Trading",
    "section": "Collecting and Parsing of Data",
    "text": "Collecting and Parsing of Data\nThis research takes into account only trades of NFTs that follow the ERC721 standard. The same principles can be applied to the trades involving the ERC1155 standard, with the only difference being the collection and parsing of trades.\nWhen an NFT trade is executed, the ERC721 compliant contracts emit a Transfer event that contains three fields: previous owner, new owner, and the token id. Using the combination of Etherscan and Alchemy APIs, it is possible to get all the events that were emitted by the transaction and to extract the needed event based on its topic along with all of its fields.\nNot every Transfer event corresponds to a trade, so there needs to be an extra processing step that will eliminate all transfers that were not made through a marketplace. To do this, one needs to go through all of the marketplace’s contract’s transactions and select only those that have the right methodID.\nAfter the seller’s (previous owner’s) and buyer’s (new owner’s) addresses are known, the last step is collecting and parsing of all of their transactions searching for Ether transfers and all the addresses they have interacted with - in this paper referred to as “associates”."
  },
  {
    "objectID": "documents/research/posts/ERFC-90.gfm.html#wash-trading-detectors",
    "href": "documents/research/posts/ERFC-90.gfm.html#wash-trading-detectors",
    "title": "Detect NFT Wash Trading",
    "section": "Wash Trading Detectors",
    "text": "Wash Trading Detectors\nHaving a set of all of the buyer’s and seller’s associates enables the creation of Wash Trading Detectors (WTD). This paper proposes and implements two basic algorithms:\n\nWTD0 that detects a direct transfer by checking if the seller’s address belongs to the set of buyer’s associates\nWTD1* that detects a set of common associates that are Externally Owned Accounts (EOAs)\n\n*WTD1 is incomplete because the detected common associate can be a Centralized Exchange’s (CEX) address which would give a false positive. The only way to make the WTD1 fully functional is to manually keep a list of all the addresses that should be ignored."
  },
  {
    "objectID": "documents/research/posts/ERFC-90.gfm.html#implementation",
    "href": "documents/research/posts/ERFC-90.gfm.html#implementation",
    "title": "Detect NFT Wash Trading",
    "section": "Implementation",
    "text": "Implementation\nUsing the proposed ways of getting the data and reasoning on it, it is possible to extract suspicious wash trading patterns, flag those trades, and perform an analysis of the results. The code shown bellow is capable of getting the count of detected wash trades performed through a marketplace in a given block range :\nimport utils\n\ndef run(contract, methodIds, start_block, end_block):\n    '''Detects potential Wash trades for a marketplace's contract'''\n\n    transactions = utils.get_all_transactions(\n        contract,\n        start_block,\n        end_block\n    )\n\n    wtd0_count, wtd1_count, total = 0, 0, 0\n\n    for  tx in transactions:\n\n        if tx['input'][:10] in methodIds:\n\n            status, logs = utils.get_logs(tx['hash'])\n\n            if status != 1: # Reverted transaction\n                continue\n\n            nft_contract, token_id, A, B = utils.parse_logs(logs)\n\n            if A == None or B == None: # not a standard ERC721\n                continue\n\n            associates_A = utils.get_associates(A)\n            associates_B = utils.get_associates(B)\n\n            wtd0_count += int(\n                utils.wtd0(A, B, associates_A, associates_B)\n            )\n            wtd1_count += int(\n                len(utils.wtd1(A, B, associates_A, associates_B)) > 0\n            )\n\n            total += 1\n\n    return (wtd0_count, wtd1_count, total)\nThe run function consists of getting all transaction data for a marketplace’s contract starting from the start_block up to the end_block and considering only those that are in the provided list of methodIds. These transactions are then parsed, and values are extracted that will be passed to the utils.wtd0 and utils.wtd1 functions which will perform detection.\nFor the full implementation of all of the used helper methods from utils module see Appendix A.\n\nExample\nFor example, let us take the OpenSea’s Wyvern V1 contract and pass three different block ranges. The ['0xab834bab'] argument corresponds to the methodID of the contract’s method that gets called when there is a trade.\nWYVERN_V1 = '0x7Be8076f4EA4A4AD08075C2508e481d6C946D12b'\n\nranges = [\n    [6652089, 6652239],\n    [7486211, 7486311],\n    [7704798, 7704898],\n]\n\nfor start_block, end_block in ranges:\n\n    print(run(WYVERN_V1, ['0xab834bab'], start_block, end_block))\nFrom the total of 23 trades that were made during the provided ranges :\n\nWTD0 flags 8 trades\nWTD1 flags 11** trades\n\n**WTD1 returns a list of associate addresses; these lists were manually checked through Etherscan to see if they belong to a CEX. The list of ignored addresses is available in Appendix B.\n\n\nImplications\nThe sample size of 23 is too small to discuss how the reported numbers relate to all of the NFT trades since the marketplace’s contract deployment. The code itself can, however, serve as a starting point for the development of a service capable of extracting the data from all of the NFT marketplaces since their creation. In that data lies the key to answering not only what trades are a wash trade but also who performed them, how many times was an address linked to a wash trade, whether one NFT has been wash traded multiple times, etc. Such a service would need to effectively manage its resources such as the collection of data and the computation needed in the detection - in the example above, set of associates is always computed from scratch (there is no storing of the result and checking if those values have already been computed). The full specification for the development of this service is out of the scope of this paper and should be a topic of a separate research.\n\nComplex Patterns\nDue to current non-negligible transaction fees on Ethereum and the fact that not many people are deeply looking into each trade, it is unlikely that there are complex patterns present in NFT wash trading. As the fees get lower and as the adoption grows, it’s almost certain that they will emerge. On the Figure 3 is shown one pattern that could be used in the process of wash trading.\n\n\n\n\ncomplex-pattern\n\n\nFigure 3 - Complex pattern\n\nThere are two NFT wash trades present (marked by the black and blue colored arrows). The sequence of transfers is the following:\n\nA finances B and C through 3 and 2 intermediaries, respectively\nB finances D through 3 intermediaries\nD buys an NFT from some non-associated address\nD sells the NFT to C\nD sends the funds to B through the same 3 intermediaries that were used before\nB finances E through 2 intermediaries\nE buys the NFT from C\n\nAfter the last step, E can sell the NFT to an unsuspecting victim. It is important to note that addresses used do not have to be discarded after each wash trade - i.e. B can be used just for routing of the funds. Furthermore, a malevolent entity can inflate the prices of not just a single NFT but for a complete collection, making it look like the collection is very popular, which attracts victims."
  },
  {
    "objectID": "documents/research/posts/ERFC-90.gfm.html#appendix-a",
    "href": "documents/research/posts/ERFC-90.gfm.html#appendix-a",
    "title": "Detect NFT Wash Trading",
    "section": "Appendix A",
    "text": "Appendix A\nImplementation of the utils module.\nimport time, json, requests\nfrom web3 import Web3\n\nconfig = {\n    \"alchemy-url\" : \"\",\n    \"etherscan-api-key\": \"\",\n}\n\nweb3 = Web3(Web3.HTTPProvider(config['alchemy-url']))\n\ndef get_all_transactions(address, start_block = 0, end_block = 19999999):\n    '''Gets all transactions using Etherscan API for the provided address'''\n    transactions = []\n\n    while True:\n        time.sleep(5)\n        result = requests.get(\n            'https://api.etherscan.io/api?module=account&action=txlist' +\n            f'&address={address}' +\n            f'&startblock={start_block}' +\n            f'&endblock={end_block}' +\n            f'&offset={1_000}' +\n            f'&sort={\"asc\"}' +\n            f'apikey={config[\"etherscan-api-key\"]}'\n        ).json()['result']\n\n        transactions += result\n\n        if len(result) < 1_000:\n            break\n\n        start_block = int(result[-1][\"blockNumber\"]) + 1\n\n\n    return transactions\n\ndef is_EOA(address):\n    '''Returns true if the address belongs to an Externally Owned Account'''\n\n    try:\n        _address = Web3.toChecksumAddress(address)\n        return web3.eth.getCode(_address) == b\"\"\n    except:\n        return False\n\ndef get_associates(address):\n    '''Returns a set of all account with which the provided addresses interacted with'''\n\n    transactions = get_all_transactions(address)\n\n    associates = set()\n    for tx in transactions:\n        if tx['from'] != address:\n            associates.update([tx['from']])\n        if tx['to'] != address:\n            associates.update([tx['to']])\n\n    return associates\n\ndef get_logs(tx_hash):\n    '''Gets the logs from the transaction receipt of the tx_hash'''\n\n    tx_receipt = web3.eth.get_transaction_receipt(tx_hash)\n\n    return tx_receipt['status'], tx_receipt['logs']\n\ndef parse_logs(logs):\n    '''Returns the NFT contract's address, token id and addresses involved in the trade'''\n\n    TRANSFER_TOPIC = \"0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef\"\n    WRAPPED_ETH = \"0xC02aaA39b223FE8D0A0e5C4F27eAD9083C756Cc2\"\n\n    nft_contracts, token_ids, _from, _to = [], [], None, None\n\n    for ev in logs:\n\n        if TRANSFER_TOPIC == ev[\"topics\"][0].hex() and ev[\"address\"] != WRAPPED_ETH:\n\n            nft_contracts.append(ev[\"address\"])\n\n            bytecode = \"\".join([x.hex() for x in ev[\"topics\"]]) + \"\".join(ev[\"data\"])\n            _from = \"0x\" + bytecode[66 : 66 * 2][-40:]\n            _to = \"0x\" + bytecode[66 * 2 : 66 * 3][-40:]\n            token_ids.append(int(bytecode[66 * 3 : 66 * 4][2:66], base=16))\n\n    return nft_contracts, token_ids, _from, _to\n\n\ndef wtd0(A, B, associates_A, associates_B):\n    '''WTD0 implementation'''\n\n    return (A in associates_B) or (B in associates_A)\n\ndef wtd1(A, B, associates_A, associates_B):\n    '''WTD1 implementation'''\n\n    EOA_associates = []\n\n    common_associates = associates_A.intersection(associates_B)\n    for ca in common_associates:\n        if is_EOA(ca):\n            EOA_associates.append(ca)\n\n    return EOA_associates"
  },
  {
    "objectID": "documents/research/posts/ERFC-90.gfm.html#appendix-b",
    "href": "documents/research/posts/ERFC-90.gfm.html#appendix-b",
    "title": "Detect NFT Wash Trading",
    "section": "Appendix B",
    "text": "Appendix B\nFollowing is the list of all the addresses that were ignored by WTD1 due to the fact that they belong to CEXs\n\n\n\n\nAddress\nCEX\n\n\n\n\n0x564286362092d8e7936f0549571a803b203aaced\nBinance3\n\n\n0x59a5208b32e627891c389ebafc644145224006e8\nHitBTC2\n\n\n0x56eddb7aa87536c09ccc2793473599fd21a8b17f\nBinance17\n\n\n0xeb2629a2734e272bcc07bda959863f316f4bd4cf\nCoinbase6\n\n\n0xd551234ae421e3bcba99a0da6d736074f22192ff\nBinance2\n\n\n0xb5d85cbf7cb3ee0d56b3bb207d5fc4b82f43f511\nCoinbase5\n\n\n0x0681d8db095565fe8a346fa0277bffde9c0edbbf\nBinance4\n\n\n0x3f5ce5fbfe3e9af3971dd833d26ba9b5c936f0be\nBinance"
  },
  {
    "objectID": "documents/research/posts/ERFC-103.gfm.html#potential-markets",
    "href": "documents/research/posts/ERFC-103.gfm.html#potential-markets",
    "title": "ROTOKEN",
    "section": "Potential markets",
    "text": "Potential markets\nPotential markets that might be interested to use ROTOKEN include: - DAOs - Companies with frequent audits by the investors, board members or shareholders, mostly startups - Banks - Government organisations related with public government funds"
  },
  {
    "objectID": "documents/research/posts/ERFC-101.gfm.html",
    "href": "documents/research/posts/ERFC-101.gfm.html",
    "title": "NFT that is bound by time",
    "section": "",
    "text": "Executive Summary\nNFTs have a unique ID and belong to a single wallet. Two standards define what an NFT is and should do: ERC721 and ERC1155, aiming to distinguish each token to be unique. The development of NFT is still in the early stage, and this research shows how NFTs can change their properties. We go through some existing solutions where some events fundamentally affect the NFT, changing its state, properties, or value. We give an overview of those solutions with a desire to cover how they work under the hood and notice potential problems. In the end, are presented possible use cases that open a new door into the NFT word.\n\n\nIntroduction\nNFTs (Non-Fungible Tokens) reached incredible popularity in 2021 Ryan Browne1. Most of created NFTs are static. We collect it, and we hope its market value will increase. In the case of static NFT, it’s characteristic that its properties and data are immutable and recorded on the blockchain, so such NFTs can’t be changed. Otherwise, there are dynamic NFT for which it’s characteristic that properties and data are mutable, often through oracles that trigger events off-chain system or by interaction with on-chain components, for example, smart contracts other NFTs.\n\n“Dynamic NFTs are the logical next step for the NFT space, allowing unique items to evolve, and sometimes decay. This replicates the ephemeral nature of the real world and potentially gives exceptional value to a collected item because of its current state. NFTs transitioning from being ‘only’ static to being dynamic can be thought of as progressing from 2D to 3D, it enables an immense possibility of use cases.” — Adrien Berthou, Head of Crypto-Native Comms at DoinGud\n\n\n\nGoals & Methodology\nThe goals of this research:\n\nIntroduce with dynamic NFT\nSearch for project that make evolvable NFTs\nResearch how they work, find some leak\nSuggest improvements\n\nMethodology for accomplishing those goals:\n\nGetting under the hood of open source solutions\nTesting existing approach\nSolidity\n\n\n\nResults & Discussion\nFirst of all, let’s briefly review some existing projects:\n\nEtherCards\n\nEtherCards is a dynamic NFT platform that allows anyone to give a base set of traits and requirements and launch their NFT collection so that the EtherCards team can create unique NFTs. Traits can be discounts, special access rights, connections to real-world events, airdrops, upgrades, and other benefits. The ability to have traits allows the creator to maximize the value of their art. Ether Cards is an integrated ecosystem composed of two major parts. Those are the platform and the Ether Cards (an advanced membership NFT card). Anybody can use the EtherCards platform, but the owner of the EtherCards card has certain privileges. Under the hood, EtherCards integrate Chainlink VRF to provide verifiable randomness on-chain. Chainlink allows developers to read data from any external API and blockchain network and perform off-chain computation. That will enable NFTs to be connected to the external world to trigger real-world events, in a word, to be dynamic.\nEtherCards have supported and worked with LaMelo Ball, Mike Tyson, Steve Aoki and DirtyRobot. In the above collections, all NFTs metadata are stored https://client-metadata.ether.cards on the central server. Within that metadata is a link that points to NFTs image on IPFS. So, inside the smart contract, we store points to metadata JSON URIs to all variants of one NFT. Later, inside tokenURI function with the support of Chainlink return dynamically created URI of NFT, only one variant.\n\nLoopheads\n\nLoopheads is a Loopring ‘Loopring - zkRollup Layer2 for Trading and Payment’2 Moody Brains NFT collection, minted on Looprings Layer 2. There are 25 variants for one Loophead avatar(5 different backgrounds and 5 different brain sizes), which one will be displayed depending on the LRC token price using Uniswap Oracles.\nAll NFT metadata are stored on decentralized storage - IPFS, within that metadata, is a link that sends to NFT’s image on IPFS. So, inside the smart contract, we store points to metadata JSON URIs to all of the Loophead’s variations. When a Loophead NFT is accessed because Loopheads use ERC1155 standard, the Loophead NFT runs the uri function, the start point of dynamic calculation, to show the loophead avatar. The calculation is done with the support of Uniswwap V3 Oracles. That changes parts of the metadata link based on LRC price and returns only one variant.\n\nUniswap LP NFT\n\nOn Uniswap V3 liquidity provider(LP) position is represented as NFT. This NFT shows information about liquidity position. Based on the pool and your parameters selected on the liquidity providing interface. The unique NFT will be minted, representing your position in that specific pool. As the owner of this NFT, you can modify or save the position. The best part of this project is that NFT is SVG generated entirely on-chain. Because of that, it is secure as an image not rely on any other service that is not on the blockchain, and it affects the price of that NFT.\nAll liquidity parameters for NFT are stored on-chain. Interesting is that SVG generation is done inside a pure function, and it returns base64 encoded metadata from the view function.\nWhen a Uniswap V3 LP NFT is accessed because it uses the ERC721 standard, it runs the tokenURI function, which is the start point that generates SVG from liquidity parameters and returns base64 encoded metadata.\n\nAavegotchi\n\nAavegotchi is a crypto collectible game. It was developed to provide users with a new blockchain-based game powered by dynamic NFTs. Aavegotchi information such as Aavegotchi name, traits, and SVG files themselves are saved as contract calldata because it is less gas cost than store in contract storage. The fundamental element of Aavegotchi’s game is randomness. Because of that, they use Chainlink VRF. The main idea behind the game is that the more you love your Aavegotchi character, the more rewards it will give you.\nTo store SVG, we pass one or more SVG images as a string, along with the information of SVG category type(aavegotchi, collaterals, eyeShapes, wearables) and size of passed SVG images. So inside tokenURI we have all NFTs prepared to return only one determined based on real-life events.\n\n\nYou can quickly determine where your NFT is by calling the tokenURI or uri function on the contract, which returns a URI that points to metadata that shows where NFT lives. Above project for NFT storage use:\n\nCentralized server\nDecentralized storage (IPFS, Filecoin, Arweave)\nOn-chain storage\n\nThe problem with the central server is that the possibility for manipulation is vast. The server owner can change the JSON scheme of your NFT whenever he wants.\nThe problem with IPFS is that there is no defined way of data replication. It just happens depending on the relevance of the content. In addition, the IPFS node can become offline. The problem is that if the relevance of our data is minor, the bigger is chance to lose data. To resolve the issue, Filecoin and Arweave come into play. Filecoin is a solution where we pay some price to store data for a set time. The problem is that we are limited by time, so our data are not stored permanently. Arweave is a solution that incentivizes the nodes to hold data permanently by paying only one fee in the Arweave token and keeping data forever. The most significant leak here is that it all comes down to having one storage layer that is separate from the blockchain and from the NFT itself on which it is located.\nWhen it comes to on-chain storage, the SVG is scalable because it does not rely on pixels to display the image. SVG is used for vector graphics where we can describe shapes and lines mathematically. But, if we want to present a more complex image in this format, we get a massive SVG file, which will lead to a considerable gas cost. Additionally, the worth of mention is EIP-2569.\n\nEIP-2569 is an Ethereum improvement proposal to allow a smart contract to save and retrieve an SVG image. Based on that, the two methods contract must have: getTokenImageSvg(uint256) view returns (string memory) and setTokenImageSvg(uint256 tokenId, string memory imagesvg) internal. As we can see, the potential flaw of function setTokenImageSvg as input parameter accepts SVG image string, which can lead to a considerable gas cost in case of complex SVG.\n\nThere is no obstacle to the realization of any project that would require the evolution of NFT. Everything needed is that our contract overrides the tokenURI or uri function from ERC721 or ERC1155. Therefore, the precondition is that we have prepared all parameters variants for the dynamic generation of potential variants. The project specification itself decides which variants to return within it.\n  function tokenURI(uint256 tokenId) public view override(ERC721) returns (string memory) {\n    require(_exists(tokenId));\n    return generateDynamicNFT(tokenId);\n  }\n  function uri(uint256 tokenId) public view override(ERC1155) returns (string memory) {\n    require(_exists(tokenId));\n    return generateDynamicNFT(tokenId);\n  }\nInside generateDynamicNFT, the user defines how and under what conditions NFT to generate, usually using oracles.\n\n\nConclusion\nThis research has shown that it is possible to change the data and properties of NFTs, and the next evaluation in NFT is moving from static NFTs to dynamic NFTs. With dynamic NTFs, some use cases could be:\n\nAn NFT ticket that could retain a value after the event is finished can turn as a discount for a related event or, as some bonus, gifts.\nSports NFT cards can evolve, such as updating their player’s stats or having a limited edition of sports event cards if the player got a super score in a match.\nSport NFT cards that receive bonuses or losses based on wins/losses.\nArtist NFT cards that change on a daily/monthly based.\nNFTs affected by social media/real-life events.\nNFTs that affect the real world, where a user can receive a physical item in exchange for NFT.\nKata NFTs were on the users’ progress the Kata NFT will change.\nGeometric shape that change as price change\n\nThe only related problem is about on-chain storage assets. Most NFTs do not store their assets directly on the blockchain because the cost of keeping them on-chain is expensive, as every action and every byte of information we hold on the blockchain has a fee. In addition, the Ethereum blockchain is designed to keep a record of transactions and not to serve as a data warehouse. Second, on-chain geometrical arts can quickly present, but for the more complex SVG files, it is possible to use the bottom-top approach. The idea is to have one zero-based image as a base and then add traits dynamically to the base image. Where will store only characteristics on-chain, and the NFT image is created dynamically. On-chain storage reduces external dependence, increasing reliability, durability, ownership, and decentralization. Keeping assets on-chain has excellent value. Users can rely on the same guarantees of immutability they use to secure property ownership, and the value of such art is more significant. When the asset is being followed on the Ethereum, we also want that asset to be placed on the Ethereum somehow.\n\n\nBibliography\n\n\n\n\n\nReferences\n\nBrowne, Ryan, ‘Trading in NFTs Spiked 21,000% to More Than $17 Billion in 2021, Report Says’, CNBC, 2022 <https://www.cnbc.com/2022/03/10/trading-in-nfts-spiked-21000percent-to-top-17-billion-in-2021-report.html> [accessed 24 March 2022]\n\n\n‘Loopring - zkRollup Layer2 for Trading and Payment’, Loopring <https://loopring.org/#/> [accessed 24 March 2022]\n\nFootnotes\n\n\n‘Trading in NFTs Spiked 21,000% to More Than $17 Billion in 2021, Report Says’, CNBC, 2022 <<https://www.cnbc.com/2022/03/10/trading-in-nfts-spiked-21000percent-to-top-17-billion-in-2021-report.html>> [accessed 24 March 2022].↩︎\nLoopring <<https://loopring.org/#/>> [accessed 24 March 2022].↩︎"
  },
  {
    "objectID": "documents/research/posts/ERFC-91.gfm.html#what-is-insurance",
    "href": "documents/research/posts/ERFC-91.gfm.html#what-is-insurance",
    "title": "Crypto Insurance - Current state, problems and possibilities of creating new products",
    "section": "What is Insurance?",
    "text": "What is Insurance?\nInsurance has a long history, there are claims that it was created around 2000 BC in Babylon, merchant receiving a loan paid the lender extra money in exchange for exemption of loan payment if the merchant’s shipment were stolen. Hovewer, the importance of insurance field cannot be presented without a mention of London’s Lloyd’s. In the 17th century, a London coffeehouse was a meeting place for people seeking marine cargo protection and people willing to take those risks in exchange for premium. The coffeeshop now is the world famous Lloyds. A sheet of cargo and ship information would be filled and the individuals who accepted that risk would sign with their names under it’s description.1 That brings us to a first term in insurance underwriting.\n\nUnderwriting is risk accessment process to determine whether to accept or reject the risk we will come into contact with this term a lot in the later paragraphs.\nAs we previously mentioned the point of insurance is to transfer and share risks.\nThe individuals or companies that would like to transfer risk to other parties by paying a certain fee (premium) are called insured. The reason why the insured avoid the risk is because the loss is too volatile to bear.\nThe party that accepts such risks and and associated fee (premium) is the insurer. Insurers are not averse to exposing themselves to the same risks as insured because of something called pooling and the law of large numbers. The essence of pooling risk is to spread losses of the few over the entire group. The law of large numbers states that the greater the number of exposures the more closely will the actual results approach to the expected average value"
  },
  {
    "objectID": "documents/research/posts/ERFC-91.gfm.html#benefits-of-insurance-and-the-nature-of-insurance",
    "href": "documents/research/posts/ERFC-91.gfm.html#benefits-of-insurance-and-the-nature-of-insurance",
    "title": "Crypto Insurance - Current state, problems and possibilities of creating new products",
    "section": "Benefits of Insurance and the nature of Insurance",
    "text": "Benefits of Insurance and the nature of Insurance\nInsurance allows the insureds to “trade” the risk of loss for the certainty of smaller payments. As a result this ensured the stable cash flow since there are no extreme losses, and if they happen, they are covered by the insurance. As a result of this “stability” provided by insurance there is less need for governments assistance which saves public resources.2\n\n\n\n\n\n\n\n\n\nApplication\nUnderwriting\nPolicy Issuance\nClaim if a loss occurs\n\n\n\n\n\nFigure 1: The process of issuing insurance\n\nThe application for insurance often starts with quoting process where the amount to be paid in premiums are estimated according to the risk the client would like to manage. After the application the underwriting process occurs.\nUnderwriter evaluates the information of the application and then accepts and then “fine-tunes” the policy using the rating tables from the actuaries. Actuaries calculate premiums, in DeFi, this is done in a different way, more words on that in the later paragraphs.\nAfter the underwriter accepts the application the policy is issued.\nIf a loss occures the claims department examines the claim and asks the insured for the proof of loss before they pay the insured amount. The payment depends on the amount of damage suffered and the decision of the claim department.\n\nThe big part of insurance also is its regulation. Insurance is one of the most actively regulated fields, especially after the financial crisis in 2008. The regulation aims to ensure solvency of the insurers. One of the ways the state regulates Insurance companies is limiting their investment tacticts and portfolio allocation , in other words they don’t let them invest in risky assets which is de-facto the norm in cryptocurrency investments. This is one of the first issues encountered if we were to cooporate with existing insurance companies or create our own protocol that is regulated.3"
  },
  {
    "objectID": "documents/research/posts/ERFC-91.gfm.html#the-current-state-of-defi-insurance-and-insurance-with-cryptocurrency-and-insurance-processes",
    "href": "documents/research/posts/ERFC-91.gfm.html#the-current-state-of-defi-insurance-and-insurance-with-cryptocurrency-and-insurance-processes",
    "title": "Crypto Insurance - Current state, problems and possibilities of creating new products",
    "section": "The current state of DeFi Insurance and Insurance with cryptocurrency and insurance processes",
    "text": "The current state of DeFi Insurance and Insurance with cryptocurrency and insurance processes\nWhen it comes to crypto-insurance with traditional insurers the market is non-existent, because of the regulation, lack of awareness and the lack of crypto adoption among general public. That’s why we will be covering DeFi insurance field.\nDeFi Insurance refers to buying coverage against losses cause by events in Decentralized Finance. With various hacks and exploits over the years the need for insuring users from the results of these events emerged. Contrary to the layman’s belief DeFi Insurance field is big and growing with different protocols emerging in it. However only 2% of all DeFi value is insured at the moment.4\nMain protections offered is capital protection against protocol hack/exploit risk, smart contract failures or stablecoin crashes. The premium user pays for a cover depends on the type of the cover, insurance provider and the duration of the cover.\nThe Decentralized part in this type of Insurance is that anybody can act as a coverage provider, which supports the initial writers assumption. They become providers by locking up capital in a capital pool of the insurance protocol thus providing needed liquidity. As coverage providers they choose for which protocols or events they want to provide coverage, for example: If they are certain that a protocol is safe from exploits they will prefer providing liquidity to the pool that covers that event.\nAnother big part of DeFi Insurance is verifying claims. This is often done by the Insurance protocol’s community. Considering the nature of insurance and pooling of risk and collecting coverage from providers they are often assembled as DAOs (Decentralized Autonomous Organizations). This means that governance token holders participate in verifying claims. There are several ways of doing that and we will be covering it in the next paragraph.\n\nMain players in this market\nThe 5 main competitors in DeFi insurance are:\n\nNexus Mutual\nBridge Mutual\nInsurAce\nNsure\nEtherisc\n\nNote: There are more insurance protocols but in order to keep this research a short overview we will showcase five\nWe will be covering them in detail to explore how they work and the type of products offered.\n\n\nNexus mutual\nNexus mutual is an Ethereum-based platform that offers insurance products led by community management and financials. Nexus mutual is set up as a DAO. Nexus offers three kinds of products:\n\nProtection against failures in any protocol used by users yield bearing token (Ethereum only)\nProtection against failures in the individual protocol user has funds in, on any chain, but not in other protocols it uses.\nProtection against hacks and halted withdrawals on exchanges or custodial wallets5\n\nSimply put, Nexus cover protects against loss of funds, not loss of value, except in the Yield Token Cover. In the Yield Token Cover Nexus may pay a claim if:\n\nDuring the cover period the face value of the covered token and the market value of the covered token differ in price by more than 10% for a continuous period of four hours or more; and\nThe Covered Member contributes to the mutual, one unit of face value of the covered token in exchange for 0.90 units of cover amount they wish to claim; and\nThe Covered Member redeems their claim payment during the cover period or within 14 days of the cover period ending.\n\nNexus does not provide Cover where the covered tokens and the cover amount are not denominated in the same refference currency. Nexus also doesn’t provide cover for any material goods loss.\nClaim assessing process:\n\nAll Covered members for a particular covered token will be assessed together for each claim event; and\nThe face value of the covered token immediately prior to the claim event shall be set as part of the claims assessment process; and\nFollowing a successful claim vote all Covered Members will be able to contribute their covered tokens and redeem their claim payment on a proportional basis up to the cover amount.6\n\nWe will not cover the other 2 products into great detail, as they are pretty straight forward. More info on them can be found here: https://nexusmutual.gitbook.io/docs/welcome/faq/cover-products\nAll protocols and custodial accounts can be covered by the platform provided that risk assesesors staked enough value against them. Risk Assessors (experienced auditors, capital providers) can stake value in the form of NXM token, thereby vouching for the security of the protocol/custodian and dropping the price of the cover. NXM can be unstaked at any time subject to a 30-day withdrawal period. When cover is subsequently sold on a protocol or custodian, Risk Assessors earn proportional rewards in NXM equivalent to 50% of the cover premium. If a claim is accepted and a payout occurs, Risk Assessors staked against the protocol/custodian will have their staked NXM burnt on a proportional basis to facilitate the payout of the cover amount. This may result in a Risk Assessor having some or all of their NXM staked against the protocol/custodian burnt to provide capital for the payout of the claim.\nCover becomes available through one of two ways:\n\nWhen Risk Assessors stake NXM against a protocol, custodian or cover product more cover is made available. The mutual places limits on the amount of cover to protect the mutual from being too exposed to any single risk. There are two limits a Specific Risk Limit and a Global Capacity Limit.\n\nSpecific Risk limit means capacity on any particular risk is limited by the amount of staking on that risk. If there is no staking the mutual cannot offer any cover. Specific Risk limit is equal to : capacity factor x net_staked_NXM .\nGlobal Capacity Limit is based on the financial resources of the Mutual and is there to ensure the mutual is not overly exposed to any particular risk, regardless of how much is staked. Global Capacity Limit = Minimum Capital Requirement In ETH (MCReth) x 20%\n\nAs cover policies expire cover becomes available. User can check Nexus Tracker for info on cover expiry.7\n\nMembership issue regarding privacy\nMembership in Nexus requires a one-off membership fee of 0.0020 ETH (~$5.50). However, to become a member users need to verify their identity following their Know Your Customer process. They also cannot accept members from 17 countries, Serbia included, thus limiting the usage of the mutual.\nTransparency\nAll deployed contracts of Nexus Mutual can be found here: https://api.nexusmutual.io/version-data/\nAll info regarding cover, staking and claims approvals/denies can be found here: https://nexustracker.io/\nHow are Cover purchases taken care of by Nexus?\nUsers specify which smart contract address they want cover for. They specify the cover amount , currency (ETH or DAI) and Cover Period. Quote will be generated and they need to make the transaction with Metamask. Users can currently pay with ETH, DAI or NXM (nexus mutual tokens). Cover Holders can submit a claim for material loss that occured within the cover period. They can also submit a claim up to 35 days after expiry. A loss that ocurs after cover policy is ended won’t be covered\nHow are claims taken care of?\nClaims are filed by submission. Members must provide cryptographic evidence of the loss (proof of loss) and their claim is later assessed by Claim Assessors by voting. Assessors are financially incentivised to take a longer-term view as they are required to lockup a stake. This stake is then burned if there is evidence of fradulent voting, which is addressed by Advisory Board. Advisory board consists of five members of founding team of the Nexus Mutual and insurance industry experts. They are said to have :\n\nTechnical Expertise on Smart Contract Security and blockchain\nTechnical Expertise on Insurance and Mutuals\nGeneral Expertise\n\nAdvisory board is there to provide techniqual guidance to the members of the mutual as well as to exercize the emergency functions if they are required.\nThis proposes a question: How do they keep the Advisory Board “in check” with Nexus’s decentralization principles?\nNexus does that by enabling members to kick-vote the Advisory Board members that they think are working maliciously. Board member can be replaced by another member if the membership base agrees. These proposals cannot be interfered with by the existing Advisory Board.\n\n\nBridge Mutual\nBridge Mutual has a similar business model as Nexus Mutual (DAO model). They provide coverage for smart contracts, stablecoins and other services. Bridge allows users to purchase coverage, provide coverage in exchange for yield, vote on policy claims and their payouts. We will not go into great detail as there are various similarities with Nexus Mutual to avoid repeating ourselves.\nThe main difference between Bridge and Nexus Mutual is that that Bridge doesn’t check customer’s ID and is available for residents all countries\nAny user on the platform can create any pool for any project as because the system is permissionless by design.\n\nInitial capital (USDT) must be put into the Project Coverage Pool by the user that is creating the pool.\nThat project can create incentives for coverage providers to provide coverage to their pool by depositing any number of Token into its designated Shield Mining pool which gets distributed to Coverage Providers alongside the typical yield.\n\nCoverage Provider examines the risk of providing Coverage Capital to the Project’s Coverage Pool. When they provide capital they are de-facto telling users that they are sure in the security and stability of the project. They recieve yield from the users purchasing policies and the BMI(Bridge Mutual) token staking. When coverage providers supply capital to the coverage pool they receive a token (bmi(project name)Cover) representing their share in the capital within this capital pool. Coverage providers can then stake those tokens in the bmiCover Staking Contract pool to get additional BMI rewards. They also recieve a BMINFT Bond that represent the amount of for example DAI staked in Cover Staking Contract pool. These NFT Bonds are tradable on any NFT marketplace. The purpose of these NFT bonds is to give the user a way out of their position without removing DAI from the ecosystem. When the Coverage Provider sells his NFT he also transfers the ownership of the staked bmiDAIx.\nPolicy Holder pays a premium for Coverage to protect against the Coverage Event that could affect the insured Project and cause them to lose funds. The total cost of the Premium is split : 80% to coverage 20% to the Reinsurance Pool. They can buy cover for minimum of 1 week and maximum of 52 weeks. Three factors determine the price of cover (premium):\n\nThe utilization ratio of the pool (ratio between cover bought and cover provided, pools that have higher utilization ratio are riskier and more expensive)\nDuration of the cover\nAmount of cover which user wants to buy\n\nThe Reinsurance pool consists of protocol owned funds that are used to provide liquidity to Coverage Pools. The reinsurance pool is funded through 20% of all premiums paid as mentioned above.\nThe Capital Pool aggregates USDT from the Coverage Pools and provides additional revenue to the protocol. Capital Pool sends USDT to yield generating platforms they deem to have low risk. We coulnd’t find exactly what those platforms are. This is a similarity with classic insurance companies which are limited in what way they invest their funds.\nProving a loss\nPolicy holders should submit any or all of the following:\n\nTransaction IDs proving that Policy Holder’s wallet deposited assets into the protocol and transaction IDs of the Coverable Event\nPosts from Protocol team, or an auditing team confirming that there was an exploit and providing additional information\nA description of the Coverable Event\nIf the address affected was not the same address that purchased coverage, any evidence that proves the Policy Holder is the bonafide owner of the address that suffered a Permanent Loss.\n\nThey also need to deposit 1% of the claim’s value in BMI to prevent spam claims. If the claim is valid, USDT is issued to the Claimant. If a claim is denied they can try again by depositing 1% again.\nA successful claim can only recieve up to the policy’s maximum coverable amount. If the DAO determines that the loss suffered was less then the maximum coverable amount, policy owners may recieve less funds. The DAO is incentivized to pay the claimant the exact amount that was lost.\nVoting process\nVotes for claim approval are anonymous and any user holding vBMI can vote. Voters can wote on multiple Claims before submitting them in a batch send. This is done to save time and gas. Only users that vote in the majority are rewarded with BMI for voting. Users that vote in the minority can lose “reputation” which decreases voting power. If there is a large diference in voting yes or no (80% to 20%) users that voted no will lose a portion of their stake.\n\nFigure 2. High Level Overview of Bridge Mutual’s Mechanism\n\n\nInsureAce.io\nInsurAce.io is a multi-chain mutual insurance protocol created in April 2021. It offers products that cover 100+ protocols, 3 CEX and 1 IDO platform. Currently they are depoloyed on Ethereum, Binance Smart Chain, Polygon and Avalanche. InsurAce hasn’t yet adopted the DAO governance mechanism, although they are working on it.\nCurrent state of Insurace.io (Capital pool size, Active cover amount, Capital Ratios etc ) can be found here: https://app.insurace.io/Data/Insurance\nThis protocols has 4 unique selling propositions :\n\n“0” Premium - Which means that the premiums are lower for their products. Their team designed portfolio-centric products to embrace risk diversification, developed models to optimize the cover cost. They did so by using advisors that are experts in the Insurance domain.\nEnriched Product Line - InsurAce.io also offers products that covers non-Ethereum DeFi protocols.\n\nTypes of protocols and smart contract systems covered:\n\nLending Protocols\nDecentralized Exchanges\nDerivative (e.g. Synthetix, Nexus Mutual)\nAsset (e.g. Badger, RenVM)\n\n\nSCR Mining - The participants earn $INSUR tokens by staking into the mining pool. The mutual capitals injected through staking will be managed with rigorous risk control models to adjust the Solvency Capital Requirement (SCR) dynamically and use the secured free capital for investment to control the mining speed accordingly.\nInsurAce tries to combat the low investment returns. Nexus mutual offers capital return to their providers from the premiums paid by users which is low compared to the yield on Compound and Aave. This problem makes users prefer putting their funds elsewhere, instead of the Insurance Protocol. Insurace combats this with offering users:\n\nOption to invest directly in the investment product depending on their risk aversion\nOption to stake in the mutual pool and get the investment carries and $INSUR tokens as rewards\nShares of the premium income\n\n\nInsurAce is operates similarly like the traditional insurance company using the insurance arm and the investment arm.\n“The insurance arm maintains reserve pools which maintain the solvency for claim coverage based on risk exposure. The investment arm maintains investment pools that generate carry to subsidize claims and attracts investors with risk appetite. The free capital in the insurance capital pool can be placed into the investment pool to gain a higher yield, while the insurance arm will protect the investment activities. Meanwhile, the investment arm’s yield will complement the premium on the insurance side and reduce the cover cost for customers.”8\nPricing model\nWhen it comes to before mentioned protocols they rely heavily on the value staked on individual protocols: the higher value staked the lower the premium will be priced. InsurAce tries to combat this with adopting the new actuary-based pricing model to mitigate this in order to assess the expected loss of insurance products fairly, reduce costs and enhance capability.\n“The model’s main inputs are the number/amount of claims and number/amount of exposures in a given time period, which will be used for selecting and training two separate models - the frequency model and the severity model. Frequency modeling produces a model that calibrates the probability of a given number of losses occurring during a specific period, while severity modeling produces the distribution of loss amounts and sets the level of deductible and limit of the coverage amount.” These models are then combined to solve aggregate loss. After that the decided aggregate loss is incorporated into the risk factors of protocols and the premiums are then calculated. The model’s parameters rely on historical data to devise and validate. They plan on taking this further with new Machine Learning methodologies.\nCapital model\nInsurAce’s capital model refers to EIOPA’s Solvency II, this regime is used for insurance and reinsurance in the European Union. It sets requirements needed for insurance products in order to protect policyholdes and beneficiaries.\n“Solvency II is an economic risk-based approach that should assess the”overall solvency” of insurance and reinsurance undertakings through quantitative and qualitative measures. Under Solvency II, the undertaking’s solvency requirements are determined based on their risk profiles and how such risks are managed, providing the right incentives for sound risk management practices and securing enhanced transparency.”\nSolvency II has different tiers of which the SCR (Solvency Capital Requirement) and MCR (Minimum Capital Requirement) are the two most important ones.\n“The SCR is the capital required to ensure that the insurance company will be able to meet its obligations over the next 12 months with a probability of at least 99.5%, while MCR represents the threshold to correspond to an 85% probability of adequacy over 12 months and is bound between 25% and 45% of the SCR. For supervisory purposes, the SCR and MCR can be regarded as”soft” and “hard” floors.”\nInsurAce uses SCR to calculate the minimum requirement funds set aside to pay all the potential claims considering all quantifiable risks. SCR is calculated with the following inputs:\n\nAll the active covers\nAll the outstanding claims\nThe potential incurred but not reported claims\nThe market currency shock risk\nThe non-life premium & reserve, lapse and catastrophe risks\nThe potential operational risk\n\nSCR% = Capital Pool Size / SCR\nA high ratio means the insurance company is financially strong with sufficient available funds to cover potential claims and other risks so the company is less likely to be insolvent . The lowest acceptable ratio is 100%.\nInsurAce also offers information of their Capital Efficiency ratio which shows the company’s current success in deploying capital.\nCER% = Active Cover Amount / Capital Pool Size\nA high ratio means the insurance company is increasing the productivity of its assets to generate income. Desired ratio is between 100% and 300%.\nRisk Assesment by InsurAce.io\nInsuraAce’s Advisory Board performs a preliminary risk assesment on the new protocols at first. InsurAce will also work with auditing firms if there is extra complexity or challenges. After that Advisory Board provides a report and rates the protocol 1 to 5. After they rate it protocol will go through the community risk assesment. Members who participate in the assesment get INSUR tokens as incentive.\nClaims Assesment by InsurAce.io\n\nFigure 3: InsureAce’s Claim Assesment process\nThis diagram shows us the whole system of claims Assesment in a clear way. The main difference is the inclusion of the Advisory board members which consists of various industry experts including the CTO of InsurAce.\n“$INSUR token holders can stake the $INSUR tokens to become the community Claim Assessor. Claim Assessor will be entitled to the right to vote in each claim assessment and earn $INSUR tokens as reward if their votes match with the voting result. During each voting session, the more tokens the user stake, the more voting tickets they will get (* capped at 5% of the total votes), and the more rewards they will receive.”\n\n\nNsure\nNsure was targeted to be a platform for users to trade risks borrowing the operation model of previously mentioned Lloyds London. With Nsure information is transparent and users are allowed not only to be outsorcing risk but also to become risk takers, capital providers, governance actors and auditors of the system.\nMore data on Nsure performance can be found here: https://app.nsure.network/#/cover/my\nProduct\nNsure offers Smart contract cover like previously mentioned protocols. The coverage and exclusions are identical so we will not go into great detail. More info can be found at: https://docs.nsure.network/nsure-network/docs/nsure-smart-contract-protect-policy-wording\nCapital model\nCapital is sourced from capital mining, with return of Nsure tokens for the miners to ensure a continuous capital support to the underwriting. Minimum Capital Requirement (MCR) is calculated based on the volume of each project and the correlation between them. A low MCR% below a pre-determined threshold will result into a lock of assets in capital pool, so as to protect the solvency the business.\nPricing model\nNsure uses a Dynamic Pricing Model to set the price. In this model capital supply and demand from the entire platform determines the price jointly similar to the pricing mechanism in the free market, by having Nsure tokens backing the policies bought. The price is self-adjustable to the movement of supply and demand, subject to the model, moderately stabilising the price change.\nRating System\nNsure uses its N-SCOSS rating system to quantify the code security of projects by assessing:\n\nHistory and Team\nExposure\nAudit\nCode quality\nDeveloper community\n\nClaim process\n\nFigure 4: Nsure claim process\nFor each policy sold there is one chance of claim filling for free. If the first claim was rejected a claim assesment fee of 10% is requested before new assesment.\nPolicy holder must provide evidence of loss on the designated project within the insurance period. Proof of loss must include:\n\nProof of ownership of affected account - After identifying his affected account, policyholder may prove his ownership over the account by signature or making a 0 amount payment to a specified address.\nEvidence of loss - Policy holder should provide:\n\nthe snapshot of the affected address’s balance at blocks before and after attack (to assist claim assessors quickly quantify the amount of lost\ntransaction of selling the damaged assets (loss is only recognised when it is realised)\ndescription of the attack from project team or security specialist\n\n\nClaim assesment\nNsure introduced a similar voting mechanism as previously mentioned protocols. Its features are:\n\nTo be registered as a claim assessor candidate, user must deposit a considerable amount of Nsure token. At launch, the deposit is set at () Nsure token.\nClaim assessors are randomly picked from registered candidate. For each claim, there will be 5 claim assessors.\nAs claim assessors’ reward is proportion to premium, users tend to register for larger size policy. To get each policy equal and fair tender, users do not know the premium of the policy at registration.\nThe token will be slashed if the claim assessor’s judgment is different from the majority.\n\nAfter claims assessors make a decision, policyholder and other Nsure token holders can challenge this decision. This will lead to a public vote for the final conclusion on the issue.\n\n\nEtherisc\nSo far we have been covering only protocols that offer smart contracts protection. Etherisc tries to include material goods into the story. Etherisc is a protocol to collectively build insurance products. Common infrastructure, product templates and insurance license-as-a-service make a platform that allows anyone to create their own insurance products. The first product Etherisc offered was FlightDelay Insurance. Products currently licensed are: Crop Insurance and FlightDelay Insurance. Products currently in design: Hurricane Protection, Crypto Wallet Insurance, Collateral Protection for Crypto backed Loans, Social Insurance (death, heavy illness). They are also open for product requests. Users have the option to build their own insurance product, but more information about the user needs to be provided.9\nThey also launched a Joint Grant Program with Chainlink to accelerate the adoption of data-driven decentralized insurance products, so we think that special attention should be paid towards potential building with Etherisc.10\nEtherisc Token\nDIP Tokens act as the native internal currency that is inseparable from the protocol and network of its users. DIP tokens are needed to earn transaction fees (% of insurance premiums or fixed cost), incentivize and reward platform users to bring risk to the network, build and maintain risk transfer products. The total supply of Etherisc Tokens is 1 Billion.\nDIP tokens give users access to the Decentralized Insurance Platform. By staking DIP token, participants provide collateral (bond) to guarantee future performance, availability, and service levels. Staking also signals quality and reputation. As a result, participants can earn money monetizing their skills, software (for example risk models or UI/UX), risk capital, insurance licenses, claim processing, or regulatory compliance/reporting services.\nFlightDelay Insurance example from the Whitepaper - Launched on January 20 2022\nIn their whitepaper’s FlightDelay Insurance product they use oraclize to obtain data from their data provider. Oraclize charges Etherisc for calling their contract. Etherisc incentivize Oraclize to provide their service correctly by :\n\nIn the buyers market (market with many oracles) - Demanding of Oraclize to put some tokens in a staking contract which will then returns tokens if they deliver in time and forward the tokens to Etherisc in case they miss their obligations.\nIn the sellers market (market with only one or few oracles) - Oraclize will earn an additional profit, again by staking tokens in a “staking contract”, but with reversed roles: Etherisc will stake tokens, and Oraclize will earn these tokens if they deliver, and in case they don’t deliver, the tokens are returned to Etherisc.\nBoth options can be combined with both parties staking tokens from historical flight delay\n\nTheir experience with the Flight Delay DApp confirmed that insurance applications need plenty of capital to be able to scale. But that entry barrier can be overcome with cryptographic tokens that enable highly customized economics. Their goal is to allow the tokenization of risks on the platform and to make them available on a global open-access marketplace.\nIn this kind of insurance the probability is calculated from historical flight data. They used flight delay initially as a POC because of low premiums assosciated with them and under normal circumstances flight dellays are well-aproximated by independent probability events. Etherisk leverages ChainLink data.\nOn January 20th 2022 Etherisc launched FlightDelay on Gnosis Chain Mainnet. It uses Chainlink Data Feeds to autonomously issue policies and execute payouts for travelers who experience flight delays or cancellations. The result of this is insurance policies that are quicker to settle, cheaper to provision thanks to decreases in human and technical overhead and more transparent given the blockchain backend.\nFlightDelay is now available for passenger flights globally. The insurance policies are purchasable with USDC using the Gnosis Chain on Etherisc’s FlightDelay portal. More payment options are in the works.\nParticipants in the Etherisc Protocol\nParticipants in the protocol are:\n\nCustomers - Customers can buy insurance using the token. For convenience, third parties can offer payment gateways and integrations which remove the necessity to own cryptocurrency from the end customer. Furthermore, participants can choose to offer insurance products in any native currency - be it a cryptocurrency, a token or a fiat currency. Use of token: Universal currency to buy insurance products.\nRisk model Providers and Actuaries - “Risk models are fundamental for any insurance product. The correctness of the model is precondition for the economic success of the product.Generally, because of the magnitude of value affected by errors and deviations in the model, a Risk Model Provider won’t take responsibility for the economic outcome of his model, but rather for his adherence to principles and established guidelines in his trade.” Use of token: Staking/Reward for providing or updating risk models\nData providers and oracles - Currently, data is collected together with the application for an insurance, and the insurance company “owns” the data - even after the insurance contract is no longer valid. In a blockchain decentralized environment, the collection of data could be separated. Customers could get paid for voluntarily offering their data to a data pool, which in turn can sell this to interested parties, leaving the ownership of the data completely with the customer. This is an interesting take on handling events in the real world and the real world application of crypto insurance. Use of token: Reward for giving data. Reward for giving access to data pools. Staking / Reward for providing reliable oracles.\nSales Agents - Sales agents are responsible for offering insurance products like in the traditional insurance. Use of token: Reward for distribution of products.\nClaim Agents - There are still many cases where automatic detection and processing the claims is not possible. Specialized and sometimes independent claims agents already exist that can be somewhat utilized e.g. in the area of car insurance, where they help insurers to process claims in shorter time. These claims agents can immediately use a decentralized platform, as soon as adequate products are available. Use of token: Reward for the provided service.\nLicense providers - Insurance in most countries depends on a proper license which can be difficult and costly to obtain. There is also a model where a license provider can act as an intermediary to regulators which is interesting if we are to build a new kind of insurance product. Use of tokens: Staking tokens to provide capital for a license provider, paying fees for licenses.\nProduct managers - Use of token: Reward for service.\n\nEtheriscs’ approach to crypto insurance is interesting but majority of their products are still in the works.Their first product FlightDelay Insurance was launched on January 10th 2022.11"
  },
  {
    "objectID": "documents/research/posts/ERFC-172.gfm.html",
    "href": "documents/research/posts/ERFC-172.gfm.html",
    "title": "Developing with Ape",
    "section": "",
    "text": "Executive Summary\nThis research examines Ape The Ethereum Development Framework For Python Developers. It examines its plugin system and ease of use. As a conclusion Web3 Tech Radar location is suggested.\n\n\nIntroduction\nApe is a new tool for creating and exploring on Ethereum and other blockchains. This framework is written in python with a goal of onboarding more python developers to Web3 thus providing much needed inclusivity in the space.\nTheir goal is to make development smoother with their modular approach. Ape is centered around their open-source plugins written in python; some of them are:\n\nApe-hardhat - Hardhat network provider for Ape\nApe-infura - Infura provider plugins for Ethereum-based networks\nApe-solidity - Support for Solidity smart contracts\nApe-ledger - Ledger Nano S and X plugin for Ape\nApe-alchemy - Alchemy Provider plugins for Ethereum-based networks\n\nThere are over 20 plugins Ape offers. Considering the open-source nature of the project a lot of new plugins are on the way.\nCurrent version of Ape is v0.2.1 and some of the new interesting features offered are:\n\nPolygon, Binance Smart Chain, and Fantom support. Developers can build multi-chain applications with ape’s network switching feature.\nImpersonated account. This let’s the developers test their project and interact with the contract on a fork network pretending to be any account. If you want to impersonate Vitalik, Ape makes that possible.\n\nThey are also working on Ape Project Templates which should increase productivity and enhance developers’ experience when using this framework. Some of the templates Ape is currently developing are:\n\nNFT template\nToken template\nVarious other templates for airdrops,minting\nTemplates for different ERC standards\n\nApe is also set out to be the “first professional-grade smart contract development framework to support multi-chain application development, including non-EVM chains like StarkWare” .[^1]\nAnother sign that Ape is growing is that a Yearn.Finance repo has officially migrated over from Brownie to Ape.\nAs previosly mentioned, this is a new framework and we are expecting more adoption and improvements in the coming months, especially as more developers “take it for a spin”.\nAnother interesting thing is that there is a possibility of developers switching to Ape from Brownie framework, as Brownie updates have slowed down.\n\n\nGoals & Methodology\nThe goal of this research paper is to explore this new player in the smart contract frameworks market, this is an opportunity to explore a new framework that is python oriented.\nThis will be done by writing a simple smart contract, deployment script for rinkeby and a couple tests for said contract and examining the documentation and tutorials present. That way we can research the ease of use for both beginners and experienced developers, and see what is the approach to development process this open-source framework is taking.\nAs a result Web3 Tech Radar location for this framework will be suggested.\n\n\nResults & Discussion\nBeginner friendly?\nAfter initial testing of this framework and considering the state of the documentation at this stage I would recommend this framework to experienced python developers venturing into Web3. Documentation is well written, still in the works and continuously updated with contributions from the community around this framework. Apeworx Team and Apeworx community is currently working on workshops to get developers up to speed and tutorials are in the works.\nCurrently, there is little materials for newcomers. Considering this is a new open-source project this is understandable. However, for absolute beginners, going through the Brownie framework first is recommended at this stage of Ape’s development. The reason for that is abundance of tutorials, workshops and well written documentation. After Brownie, switch to Ape and its plugin system is smooth.\nPerformance:\nApe framework performs well. Smart contract was developed and deployed to Rinkeby test network using a python script without any problems. Verification of the contract on Etherscan via a python script is not yet possible but is in the works in the Etherscan plugin.\nTesting works well both locally and when using network forks, which makes exhaustive testing possible. Currently Ape doesn’t include built-in smart contract fuzz-testing tool.\nCurrently the speed of the framework is satisfactory and more improvements are on the way.\nPlugins:\nOpen-source modular plugins are definitely the highlight of this framework. It allows developers to easily install and remove the functionality they need in their development process and I could see this being a way to onboard new developers from the python world and a way to incentivize developers to develop their own plugins. Some of the interesting Ape plugins are:\nape-tokens is an interesting plugin which allows developers to get token contracts without putting in the addresses themselves.\nExample:\nfrom ape_tokens import tokens\n\nlink = tokens[\"LINK\"]\n\nprint(link.address)\nThis will print out the eth adress of the LINK token. “link” can now be used in various python scripts, be it testing or development.\nape-ledger is a plugin for Ape Framework which integrates with Ledger devices to load and create accounts, sign messages, and sign transactions.\nRequirements\n\nhave the Ledger USB device connected\nhave the Ledger USB device unlocked (by entering the passcode)\nhave the Ethereum app open.\n\nLedger accounts have the following capabilities in ape:\n\nCan sign transactions\nCan sign messages using the default EIP-191 specification\nCan sign messages using the EIP-712 specification\n\nape-trezor is a plugin for Ape Framework which integrates Trezorlib ethereum.py to load and create accounts, sign messages, and sign transactions.\nYou can load the account like any other account in Ape console and then use it to sign transactions like this:\nape trezor sign-message [YOUR TREZOR ALIAS] \"hello world\"\nape trezor verify \"hello world\nThe output of verify should be the same address as the account $account_name.\nApe Polygon Ecosystem Plugin - Ecosystem Plugin for Polygon support in Ape\nApe Fantom Ecosystem Plugin - Ecosystem Plugin for Fantom support in Ape\nape-addressbook is plugin that allows tracking addresses and contracts in projects and globally. This is an interesting way to improve developers user experience and is currently in development.\n…And many more.\n\n\nConclusion\nTech Radar Proposal:\nRecommended location is the Assess ring at this stage. The reason for that is the sheer novelty of this framework. However, the development team is great, community is growing, and we are seeing new projects emerging using Ape. This framework is with its simplicity aiming to become the industry standard in Ethereum development for python developers and is on a great way to do so."
  },
  {
    "objectID": "documents/research/posts/ERFC-154.gfm.html#blacklisting",
    "href": "documents/research/posts/ERFC-154.gfm.html#blacklisting",
    "title": "Blacklisting Platform based on untransferable NFTs",
    "section": "Blacklisting",
    "text": "Blacklisting\nAs we pointed above we did a deep-dive into contracts that utilize blacklisting. In the beginning of the research we have searched for contracts that previously used blacklisting and we have come across the Tether case as it is the biggest one. There are various contracts that use the blacklist contracts but it is the exact same approach. When it comes to individual sites and addresses MetaMask already blocks sites that are known to steal funds.2\nStablecoin issuer Tether froze the ethereum address holding over $715,000 worth of USDT. This address is the address of hackers who stole $3 million on the Multichain bridge. This means that they cannot move the funds.\n\nThe question here is how did Tether manage to do that?\n\nThey managed that by importing the Blacklist contract in their main contract. We will try to explain the contract in the code-box comments below:\n    contract BlackList is Ownable, BasicToken {\n\n        // Getters to allow the same blacklist to be used also by other contracts.(including upgraded Tether) \n        function getBlackListStatus(address _maker) external constant returns (bool) {\n            return isBlackListed[_maker];\n        }\n\n        // Returns the owner of the contract address.\n        function getOwner() external constant returns (address) {\n            return owner;\n        }\n\n        // Puts the blacklisted addresses in the mapping for checking and later use.\n        mapping (address => bool) public isBlackListed;\n\n        // Self explanatory, adds the address to the isBlacklisted mapping. Only owner of the contract can call the function\n        function addBlackList (address _evilUser) public onlyOwner {\n            isBlackListed[_evilUser] = true;\n            AddedBlackList(_evilUser);\n        }\n\n        // Removes the address from the blacklist and \"unfreezes the assets\". Only owner of the contract can call the function.\n        function removeBlackList (address _clearedUser) public onlyOwner {\n            isBlackListed[_clearedUser] = false;\n            RemovedBlackList(_clearedUser);\n        }\n\n        // Destroys the funds of the blacklisted address and reduces the total suply by the same amount. Only owner of the contract can call this function.\n        function destroyBlackFunds (address _blackListedUser) public onlyOwner {\n            require(isBlackListed[_blackListedUser]);\n            uint dirtyFunds = balanceOf(_blackListedUser);\n            balances[_blackListedUser] = 0;\n            _totalSupply -= dirtyFunds;\n            DestroyedBlackFunds(_blackListedUser, dirtyFunds);\n        }\n        // Simple events for transaction logs.\n        event DestroyedBlackFunds(address _blackListedUser, uint _balance);\n\n        event AddedBlackList(address _user);\n\n        event RemovedBlackList(address _user);\n\n    }\nThen they simply put the require statement in all their main contract functions(except for the ones with the “Only Owner modifier”) for example:\n    // Require statement above makes sure the blacklisted address can't access the function.\n    function transferFrom(address _from, address _to, uint _value) public whenNotPaused {\n            require(!isBlackListed[_from]);\n            if (deprecated) {\n                return UpgradedStandardToken(upgradedAddress).transferFromByLegacy(msg.sender, _from, _to, _value);\n            } else {\n                return super.transferFrom(_from, _to, _value);\n            }\n        }\nThis approach to blacklisting gives the Tether the absolute control in what addresses it blacklists and for how long.\nAs we can see in the getBlacklistStatus the other contracts can use the same Blacklist to limit their usage as Tether thus leaning on their decision making.\nIn theory a blacklist protocol can be created where the voting what address to blacklist could be done by the community. The said addresses would be stored in the contract and those addresses could be whitelisted by voting again. Then, other contracts could lean on the “list” and block the addresses from using their functions. This would also make our solution the major point of centralization and considering how easy it is to set up blacklisting individually this proposes a question is the such solution needed?\nImplementing a separate blacklist functions is not challenging and any contract can include them and have the complete control in what addresses it freezes and for how long."
  },
  {
    "objectID": "documents/research/posts/ERFC-154.gfm.html#untransferable-nfts",
    "href": "documents/research/posts/ERFC-154.gfm.html#untransferable-nfts",
    "title": "Blacklisting Platform based on untransferable NFTs",
    "section": "Untransferable NFTs",
    "text": "Untransferable NFTs\nUntrasferable NFTs have been a topic of interest for a while in Web3 and there are various use cases that have been explored. Such as:\n\nidentity\nbadges\nachievements, etc\n\nVitalik Buterin in his blog post showed his interest in “soulbound NFTs”. If we want these NFTs to be truly soulbound (untrasferable) we would need to block transferability thus limiting them to only one address. When it comes to badges and achievements there are already POAPs. POAP has made the decision not to block transferability of POAPs themselves since the owners might want to change addresses and migrate assets for various reasons. There are various cases where they have been sold or even given out for free and after that sold for the highest bidder.3\nWhen it comes to creating a “soulbound NFT” we think that it is possible and that it can be done by modifying the transfer function from the ERC-721 standard.\nThe main issue here is utilizing them in the Blacklisting sense, the mint function from the ERC-721 interface can be included in the addBlacklist() function which would mint the said token to the said address. But so far we haven’t come to the use-case except for “flagging” the addresses for the world to see, so it seems unnecessary at the moment."
  },
  {
    "objectID": "documents/research/posts/ERFC-171.gfm.html#opensea",
    "href": "documents/research/posts/ERFC-171.gfm.html#opensea",
    "title": "Royalty Contract Standardization - RCS",
    "section": "OpenSea",
    "text": "OpenSea\nOpenSea offers royalties for Artists and Creator which are usually around 10%. They are also applied to secondary sales and the proceeds after fees go to the seller of the NFT.\nUsers can check the royalty fees with 3 methods:\n\nAttempting to buy an NFT which will then open up a checkout window where the royalty amount is listed under the name of the NFT.\nInstalling the Flava Chrome extension which shows the royalty next to the NFT without needing to open the checkout menu.\nUsing the NFT analytics tools - There are numerous NFT analytics tools that include creator royalties in their stats.1\n\nAs we can see royalties are very important part of NFT space and they are of great importance for both traders and creators.\n\nHow do creators earn their royaties\nOn OpenSea proceeds from the primary sales of the NFT are immediately forwarded to the creators address. Royalties are usually held by OpenSea for 2-4 weeks before paid out to the creator, this includes both primary sales and secondary sales.\nRoyalties are not automatically set on OpenSea and the creator of the collection must set the royalty percentage and the payout address on the collection level.\n\n\nOpenSea royalty on other marketplaces\nOpenSea royalties are enforced on many other platforms. This is a result of various legal agreements between platforms.\nIf we are talking about ERC-721 and ERC-1155 standard tokens there is no royalty support on token or smart contract level. That is why previosly mentioned legal agreements are needed to enforce royalty payments.2"
  },
  {
    "objectID": "documents/research/posts/ERFC-171.gfm.html#looksrare",
    "href": "documents/research/posts/ERFC-171.gfm.html#looksrare",
    "title": "Royalty Contract Standardization - RCS",
    "section": "LooksRare",
    "text": "LooksRare\nLooksRare is a decentralized NFT marketplace which rewards traders, token stakers, creators and collectors for participating on the platform.\nIt was launched in January 22 with an aim to dethrone Opensea from it’s spot as a leader in the NFT market.\nAs a community-first platform all the revenue generated is distributed to the stakers of LOOKS token.\n\nToken\nLOOKS is the native token that powers the LooksRare marketplace, its price is $0.5908 at the time of this writing. It is used for staking and various rewards.\n\n\nRoyalties\nWhenever the NFT is traded on LooksRare there are 2 types of fee the seller is charged:\n\nPlatform fee (2%)\nCreator royalties\n\n“Creator royalties are fees that are decided by the collection creator. Collection owners can specify a percentage of royalties they wish to recieve on their collection management page.”\nRoyalties are on-chain. Whenever a sale is made through the platform the royalties are paid in the same transaction as the sale and the creators instantly recieve their royalty. This is one of the ways LooksRare is different from OpenSea.\nLooksRare also supports EIP-2981 royalty standard which takes precedent over any royalties specified directly on LooksRare."
  },
  {
    "objectID": "documents/research/posts/ERFC-171.gfm.html#eip-2981",
    "href": "documents/research/posts/ERFC-171.gfm.html#eip-2981",
    "title": "Royalty Contract Standardization - RCS",
    "section": "EIP-2981",
    "text": "EIP-2981\nThis standard provides a way to retrieve royalty payment information for NFTs with a goal to enable universal support for royalty payments across all NFT marketplaces and ecosystem participants.\nThis standard enables all marketplaces to retrieve royalty payment information for a given NFT. This enables accurate royalty payments regardless of which marketplace the NFT is sold or re-sold at.\nThis standard only provides a mechanism to fetch the royalty amount and recipients. The actual funds transfer is something the marketplace needs to do.\n“Royalty amounts are always a percentage of the sale price. If a marketplace chooses not to implement this EIP, then no funds will be paid for secondary sales.”\nThat is one of the reasons hardcoding royalties idea was proposed.\nEIP-2981 can also be integrated with other contracts to return royalty payment information. ERC-2981 is a royalty standard for many asset types.\nIt is recomended to read the full specification of the proposal in order to better understand the issue at hand and the way it is handled in the EIP-2981.\nIn the proposal the writers have come to these conclusions:\n\n“It is impossible to know which NFT transfers are the result of sales, and which are merely wallets moving or consolidating their NFTs. Therefore, we cannot force every transfer function, such as transferFrom() in ERC-721, to involve a royalty payment as not every transfer is a sale that would require such payment.”\n“It is impossible to fully know and efficiently implement all possible types of royalty payments logic. With that said, it is on the royalty payment receiver to implement all additional complexity and logic for fee splitting, multiple receivers, taxes, accounting, etc. in their own receiving contract or off-chain processes. Attempting to do this as part of this standard, it would dramatically increase the implementation complexity, increase gas costs, and could not possibly cover every potential use-case.”\n“This EIP mandates a percentage-based royalty fee model. It is likely that the most common case of percentage calculation will be where the royaltyAmount is always calculated from the _salePrice using a fixed percent i.e. if the royalty fee is 10%, then a 10% royalty fee must apply whether _salePrice is 10, 10000 or 1234567890.”\n“This EIP does not specify a currency or token used for sales and royalty payments. The same percentage-based royalty fee must be paid regardless of what currency, or token was used in the sale, paid in the same currency or token. This applies to sales in any location including on-chain sales, over-the-counter (OTC) sales, and off-chain sales using fiat currency such as at auction houses. As royalty payments are voluntary, entities that respect this EIP must pay no matter where the sale occurred - a sale outside of the blockchain is still a sale.”\n\nThey also plan on taking on the mechanism for paying and notifying the recepient in the future EIPs.3\n\nOur experiment\nAfter trying to implement hardcoding the royalties the same issue with the transfer function occured.\n// SPDX-License-Identifier: MIT\npragma solidity ^0.8.13;\n\nimport \"@openzeppelin/contracts@4.5.0/token/ERC721/ERC721.sol\";\nimport \"@openzeppelin/contracts@4.5.0/token/ERC721/extensions/ERC721Enumerable.sol\";\nimport \"@openzeppelin/contracts@4.5.0/access/Ownable.sol\";\nimport \"@openzeppelin/contracts@4.5.0/utils/Counters.sol\";\nimport \"@openzeppelin/contracts@4.5.0/interfaces/IERC2981.sol\";\n\ncontract InstantRoyaltyToken is ERC721, ERC721Enumerable, Ownable, IERC2981 {\n    using Counters for Counters.Counter;\n\n    Counters.Counter private _tokenIdCounter;\n\n    address royaltyAddress;\n    uint256 royalty = 10_000;\n\n    constructor(address _royaltyAddress ) ERC721(\"InstantRoyaltyToken\", \"IRT\") {\n            royaltyAddress = _royaltyAddress;\n    }\n\n    function safeMint(address to) public onlyOwner {\n        uint256 tokenId = _tokenIdCounter.current();\n        _tokenIdCounter.increment();\n        _safeMint(to, tokenId);\n    }\n\n    // The following functions are overrides required by Solidity.\n\n    function _beforeTokenTransfer(address from, address to, uint256 tokenId)\n        internal\n        override(ERC721, ERC721Enumerable)\n    {\n        super._beforeTokenTransfer(from, to, tokenId);\n    }\n\n    function supportsInterface(bytes4 interfaceId)\n        public\n        view\n        override(ERC721, ERC721Enumerable, IERC165)\n        returns (bool)\n    {\n        return interfaceId == type(IERC2981).interfaceId || super.supportsInterface(interfaceId);\n    }\n\n    function royaltyInfo(uint256 tokenId, uint256 salePrice) public view override returns(address receiver, uint256 royaltyAmount) {\n        return (royaltyAddress, royalty);\n    }\n    /* \n\n        * The function below from ERC721 is the main issue. Making this a\n        * payable function would add unecessary complexity to the standard\n        * and would make the function payable, thus requiring payment \n        * even when sales are not ocurring.\n\n    */\n    function transferFrom(\n        address from,\n        address to,\n        uint256 tokenId\n    ) public virtual override(ERC721,IERC721) {\n            require(_isApprovedOrOwner(_msgSender(), tokenId), \"ERC721: transfer caller is not owner nor approved\");\n            //royaltyInfo and pay royalty for transfer part would go here\n            //implementing this would make the function payable\n            _transfer(from, to, tokenId);\n    }\n\n}"
  },
  {
    "objectID": "documents/research/posts/ERFC-147.gfm.html#oauth-standard",
    "href": "documents/research/posts/ERFC-147.gfm.html#oauth-standard",
    "title": "OWT - Omni Web Token",
    "section": "OAuth standard",
    "text": "OAuth standard\nOpen Authorization, or OAuth for short, is an authorization standard followed by many APIs worldwide. The standard specifies the protocol between client and authorization server and data transferred in protocol messages. There are two versions of OAuth standard, 1 and 2.\nOAuth standard doesn’t specify the transfer method for sending messages, but one widely adopted standard is using JWT tokens as the authorization data container."
  },
  {
    "objectID": "documents/research/posts/ERFC-147.gfm.html#jwt-tokens",
    "href": "documents/research/posts/ERFC-147.gfm.html#jwt-tokens",
    "title": "OWT - Omni Web Token",
    "section": "JWT Tokens",
    "text": "JWT Tokens\nIn 2015, Michael Jones, John Bradley and Nat Sakimura introduced JSON Web Tokens (JWT) through RFC-7519 as a compact structure for representing claims transferred between two parties [3]. Since then, JWTs have been used for client authentication on the Web. Initially, the authentication was performed between APIs but quickly found use in client authentication following the growth of client-heavy applications.\nJWT has a general structure, made of a header, body, and signature segments. The header segment includes the token type and a label of the algorithm used for creating signatures.\nExample JWT header\n    {\n        \"type\": \"JWT\",\n        \"alg\": \"ES256\"\n    }\nThe body segment is a container segment used for storing protocol-specific information. OAuth 2.0 standard, introduced with RFC-6749 [4], specifies several fields required for authorization that are members of the body of a JWT token. Some of those fields are:\n\naud (Audience) - Identifier of the user to whom the client will present the token for executing an action\nexp (Expiration) - Token expiration timestamp\niss (Issuer) - Identifier of the token issuer, authorization service provider\nscope (Action scope) - List of actions for which the token owner would be authorized to perform\nsub (Subject) - Identifier of the token owner\niat (Issued at) - Isusing timestamp\n\nExample JWT Body\n    {\n      \"sub\": \"user1\",\n      \"name\": \"John Doe\",\n      \"iat\": 1516239022,\n      \"aud\": \"server1\",\n      \"scope\": [\"data.fetching\"]\n    }\nField name in the JWT body example represents a custom, application-specific data field.\nThe signature part of the JWT token contains the hash or signature of the token provided by the token issuer.\nThe three token parts are put together into one base64 encoded string (without trailint = symbols), having the tree parts separated by the dot . symbol.\nExample of a complete JWT string\neyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiaWF0IjoxNTE2MjM5MDIyfQ.SflKxwRJSMeKKF2QT4fwpMeJf36POk6yJV_adQssw5c"
  },
  {
    "objectID": "documents/research/posts/ERFC-147.gfm.html#general-authorization-protocol",
    "href": "documents/research/posts/ERFC-147.gfm.html#general-authorization-protocol",
    "title": "OWT - Omni Web Token",
    "section": "General Authorization Protocol",
    "text": "General Authorization Protocol\nA general protocol for client authorization on the Web using JWT tokens consists of two steps: - Requesting token from the authorization server for executing some action - Presenting the authorization token received from the authorization server to the server which would perform the requested action"
  },
  {
    "objectID": "documents/research/posts/ERFC-147.gfm.html#existing-approaches",
    "href": "documents/research/posts/ERFC-147.gfm.html#existing-approaches",
    "title": "OWT - Omni Web Token",
    "section": "Existing Approaches",
    "text": "Existing Approaches\nThere are existing approaches for combining the Blockchain and OAuth tokens. In one research [5], the authors used NFTs as authorization tokens generated on-chain that would be verifiable using the OAuth 2.0 protocol. That approach is inverse to this research: it doesn’t reduce costs but puts authorization tokens on the chain."
  },
  {
    "objectID": "documents/research/posts/ERFC-147.gfm.html#chain-token-ct-representation",
    "href": "documents/research/posts/ERFC-147.gfm.html#chain-token-ct-representation",
    "title": "OWT - Omni Web Token",
    "section": "Chain token (CT) representation",
    "text": "Chain token (CT) representation\nThe essential requirements for the token that would be used on the chain are:\n\nEfficient verifiability; token should be efficiently verifiable on the chain\nAuthenticity; the probability of token forgery should be negligible\nSuccinctness; token should be small in byte size\nNon-transferability; token should be used only by the user who received the token\n\nAs an example throughout this research, we will use the money checkout allowance problem, where a client needs to be authorized to payout a certain amount of money from the account of the authorizing entity. An existing approach for solving this problem is calling the approval method on the smart contract or whitelisting clients and explicitly approving the total allowed amount for each client. Method signature without a token argument would look like this:\npayout(address account, address receiver, uint256 amount)\nA naive approach for constructing a token that would follow JWT logic would be providing all the values bound by the token. In our example, the values that are required for money checkout from the account are: - account owner identifier; 20 bytes address value - authorized client identifier; 20 bytes address value - allowed amount; 32 bytes value\nNext, the apparent problem is a forgery. Everyone can create a token with listed values and submit it on the Blockchain. The solution for this problem is to provide a signature made by the authorization entity, which confirms the provided values. Ethereum signatures contain three segments, v, r, and s, 65 bytes long. All summed up to 137 bytes of memory. Even if this doesn’t look like a significant sum, the main issue is that the token size asymptotically grows by \\(O(n)\\), linearly with the number of arguments n. In other words, it would become costly, or even unusable, for methods with more arguments. It is not very efficient, but it is a start.\n\nToken size optimization\nThe problem with the naive approach is linear growth with the number of arguments. Solving this problem requires looking closely at the method that is being called. The method payout already contains the token values as a method argument, and it would be redundant to provide them again in the token. The same pattern is visible with different use-cases. This observation suggests that we can avoid providing the values within the token but use only a hash of the approved values and check if the hash of the provided input values matches the token hash. Using the hashing method clears the linear growth of the token as the size of the keccak256 hash is fixed to 32 bytes in length, achieving a constant \\(O(1)\\) size of the token. The hash can also cover other values that could be hard-coded into the smart contract without a token size increase, which will become an essential feature in later sections.\n\n\nSignature size optimization\nThe Ethereum signature size is fixed to 65 bytes. That means that it passes the size of 2 memory words (32 bytes in size) and requires three blocks of memory. The solution for this problem comes with EIP-2098 [6], which proposes a simple technique for compact signature representation, reducing its size by 1 byte and allowing it to fit into two memory blocks.\nThe total token size is now fixed to 32 bytes of token hash plus another 64 bytes of signature data totaling 96 bytes or three blocks of memory.\n\n\nMapping OAuth2.0 parameters\nNow that we have a token structure, we need to figure out how to standardize token parameters to comply with the OAuth2.0 standard.\nThe audience parameter refers to the smart contract containing the payout method. It doesn’t need to be provided explicitly as a method argument as it is already encoded in the smart contract.\nThe token issuer can be deduced from the signature and doesn’t need to be explicitly provided.\nThe subject parameter is provided both as the input argument of the payout method. It doesn’t have to be provided explicitly. It should not be provided even as the argument, as it is already given as the message sender value.\nThe scope parameter describes the action that should be executed. It should not be explicitly provided as it should be hardcoded in the method.\nThe token expiration time is a tricky one. It doesn’t naturally belong to method arguments, so it should be provided explicitly. To prevent the token size increase, we can do a simple modification of the token hash. The value of the timestamp can be stored in 8 bytes. A straightforward solution is to provide another method argument with an 8-byte value. A more elegant solution is to transform the keccak256 value into “pseudo-keccak224” (SHA224 [7] value, with unchanged initialization value) by truncating the hash size to its 224 bytes prefix and appending 8 bytes extension with expiration timestamp as the last 8 bytes of the token hash. This transformation returns us to the previously proposed token size without extra arguments.\nAs we can see, all the crucial parameters of the OAuth2.0 protocol can be successfully mapped to the Blockchain transactions and the chain token."
  },
  {
    "objectID": "documents/research/posts/ERFC-147.gfm.html#token-reusability",
    "href": "documents/research/posts/ERFC-147.gfm.html#token-reusability",
    "title": "OWT - Omni Web Token",
    "section": "Token reusability",
    "text": "Token reusability\nSome authorization tokens are reusable many times until the expiration date. However, some use-cases require that the tokens may be used only once. An example of such a use case is exactly the example we have used so far. Once the payout method has been executed, the user should not be able to perform double-payout transactions. This problem can be solved the same way as in Web 2.0 - by introducing a payment identifier. The payment identifier should be included as an extra method argument and included in the token hash. This solution also solves the issue of the lost token, as the token can be reissued with the same payment identifier and used for the same payment only once. Specific use cases may require the existence of only one token. In that case, the token (and method arguments) may include jti parameter or JWT token ID as a token identifier or use the token hash as an identifier. The smart contract should implement mechanisms for preventing double payments and a potential blacklisting of tokens."
  },
  {
    "objectID": "documents/research/posts/ERFC-147.gfm.html#action-scope-schemas",
    "href": "documents/research/posts/ERFC-147.gfm.html#action-scope-schemas",
    "title": "OWT - Omni Web Token",
    "section": "Action scope schemas",
    "text": "Action scope schemas\nOAuth2.0 proposes a parameter that includes the action scope or label of the action for which the client can use the token. Action scopes are not specified as they can be represented using any string value. The problem here is the cost of using string data types in smart contracts. This research proposes restriction for action type values to numerical data types of 4 bytes. This restriction allows \\(2^32\\) possible scope values that could represent many use cases.\nA library of token schemas can help developers properly format their tokens based on scope number schemas. Furthermore, enumeration of action types can introduce standardization for tokens where a smart contract may require, for example, “scope 42” tokens for executing a method. We present the first two token scopes that would be used in the following use-cases:\n\nScope 1: Generic Identity\nGeneric identity scope should be used for verification of the client’s identity. The verification is based on the client’s wallet address and unique identifier in the issuer’s database. The hash for “scope 1” tokens be made by hashing the following array of values in their respective order:\n\naddress iss; token issuer address aud; smart contract address or 0 address for general identification\naddress sub; client’s wallet address\nuint4 scope; action scope with value 1 for generic identification\nbytes32 uuid; unique identifier of the client in the issuer’s\nuint8 exp; token expiration timestamp in UNIX timestamp format database\n\n\n\nScope 2: Allowance\nAllowance tokens, used for crypto cheques, should have hashes made by hashing the following array of values in their respective order:\n\naddress iss; token issuer\naddress aud; smart contract address\naddress sub; money receiver\nuint4 scope; action scope with value 2 for allowance\nbytes32 paymentId; payment identifier (NOTE: token hash may be used as payment ID but it can introduce new problems)\nuint256 amount; amount to transfer\nuint8 exp; token expiration timestamp in UNIX timestamp format\n\nThe reader may notice that in both cases, there are two logical groups of parameters: - general parameters; iss, aud, sub,scopeandexp- application-specific parameters;userId,paymentId,amount`\nThe general parameters are the same for all tokens, and application-specific parameters are different for each scope."
  },
  {
    "objectID": "documents/research/posts/ERFC-147.gfm.html#from-chain-token-ct-to-omni-web-token-owt",
    "href": "documents/research/posts/ERFC-147.gfm.html#from-chain-token-ct-to-omni-web-token-owt",
    "title": "OWT - Omni Web Token",
    "section": "From Chain Token (CT) to Omni Web Token (OWT)",
    "text": "From Chain Token (CT) to Omni Web Token (OWT)\nNow that we have the complete definition of the chain token, we can go one more step and make it usable and transferrable in the Web 2.0 world. We can do this by packing chain token data as part of theJWT token, following the OAuth 2.0 schema.\n\nOWT Body\nThe token scopes define OAuth 2.0 parameters, so the only remaining thing is appropriately packing the chain token into the JWT body and creating a proper JWT signature. The verifier needs to know the CT hash value and the parameters used in the construction of the token.\nThe proposed OWT body schema is:\n    {\n   \"aud\":\"<smart contract address>\",\n   \"iss\":\"<issuer's wallet address>\",\n        \"scope\": \"<readable name of the '<scope>'\",\n        \"exp\": \"<token expiration timestamp>\",\n        \"chain_token\": {\n            \"token_hash\": \"<CT hash with expiration timestamp>\",\n        \"r\": \"<signature r value>\",\n        \"sv\": \"<compact representation of s and v signature values>\",\n            \"params\": \"[<ordered list of token parameters in form of:\n                     {\n                        param: <parameter name>,\n                        value: <parameter value>,\n                        type: <parameter data type>,\n                     }\n                >]\"\n        }\n\n\nOWT Signature\nThe JWT standard allows using the Ethereum secp256k1 signatures by providing the EC256 algorithm type value in the token header. The signature of the OWT token is created using the issuer’s private key. To verify the signature, the verifier needs access to the issuer’s public key, which should be available from the issuer’s /.well-known/jwks.json route of the authorization API."
  },
  {
    "objectID": "documents/research/posts/ERFC-147.gfm.html#owt-issuing",
    "href": "documents/research/posts/ERFC-147.gfm.html#owt-issuing",
    "title": "OWT - Omni Web Token",
    "section": "OWT Issuing",
    "text": "OWT Issuing\nThe OAuth 2.0 protocol allows using client id and client secret parameters when requesting the authorization tokens. OWT requests can be made using wallet address as client’s identity and signature of the clients wallet address as the client secret parameter"
  },
  {
    "objectID": "documents/research/posts/ERFC-147.gfm.html#allowance-use-case",
    "href": "documents/research/posts/ERFC-147.gfm.html#allowance-use-case",
    "title": "OWT - Omni Web Token",
    "section": "Allowance Use Case",
    "text": "Allowance Use Case\nThe allowance use case tested the token issuing protocol for payout allowances, evaluated the costs of performing payouts using the token, and compared the results with the basic whitelisting protocol.\n\nTest setup\nAuthorization entity submitted allowance amount for the client using the whitelist smart contract method. The cost of the whitelisting transaction was 44,484 gas.\n\n\nTest 1: Payout by Whitelisting\nThe client executed the payoutOld method of the smart contract with a propper allowance amount. The cost of the payout transaction was 22,389 + T gas.\n\n\nTest 2: Payout with Token\nThe client requested an OWT from the authorization server using OAuth 2.0 request schema. The authorization server verified the client’s credentials and issued a “scope 2” token to the client with a specified allowance amount. The client verified the OWT data and extracted the chain token and its parameters from the OWT. After the OWT verification step, the client submitted them to the smart contract using the payoutNew method. The payout was successful, and the execution cost was 59,935 + T gas.\n\n\nTest 3: Payout with Token using Token Hash as Payment Identifier\nThe protocol for issuing and verifying the OWT was done the same way as in the previous test. The only difference was that the token did not follow the “scope 2” schema but left out the payment identifier. The client performed a payout using the payoutNewShort method. Again, the payout was successful, and the execution cost was 59,189 + T gas.\n\n\nCost Analysis\nThe test results show that the cost of the payout protocol using whitelisting was 44,484 gas for the issuer and 22,389 gas for the client, resulting in 66,873 gas for the entire protocol. Test 2 showed that using the token has reduced the issuer’s costs to 0, but the client’s cost was increased to 59,935 gas which is 37,546 gas more than in test 1. Interestingly, the overhead cost for the client is lower than the costs of the issuer’s whitelisting, which indicates that using the token reduces the issuer’s fees, compared to whitelisting, even if the issuer compensates the overhead client’s costs by increasing the allowance amount. Test 3 showed that using token hash as payment identifier reduces the costs for extra 746 gas. However, this introduces problems with token reissuing. The exact token needs to be reissued every time the original one gets lost, requiring the expiration timestamp to be the same as the original one for the hashes to match and thus have the same payment identifier. This situation can cause the issuer to be unable to reissue the token as its timestamp has already expired. The results of the cost analysis are presented in Table 1. \n\n\n\nMethod\n\n\nIssuer’s costs (gas)\n\n\nClient’s costs (gas)\n\n\nTotal protocol costs (gas)\n\n\n\n\nWhitelisting\n\n\n44,484\n\n\n22,389\n\n\n66,873\n\n\n\n\nOWT allowance\n\n\n0\n\n\n59,935\n\n\n59,935\n\n\n\n\nShort OWT allowance\n\n\n0\n\n\n59,189\n\n\n59,189\n\n\n\n\nTable 1. Cost anlysis of the allowance use case methods"
  },
  {
    "objectID": "documents/research/posts/ERFC-147.gfm.html#generic-identity-use-case",
    "href": "documents/research/posts/ERFC-147.gfm.html#generic-identity-use-case",
    "title": "OWT - Omni Web Token",
    "section": "Generic Identity Use Case",
    "text": "Generic Identity Use Case\nThe generic identity use case tested the issuing and verification of the “scope 1” tokens and assessed the costs of using identification tokens on the chain.\nThe client acquired OWT from the authorization server and submitted the token with the received user id from the issuer’s database. The client submitted the token to the verifyIdentity method of the smart contract and successfully verified the identity token. The cost of the verification was 36,816 gas, which suggests that the verification proces cost was 15,817 gas, leaving out the base transaction cost.\n\nUse case analysis\nThe test showed that the costs for verifying the identity tokens are low and open a new path for the identity representations valid on multiple chains. Additional use cases may specify scope schemas for more specific identity tokens and introduce low-cost decentralized identities to a multi-chain environment."
  },
  {
    "objectID": "documents/research/posts/ERFC-147.gfm.html#general-purpose-token-verification-service-gptvs",
    "href": "documents/research/posts/ERFC-147.gfm.html#general-purpose-token-verification-service-gptvs",
    "title": "OWT - Omni Web Token",
    "section": "General Purpose Token Verification Service (GPTVS)",
    "text": "General Purpose Token Verification Service (GPTVS)\nThe entire verification protocol can be summed up into a token verification smart contract that can be deployed and used by multiple users to verify the authorization tokens for their smart contracts. Blacklisting can be introduced for tokens or clients. This service should allow listing approved issuers and smart contract addresses from which the requests may come. Also, the service can be monetized by requiring a certain amount of Ethers or ERC-20 tokens to be submitted monthly by the users to the verification smart contract, or the service will become unavailable for the requests coming from the user’s smart contracts."
  },
  {
    "objectID": "documents/research/posts/ERFC-147.gfm.html#appendix-1-chain-token-verification-smart-contract",
    "href": "documents/research/posts/ERFC-147.gfm.html#appendix-1-chain-token-verification-smart-contract",
    "title": "OWT - Omni Web Token",
    "section": "Appendix 1: Chain token verification smart contract",
    "text": "Appendix 1: Chain token verification smart contract\n\n// SPDX-License-Identifier: GPL-3.0\npragma solidity >=0.7.0 <0.9.0;\n\ncontract VerifierContract {\n        mapping(address => uint256) whitelisted;\n        mapping(bytes32 => bool ) usedPaymentIds;\n        bytes prefix = \"\\x19Ethereum Signed Message:\\n32\";\n        uint32 genericIdentityScope = 1;\n        uint32 payoutScope = 2;\n\n        function checkSignature(bytes32[3] calldata token, address signer) public returns (bool) {\n                // Decode r, s, v values\n                bytes32 hash = token[0];\n                bytes32 sv = token[2];\n                bytes32 r = token[1];\n                bytes32 s = sv & bytes32((uint((1 << 255) - 1)));\n                uint8 v = uint8(uint(sv >> 255) + 27);\n\n                // Create signature hash\n                bytes32 prefixedProof = keccak256(abi.encodePacked(prefix, hash));\n\n                // Verify signer\n                address recovered = ecrecover(prefixedProof, v, r, s);\n                return recovered == signer;\n        }\n\n        function whitelist(address client, uint256 amount) public {\n                whitelisted[client] = amount;\n        }\n\n        function payoutOld(uint256 amount) public {\n                require(amount <= (whitelisted[msg.sender]));\n                whitelisted[msg.sender] -= amount;\n        }\n\n        function payoutNew(address sender, uint256 amount, bytes32 paymentId, bytes32[3] calldata token) public{\n                // Check if token has already been used\n                require(usedPaymentIds[paymentId] == false);\n\n                // Check token expiration\n                uint32 exp = uint32(uint256(token[0]));\n                require(exp > block.timestamp, \"Token expired\");\n\n                // Check token signature\n                require(this.checkSignature(token, sender) == true, \"Invalid signature\");\n\n                // Check token values\n                bytes32 prefixedProof = keccak256(abi.encodePacked(sender, address(this), msg.sender, payoutScope, exp, paymentId, amount));\n                require (bytes32((uint256(prefixedProof >> 32 << 32) | uint256(exp))) == token[0]);\n\n                usedPaymentIds[paymentId] = true;\n        }\n\n        function payoutNewShort(address sender, uint256 amount, bytes32[3] calldata token) public{\n                // Check if token has already been used\n                require(usedPaymentIds[token[0]] == false);\n\n                // Check token expiration\n                uint32 exp = uint32(uint256(token[0]));\n                require(exp > block.timestamp);\n\n                // Check token signature\n                require(this.checkSignature(token, sender) == true);\n\n                // Check token values\n                bytes32 prefixedProof = keccak256(abi.encodePacked(sender, address(this), msg.sender, payoutScope, exp, amount));\n                require (bytes32((uint256(prefixedProof >> 32 << 32) | uint256(exp))) == token[0]);\n\n                usedPaymentIds[token[0]] = true;\n        }\n\n        function verifyIdentity(address issuer, bytes32 userId, bytes32[3] calldata token) public {\n                // Check token expiration\n                uint32 exp = uint32(uint256(token[0]));\n                require(exp > block.timestamp);\n\n                // Check token signature\n                require(this.checkSignature(token, issuer) == true);\n\n                // Check token values\n                bytes32 prefixedProof = keccak256(abi.encodePacked(issuer, address(this), msg.sender, genericIdentityScope, exp, userId));\n                require (bytes32((uint256(prefixedProof >> 32 << 32) | uint256(exp))) == token[0]);\n        }\n}"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "3327 Research",
    "section": "",
    "text": "Welcome to our Research page\n\nCrazy. Bold. Forward.\n\nBADASS PEOPLE INSPIRED BY A BADASS SCIENTIST\n\n\nR&D digital collective working on cutting edge web3 technologies\n\n3327 is an open, distributed, and autonomous collective. Some may call it a DAO, but we tend to say we are already past that point.\n\n\n\n\n3327 is a celebration of the greatest geek who ever lived. Our collective is named after the NYC hotel room where Nikola Tesla spent his last decade, working on his crazy, bold, and progressive innovations.\n\n\n\nVisit 3327 Website\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s Nerd Out Together\n\n\n\nResearch"
  }
]