[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "documents/research/posts/ERFC-57.gfm.html",
    "href": "documents/research/posts/ERFC-57.gfm.html",
    "title": "Solidity++ (S++)",
    "section": "",
    "text": "Executive Summary\nWriting efficient code in modern languages is mostly reflected in writing efficient algorithms and business logic. Writing efficient code in Solidity is mostly reflected in thinking as an assembler, moving focus from logic implementation to memory optimisations and low level hacks. Highly optimised code may become unreadable and unmaintainable with possible bugs hidden between the lines of bit level operations. To solve this problem, this project proposes multiple automatic optimisation techniques that will be all summed up into a transpiler and Solidity language extensions called Solidity++.\n\n\nIntroduction\nJavascript introduced classes in ES6 which are very useful construct for writing clean and readable code. Browser, however, could not understand ES6 code so the code needed to be transpiled into ES5. Programmers could use classes to write logic and transpiler took care of transforming the code into an optimised ES5. The same parallel could be made with Solidity. Programmers should be focused on writing business logic in Solidity-like S++ code and the code should be automatically transpiled into efficient Solidity code. S++ should not drastically modify basic Solidity syntax, but introduce new annotations and helper functions that would enable efficient transpiling to pure Solidity code.\nThere are multiple ways to improve the smart contract code on bytecode level. This project will focus on improvements on Solidity code level but the end product - optimised Solidity code will be compiled into bytecode and further optimisations on bytecode level can be performed.\nOptimisations on code level include variable declaration ordering (state variables, local variables and structs), which correlates with the order of memory block stacking and directly influences the costs of storing the data. Some variables might occupy more space than needed to store values in ranges that require less bits than the closest Solidity data type. The most extreme example is boolean data type which stores true/false values, that could be stored in 1 bit of memory, but really use an entire byte. Accessing storage structs requires more operations than accessing a local variable, which becomes obvious problem when it is done in loops. Any fixed sized data type, like, int64, bytes32 and so on, are always cheaper than dynamic data types, such as string, and those dynamic types should be replaced with fixed ones wherever is possible (wherever the value range of the variable is known). Cost reduction can even be achieved by deleting variables - freeing blockchain space. Execution of a code that performs delete command on some variable/mapping value/… rewards executor with a refund of up to 50% of transaction cost, depending on the amount of freed space. There were some attempts to remove this feature (https://eips.ethereum.org/EIPS/eip-3298) but so far it is still valid and exploitable. Usage of lazy evaluation is, an in other languages, preferred way of saving execution steps, which for Solidity directly equals saved money.\n\nExisting solutions\nThere are optimisations on loop level [1] that do not include variable level optimisations but recognise code patterns that are classified into several categories that can be automatically optimised. Also, there are papers [2] that uncover additional space for optimisation, such as removing conditions that always equal the same value or values in the loops that never change. There are also blog advices for writing optimised smart contracts, such as [3], that can and will be referenced when implementing automatisation methods.\n\n\n\nGoals & Methodology\nSo far, there is no tool or IDE that either automates these optimisations or includes multiple improvements in one software. The goal of this project is to evaluate feasibility of implementing such tool and Solidity language extensions that could allow its easier implementation. End PoC toll planned as a result of this project focuses on space optimisation techniques and includes: * Implementing parser for Solidity language that enables language semantic and syntax analysis * Implementing optimisation methods: * Variable declaration reordering * Struct values declaration reordering * Defining data type extensions for storing custom-sized data (i.e. integers with values between 20 and 40) and implementing language parser extensions * Implementing generator for optimised code\n\n\nResults & Discussion\nProto-parser for Solidity code, which includes basic PoC constructs was implemented, as well as variable declaration reordering to enable more efficient space usage; The plan is to extend the parser and to include other optimisation techniques listed in the previous segment.\n\nVariable declaration reordering\nVariable declaration reordering is a problem synonymous to binning problem, where the goal is to pack objects of a certain size into bins of fixed size in any order using the least number of bins. The problem is a known optimisation problem which comes from NP (non-deterministically polynomial) class and has no known efficient solution. The approach for implementing variable declaration reordering was using branch-and-bound algorithm which resulted in decent reordering time of less than 10 seconds for up to 16 variables in one block. The reordering was performed per block. The same algorithm will be performed on structs.\n\n\nData type extensions\nTo enable extensions of data types there are two steps: * Defining language extensions * Map variables to bits of memory blocks and create getters and setters\nNew data types will be mapped to memory blocks, variables of 256 bits, on bit level using bit masks. Packing of those values will be optimised using the previously defined variable stacking algorithm\nProposed language extension of integer data types consists of using the existing base type (int, uint) followed by value range given in brackets. Example custom type uint<10, 1030> represents unsigned integer values between 12 and 98. The optimiser would determine the closest known data type that covers the range (in this case uint16). The real number of bits required to store value 1030 is 11 and the closest data type is 16 bits long which means that 5 bits are redundant. First level of optimisation may be using 11 bits mapped to a memory block of 256 bits and implementing getters and setters to interact with the value, which is casted to uint16 when used. But there is another hidden optimisation that can further improve space management. Notice that the upper limit of the interval is indeed 1030 which required 11 bits to store but the lower limit is 10 which means that total number of values is 1030 - 10 = 1020 and that many values can be stored in 10 bits of memory, so the optimisation will take into account the overall interval range and generate getters / setters that will include offsets before storing/loading values.\nBoolean type optimisation will be straight forward and, while it will keep the type name, it will be mapped to only one bit of memory.\nString data types would be extended with indicator of the number of characters as string<10> will represent fixed sized string that will be mapped to bytes.\n\n\nCode generation\nThe parser generates three-like structure of the Solidity code. Every statement or declaration is stored as an object with parameters related to the nature of the statement/declaration. For example, declaration uint256 private var1 is stored as VariableDeclaration object with attributes type=uint256, modifier=private identifier=var1. Each object contains information to reconstruct itself back to string from. This type of organisation enables easy traversal through code, easy recognition of higher-level constructs, such are infinite loops, and easy reconstruction to Solidity code (in this case - optimised code).\n\n\n\nConclusion\nThe feasibility of the proposed project is confirmed by implementing PoC prototype, the optimisation evaluation on the existing smart contracts is yet to come but even the simple reordering of variables that reduces the number of used memory blocks clearly shows benefits of money savings (saved memory) and time savings (eliminated thinking about such generic problem that can and should be automatised). The borderline is - it makes no harm to use it, it can only be an improvement.\nThe value of this project is assumed, but still has to be confirmed. That is why the next step of this research should be further assessment by target user group, which should include developers that are actively using Solidity in their everyday work and domain experts. # Appendices\n\n\nBibliography\n[1] B Mariano, Y. Chen, Y. Feng Demystifying Loops in Smart Contracts https://fredfeng.github.io/papers/ase20-consul.pdf\n[2] T.Brandstaetter Optimisation of Solidity Smart Contracts https://repositum.tuwien.at/bitstream/20.500.12708/1428/2/Brandstaetter%20Tamara%20-%202020%20-%20Optimization%20of%20solidity%20smart%20contracts.pdf\n[3] https://mudit.blog/solidity-gas-optimization-tips/"
  },
  {
    "objectID": "documents/research/posts/ERFC-38.gfm.html#executive-summary",
    "href": "documents/research/posts/ERFC-38.gfm.html#executive-summary",
    "title": "Building on Filecoin and Filecoin Proofs",
    "section": "Executive Summary",
    "text": "Executive Summary\nWhen looking to build dApps that utilize decentralized storage Filecoin seems like the best option even with its flaws like : absent proof of deletion for the client, absent encryption, impossible modifying of the stored data and the durability problem of Filecoin’s Proof of Replication. This research gives an overview of competitors in decentralized storage solution field with a focus on Filecoin protocol. It also shows how its competitor Storj differs from Filecoin and the current grant opportunities for potential building on it. Considering the results of the research Filecoin seems to be the best option considering the popularity and the size of its ecosystem. Currently there is no interest for building on and with Filecoin and this is a purely explorative work without experiments, in order to test the researcher’s methodology and the approach to research. However, it proposes a question about a potential way of improving Filecoin, or creating both safer protocol for the user and cheaper for the storage miners."
  },
  {
    "objectID": "documents/research/posts/ERFC-38.gfm.html#goals-methodology",
    "href": "documents/research/posts/ERFC-38.gfm.html#goals-methodology",
    "title": "Building on Filecoin and Filecoin Proofs",
    "section": "Goals & Methodology",
    "text": "Goals & Methodology\nThe aim of this research is to explore Filecoin protocol and show its fallacies as much as writer is possible with a goal of inspiring building applications that utilize Filecoin, new protocols or improving Filecoin via building for it. The paper will compile the list of all fallacies and possibilities for improvement for Filecoin, opportunities for building on it, and the current state of decentralized storage market mainly comparing Storj and Filecoin."
  },
  {
    "objectID": "documents/research/posts/ERFC-38.gfm.html#introduction",
    "href": "documents/research/posts/ERFC-38.gfm.html#introduction",
    "title": "Building on Filecoin and Filecoin Proofs",
    "section": "Introduction",
    "text": "Introduction\nUnlike a centralized server operated by a single company, decentralized storage systems consist of a peer-to-peer network of user-operators who hold a portion of the overall data. The platforms can store any data sent by the user, with some platforms more focusing on encryption. There is no official data disclosing the types of data stored. This research will be covering contract-based persistence platforms with a focus on Filecoin.\nContract-based persistence means that data cannot be replicated by every node and stored forever, and instead must be upkept with contract agreements. These are agreements made with multiple nodes that have promised to hold a piece of data for a period of time. They must be refunded or renewed whenever they run out to keep the data persisted. Platforms with contract-based persistence currently present on the market are:\n\nFilecoin\nSkynet\nStorj\n0Chain\n\nFilecoin is a peer-to-peer network that stores files with built in economic incentives to ensure files are stored reliably over time. Users pay to store their files on storage providers. Storage providers are computers responsible for storing files and proving they have stored the files correctly over time. Available storage and the price of it is not controlled by any company. Anyone who wants to store their files or get paid for storing other users files can join Filecoin is written in its documentation, but is that really the case? It will be further explored in later sections. Filecoin’s native currency is FIL. Storage providers earn units of FIL for storing user’s data. Its blockchain records transactions along with proofs from storage providers that they are storing files correctly.\nCurrently Filecoin stores over 40.0453 PiB of users data over 1,848,292 deals.1\nWhen users want to store their files on Filecoin they use terminal or different guis that have been built by developers to choose between cost, redundancy and speed and they select the storage provider whose storage offer is best suited for their needs. Applications that implement filecoin negotiate storage with storage providers. There is no need for different API for each provider.2\n“While interacting with IPFS does not require using Filecoin, all Filecoin nodes are IPFS nodes under the hood, and (with some manual configuration) can connect to and fetch IPLD-formatted data from other IPFS nodes using libp2p. However, Filecoin nodes don’t join or participate in the public IPFS DHT. IPFS alone does not include a built-in mechanism to incentivize the storage of data for other people. This is the challenge Filecoin aims to solve. Filecoin is built on IPFS to create a distributed storage marketplace for long-term storage.”3"
  },
  {
    "objectID": "documents/research/posts/ERFC-38.gfm.html#filecoins-proof-system",
    "href": "documents/research/posts/ERFC-38.gfm.html#filecoins-proof-system",
    "title": "Building on Filecoin and Filecoin Proofs",
    "section": "Filecoin’s Proof System",
    "text": "Filecoin’s Proof System\nFilecoin uses Proof of Replication (PoRep) and Proof of Spacetime (poSt).\nIn a Proof of Replication, a storage miner proves that they are storing a physically unique copy, or replica, of the data. Proof of Replication happens just once, at the time the data is first stored by the miner. As the storage miner receives each piece of client data they place it into a sector, fundamental unit of storage in Filecoin. Sectors can contain pieces from multiple deals and clients. Steps in PoRep:\n\nProof of Replication\n\nFilling sectors and generating the Commd\n\nOnce a sector is full a Commitment of Data (CommD) is created, representing the root node of all the piece CIDs contained in the sector.\n\nSealing sectors and producing the Commitment of Replication\nSector data is encoded through a sequence of graph and hashing processes to create a unique replica. The root hash of the merkle tree of the resulting replica is called CommRLast. CommRLast is then hashed together with the CommC(another merkle root output from PoRep). This generates the CommR (Commitment of Replication) which is then recorded on Filecoin’s Blockchain. CommR, last is saved privately by the miner for future use in Proof of Spacetime but is not saved to the chain.\nEncoding process is slow and computationally heavy. Filecoin doesn’t provide encryption by default so users must encrypt data before adding it to the Filecoin network. This is the first issue encountered with Filecoin : Filecoin is optimized for public data and doesn’t yet support access controls.\nThe CommR offers clients the proof that the miner is storing a physically unique copy of the client’s data. If a client stores the same data with multiple storage miners, or makes multiple storage deals for the same data with the same miner, each deal will yield a different CommR. The sealing process also compresses the Proof of Replication using zk-SNARKs to keep the chain smaller so that it can be stored by all members of the Filecoin network for verification purposes.\nUnlike PoRep which is run once to prove that a miner stored a physically unique copy of the data at the time the sector was sealed. PoSt is run repeteadly to prove that the miners are continuing to dedicate storage space to that same data over time.4\nProof of Spacetime\nPoSt builds on several elements created during PoRep: the replica, the private CommRLast and public Commr. PoSt then selects some leaf nodes of the encoded replica and runs merkle inclusion proofs on them to prove that the miner has the specific bytes that indicate that he still holds the clients data. The miner then uses the privately stored CommRLast to prove that they know of a root for the replica which both agrees with the inclusion proofs and can be used to derive the CommR. As the final step PoSt compresses these proofs into a zk-Snark.\nIf the miners fail the Proof of Spacetime at any point they will lose their staked collateral. Aside for this fine, there is no other incentive to keep the miners storing the data. That becomes a problem if client’s storing private data or data of great significance.5"
  },
  {
    "objectID": "documents/research/posts/ERFC-38.gfm.html#results-discussion",
    "href": "documents/research/posts/ERFC-38.gfm.html#results-discussion",
    "title": "Building on Filecoin and Filecoin Proofs",
    "section": "Results & Discussion",
    "text": "Results & Discussion\n\nCritique From the users perspective:\nAs of today the price of 1 Filecoin token (FIL) is $22.05. The users can store a gigabyte of data for as little as 0.0000006157315278020465 FIL (0.01% the cost of Amazon S3) which means the user can store 100GB of data for $0.00135.6 From the price perspective the upside of Filecoin is it’s cheap storage but there are various downsides such as:\n\nAccesibility: if the user is not tech-savvy there is a big barrier to entry even with GUIs currently available. They are often not simple to install and it is hard to get them to work. Hovewer there are various Apps that make that somewhat easier like ChainSafe Files and Web3.Storage.When it comes to being a storage miner there minimal needs for hardware are:\n\n\n8+ core CPU\n128 GiB of RAM atleast\nA strong GPU for SNARK computations\n1TiB NVMe-based disk space for cache storage is recommended\nThis data shows that there needs to be a significant investment on storage miners part which is nothing out of the ordinary but significantly reduces the accessibility to the average person, ofcourse assuming the person wants to be a storage miner.\n\n\nAs the price of FIL tokens fluctuate the price of storage fluctuates as well. There is also a risk if any extra FIL is left in the customers wallet after the storage contract then token could potentially drop/rise in value, not to mention the fees of converting fiat into cryptocurrency.\nThere is no built in encryption. Users need to encrypt their data on their own. Encryption/decryption of files cost compute resources (RAM, CPU), and therefore money. Most end users would prefer the implementation of this functionality to be handled, optionally, by the Filecoin web, cli or desktop client software they choose to make use of. This problem is seemingly taken care of with apps like ChainSafe Files.7\nChainSafe Files is an online platform to store, view, and share files. Its main focus is data privacy of the users and self-reliance. When it comes to self-reliance ChainSafe Files makes sure that the users can access the stored files even if the Files platform becomes unavailable. It also offers authentication flow using a decentralized login provider called tKey, by Torus. tKey is a private key generator that can link keys to social accounts among other functions. The Files’ backend is built on top of ChainSafe Storage, and any file that is uploaded to Files is also pinned by a node on its infrastructure (each file has an CID) thus making the data retrieval possible even in the case of ChainSafe files app outage. In the case of retrieval users still have to decrypt the data, since all the data stored by ChainSafe Files is encrypted by default.8\nIf the storage provider doesn’t respect his end of the deal he will be penalized and lose his staked FIL. Unless negotiating a great number of deals for the same data and storing a lot of copies in Filecoin, there is no guarantee that the data will be safely stored. These deals for the same data increase the cost of the service if the user wants to have somewhat durable data. Filecoin tries to mittigate this by having storage miners put 100+ FIL in collateral which also lowers the accessibility to the average person to become a storage miner. Currently most of the storage miners are located in China.\nFilecoin storage is cold storage. There is no way to modify data. If the user needs to change data , new data must be written.9\nIf the user issues a deletion command there is no guarantee that the client performs the operation. There is no way yet to Construct a formal Proof of Deletion.\n\n\n\nThe issue with Replication and Filecoin\nWhen decentralized storage network is utilized any storage node could go offline thus the stored data would be at risk of getting lost. To achieve a somewhat reliable storage many decentralized providers use replication, which means the only way to keep the users data reliably besides penalizing storage miners is to store multiple copies of it. Replication is not good for the network expansion factor. If Filecoin wants more durability for its data it needs more copies. For every increase of durability (storing or repairing the data) another multiple of the data size in bandwith is needed. Eg. If the durability level requires a replication strategy that makes 10 copies of the data this yields and expansion factor of 1000%. This data needs to be stored on the network, using bandwith in the process. The more replication the bigger the bandwith usage. Hovewer, if the node goes offline , only one of the storage nodes is needed to bring a new replacement node in, which again means that the 100% of the replicated data must be transferred. Excessive expansion factors produce an inefficient allocation of resources.10\nAnother issue with replication is churn (nodes joining and leaving the network). Quoting Patrick Gerbes and John Gleeson: “Using replication in a high-churn environment is not only impractical, but inevitably doomed to fail. Distributed storage systems have mechanisms to repair data by replacing the pieces that become unavailable due to node churn. However, in distributed cloud storage systems, file repair incurs a cost for the bandwidth utilized during the repair process. Regardless of whether file pieces are simply replicated, or whether erasure coding is used to recreate missing pieces, the file repair process requires pieces to be downloaded from available nodes and uploaded to other uncorrelated and available nodes.”11\nCurrently the circulating supply of FIL token is 162,302,978.00 FIL. The potential circulating FIL could reach 1.977 Billion tokens if the network hits a Yottabyte of storage capacity in under 20 years which is brave considering the current data stored in data centers today is less then a Zettabyte and a Yottabite is 1000 times larger. The 770 Million of which is for baseline minting.\n330 million FIL tokens are released on a 6 year half-life based on time. A 6 year half-life means that 97% of these tokens will be released in aproximately 30 years. This amount is amount is minted to provide counter pressure to shocks.\nAnother 300 million FIL is held back in reserve to incentivize future types of mining. How they are released is up to the Filecoin community.12\n\nFigure 1: Maximum and Minimum Minting from Storage Mining.\n\nFigure 2: Network Storage Baseline for Max Baseline Minting on Log Scale.\nAs of today 29,180,207.338966275 FIL has been slashed.This means that if a network participant misbehaves, part of their FIL collateral or potential FIL rewards is confiscated and burned. FIL is also slashed for various other reasons.\nConsidering the token release we can expect total supply of FIL token to be almost doubled in the next five years. If we look the rate of slashing so far the dilluting process will exceed the tokens burned. This makes specullating Filecoin token price risky, both for the investors and users.13"
  },
  {
    "objectID": "documents/research/posts/ERFC-38.gfm.html#filecoin-and-storj-comparission",
    "href": "documents/research/posts/ERFC-38.gfm.html#filecoin-and-storj-comparission",
    "title": "Building on Filecoin and Filecoin Proofs",
    "section": "Filecoin and Storj comparission",
    "text": "Filecoin and Storj comparission\nWe will compare Filecoin and Storj by: 1. Consensus Algorithms\nFilecoin’s consesus mechanism has been covered in the paragraphs above.\nStorj does not have its own chain: the platform is built on Ethereum(currently built on PoW). The reason they decided to build on the Ethereum network is because of simplicity of using it as a method exchange. From the start Storj was never intented to be a true decentralized storage network.\n\nBlock time\nFilecoin’s block time is thirty seconds on average.\nStorj is the only “decentralized storage” solution to not employ their own chain. Storj decided to use the Ethereum network due to the easy deployment of a coin on it. Because of this decision, the block time is twelve seconds and needs to deal with the consequences of sharing blockchain with other projects.\nEnforcement of data retention\nDue to the PoSt mentioned in the paragraphs above the entire network is designed around data retention. In the case of miner failing to keep his promise , the only backup of the user’s data was irreversibly lost since Filecoin doesn’t use redundancy which is also mentioned in the paragraphs above.\nIn Storj the actual enforcement of data isn’t clearly documented. Each Satellite (the interface between the storage operators and the clients) does the enforcement of data retention all on their own. Each Satellite has its own subset of storage nodes that it knows have a good reputation and it trusts, it uses these hosts to upload data to. Then it, at regular intervals, checks random data segments with “Berlekamp-Welch” error correction to make sure that the data is still there. If they fail to prove they store the data the reputation of that host is changed and data migrates to a new host. There is no chain-level enforcement for data retention.\nContent distribution\nIn Filecoin data has to be sealed to be counted as a provable storage to the chain and because it is computation heavy it isn’t practical for the miners to serve data. 1 MiB file can take 5 to 10 minutes to unseal and 32 GiB file takes 3 hours on minimum hardware requirements mentioned above. To battle this Filecoin introduced a method to store cached and unsealed versions of data while storing the same data as sealed in order to provide proofs. This leads to the issue of miner storing the double amount of data while also posing the problem that unless the data is frequently accessed the miners will not store it because it isn’t profitable for them. That makes creating Google drive equivalent on Filecoin not practical because the data is not frequent enough to makes sense to cache while also being low latency (because of slow sealing and unsealing).\nIn Storj’s case data is being accessible only through the S3 gateway of centralized data data Satellites. Users can transform any data to public data and can send anyone a link to that data.\nSector size\nSector size is the minimum amount of data that can be sent to host and paid for. Filecoin has a fixed 32 GiB sector size. For each 256b stored 2b are proofs. Which means 1% of storage paid for is for proof. Also everything sent must be in a .car file which can be computationally heavy on the client-side.\nIn Storj’s case the sector size is not really clear. Current object fee is $0.0000022 per file stored. Which means that if there is a large amount of small files stored that would bring extra costs to the user.\nDecentralization\nBecause of the Filecoin’s Hardware requirements for hosts that means not everyone can run a storage node. On the other networks basically anyone can run a storage node( most minimal requirements are 2 cores 8GB of RAM and some storage) but in Filecoin only people with sufficient funds for initial investments can run hosts which reduces the spread of the network. Which makes us question the actuall decentralization of the network.\nStorj isn’t decentralized. The blockchain nature of the Storj coin is only designed for for efficient transacting not decentralized enforcement of data retention which means that Storj is actually a distributed storage provider but not fully decentralized.14"
  },
  {
    "objectID": "documents/research/posts/ERFC-38.gfm.html#filecoin-grants",
    "href": "documents/research/posts/ERFC-38.gfm.html#filecoin-grants",
    "title": "Building on Filecoin and Filecoin Proofs",
    "section": "Filecoin Grants",
    "text": "Filecoin Grants\nFilecoin offers various types of grants for building with Filecoin.\n\nNext Step Microgrants\nFilecoin offers grants of 5000$ in FIL to support taking the next step when the initial prototype is created. Their purpose is financing projects in the early stage. Acceptance criteria is simple. Projects must meet these criteria:\n\nApplicant has already built something with Filecoin (or closely related technologies such as IPLD, libp2p, or frameworks or services such as NFT.storage, Textile Powergate, etc.), independently or as part of a course or hackathon.\nApplicant must provide clear description of the Next Step after grant support\nThe project can be completed within 3 months.\nProject must be open-sourced.\nThe applicant must complete weekly updates and a grant report upon conclusion.\n\nProjects that qualify for Microgrants: 1. Projects that publish data or files to IPFS 2. Projects that don’t use IPFS directly 3. Projects that save data or retrieve data from the Filecoin Network 4. Non-coding projects, videos, tutorials etc\nThese grants are offered on the quarterly basis.\n\n\nOpen Grants\nFilecoin’s focus areas currently are: 1. Core development - core protocol research, specification and implementation work 2. Application Development - applications that utilize Filecoin as a decentralized storage layer 3. Developer tools and libraries - tools and libraries for protocol developers and application developers 4. Integration and adoption - integration into existing app or projects with significant usage 5. Technical design - improvement proposals for the core storage protocol 6. Documentation 7. Community building 8. Metaverse - experiences, applications, communities, tooling, standards, infrastructure, et cetera15 9. Research that explores Filecoin and decentralized storage16"
  },
  {
    "objectID": "documents/research/posts/ERFC-38.gfm.html#conclusion",
    "href": "documents/research/posts/ERFC-38.gfm.html#conclusion",
    "title": "Building on Filecoin and Filecoin Proofs",
    "section": "Conclusion",
    "text": "Conclusion\nWhen it comes to decentralized storage solutions Filecoin draws all the attention compared to its competitors like Storj and Sia. Storj solves Filecoin’s replication problem, on the expense of decentralization but the internet search statistics still shows that Filecoin is the main contender in Decentralized storage.\n\nFigure 3. Filecoin (blue) vs Storj (red) search interest in the past 12 months.\nEven with unclear tokenomics a nd problematic durability to the writer of this research, building with Filecoin seems like the best solution when building dApps that utilize decentralized storage because of its low cost and grant opportunities, even though storage miners are somewhat centralized. If there is a way to improve current storage miner’s/clients experience on Filecoin both in cost and ease of use that would surely be a great grant opportunity. Also there could potentially be a way to draw them to a protocol that is a cheaper alternative. Ofcourse, these are all assumptions, since much of the data on these protocol is unclear and not available."
  },
  {
    "objectID": "documents/research/posts/ERFC-37.gfm.html#ronin-sidechain",
    "href": "documents/research/posts/ERFC-37.gfm.html#ronin-sidechain",
    "title": "Gaming DAPPs - Play to Earn model",
    "section": "Ronin sidechain",
    "text": "Ronin sidechain\nThe game was developed on Ethereum in 2018 by the Sky Mavis company located in Vietnam. Due to high Ethereum fees, the game’s creators moved to their own EVM compatible sidechain called Ronin which uses Proof of Authority with validators being chosen by the company5 .\nTo start playing a player needs to :\n\ncreate their own Ronin wallet\ncreate their user account on the Axie Marketplace and connect their wallet to it\ntransfer some amount of ETH and buy at least 3 Axies with a floor price of ~42 dollars per Axie\ninstall a PC or Mobile app and login with their user account\n\nDisregarding the difficulty of the onboarding process, the whole point of a blockchain game is to have the players take actions on the blockchain. Some arguments could be made that at the time the game was developed this was needed but with the introduction of Ronin it is unclear why the whole game was not ported to it.\nThe only actions that are taken on the Ronin sidechain are dedicated to trading - buying, minting and gifting of Axies. There are no fees on Ronin, but the number of transactions per wallet address per day is limited. Deploying on Ronin, also, requires company’s permission so the development of new games and the whole Ronin ecosystem is slowed down."
  },
  {
    "objectID": "documents/research/posts/ERFC-37.gfm.html#scholarships",
    "href": "documents/research/posts/ERFC-37.gfm.html#scholarships",
    "title": "Gaming DAPPs - Play to Earn model",
    "section": "Scholarships",
    "text": "Scholarships\nMarket decides on the price of Axies and with a high entry cost of starting with the game, a new model of onboarding has emerged. Newcomers (scholars) can “rent out” the Axies for a certain period of time and negotiate with the owners (managers) the terms of the profit distribution. This is not a official in-game feature and is enabled by having the managers controlling the Ronin wallet and scholars controlling the user account associated with it. This leads to a bad position for the scholar as the profits are claimed by the manager and then the scholar’s share is sent to the their wallet’s address. Scholars are essentially at the mercy of the managers as their earnings and the scholarship itself can be revoked at any time.\nMore fairer way of enabling Scholarships, would be to have a smart contract that would have complete control of the Axies and through which the scholars would make in-game actions. The profits would go to the contract’s address that would perform a fair split. The terms of the agreement (the minimum amount of profits to be earned by the scholar) and the scholarship’s time period would be embedded in the contract. If both parties agree, the terms could be changed later on."
  },
  {
    "objectID": "documents/research/posts/ERFC-37.gfm.html#game-breakdown",
    "href": "documents/research/posts/ERFC-37.gfm.html#game-breakdown",
    "title": "Gaming DAPPs - Play to Earn model",
    "section": "Game Breakdown",
    "text": "Game Breakdown\nThe game itself is organized in 1v1 matches where players do not have any influence against who they will get matched. Matching is done by the server based on players’ Match Making Rankings (MMR) which is determined by the win/lose ratio of those players. All of this could be implemented in a smart contract which would keep track of all of the players’ MMRs and update them after each match. Players could start a new match or join an existing one if the absolute difference in their MMRs is under a certain threshold. The benefits of this approach is that they could also choose what match they will join or challenge a specific player.\nWinner of each match gets some amount of game’s “Smooth Love Potion” (SLP) tokens. That amount is dependent on the MMR of that player (the higher the MMR the more SLP tokens they will win). SLP is an inflationary ERC20 token that gets minted after each match. Even though there are SLP burning mechanisms through some in-game actions, most players opt to cash out their winnings so this might not be a viable economic model. One alternative would be to have both players stake some amount of tokens in a match with the winner taking the sum of those stakes. The problem is that it introduces betting connotations and games go to extreme lengths in order to not be considered a betting game as it potentially introduces regulation.\nInside the game, there is also “Axie Infinity Shard” (AXS) token. AXS is an ERC20 token which has a fixed total supply. The company behind the game has roughly 20% of the total AXS supply and small amounts of AXS is distributed to the top players of the month.\nThe game makes heavy use of Axies which have certain characteristics. Players do not completely own Axies as it was discovered that the company can freeze them, making them useless. Once an Axie is frozen, it cannot be used in the game nor it can be traded on the Axie Marketplace. This possess a major concern as the players’ assets are constantly under a threat of those players being banned from the game by the Sky Mavis company. Full list of violations that will result in a ban can be seen here6 .\nEach Axie has one of the 9 classes, 4 statistics and 4 cards associated with it. Classes are grouped into three groups that form a “rock paper scissor” relationship. Meaning that, group G1 does 15% extra damage to group G2 but takes 15% extra damage when attacked by group G3. Statistics determine the Health, Skill, Speed and Morale of an Axie. This statistics affect the matches and their state transitions as the Speed for example determines the order of attacks. Cards can have positive or negative effects on an Axie as well additional effects that affect the players. Four cards associated with an Axie are added to the player’s deck when that Axie is used inside a match. More information about Axies is provided in Appendix A and the way new Axies are created is provided in the Breeding section of the Whitepaper7 ."
  },
  {
    "objectID": "documents/research/posts/ERFC-37.gfm.html#matches",
    "href": "documents/research/posts/ERFC-37.gfm.html#matches",
    "title": "Gaming DAPPs - Play to Earn model",
    "section": "Matches",
    "text": "Matches\nOne match consists out of:\n\nBoth players choosing a team of 3 Axies and deciding on their positions on the field (creating team formations)\nRounds being played out until one of the players doesn’t have a standing Axie\n\n\n\n\n\nAxieMatch\n\n\nPicture 1 : Axie Infinity Match View\n\n\nTeam formations\nThe positioning of an Axie matters because it determines what Axie will take the damage from the enemy. Each player has 5 rows where an Axie can be positioned. The closest row to the enemy lines will be attacked first. The Axies cannot change their position and they stay where they initially were until they get knocked out. Choosing of a team formation would require two transactions per player. Those transactions would be organized in the commit-reveal scheme so that the players wouldn’t have an advantage against the opponent that naively sent the transaction revealing their team and their positions.\n\n\nRounds\nEach Round is carried out in the following order:\n\nPlayers randomly draw 3 cards* from their own decks (consisting of 24 Cards**) and decide on what cards they will play\nCards are revealed and their affects are applied to Axies\nBattle of the Round takes place\nIf one of the players doesn’t have a standing Axie then the match is over\n\n* Exception is that in the first Round, players draw 6 cards.\n** There are 3 Axies per team, each Axie adds two copies for each of the 4 cards. So in total there are 3*2*4 = 24 cards in one player’s deck.\n\n\nDrawing of Cards\nOnce the teams have been revealed both players know each other decks. However, they should not know what cards the opponent has in their hands and so they should not know what cards they have yet to draw.\nOne scheme that could be applied is the following:\n\nHave both players choose their secret “random” number and commit to it with a hash\nHave the game’s contract ask for a random number from Chainlink’s VRF8\nOnce the randomness has been fulfilled, both players know what is the order of cards they will be drawing as the seed for shuffling their decks will be some function of their own number and the received number from Chainlink’s VRF\n\nThe drawing order of cards in the deck is now known only to the players. During the match, each player can play any card inside their deck and until a match ends, players trust each other that the played card was in the opponent’s hand. However, when the match ends both players would need to reveal their secret numbers as the contract needs to verify if they honored the drawing order and if the card they played at a certain moment was one of the cards they were holding. The first discrepancy would end the verification process and the cheater would be penalized.\nOne additional subproblem is that after each round the remaining cards inside the deck should be reshuffled. This could be done with requesting another random number which would help form a seed for random shuffling of the remaining cards. The verification process would need to be modified to support this.\nNote: after all of the cards have been drawn, the deck resets.\n\n\nReveal of the Cards\nThe reveal of the cards would also need two transactions per player so that one player wouldn’t just wait for the opponent to reveal their cards and then change their strategy accordingly.\nOnce cards have been revealed, before the battle takes place, the effects of the played Cards are applied. In total there are 3 constant effects (Attack damage, Defensive points and the cost of playing that Card) and 19 additional effects that a Card may have. For example, one of those additional effects are an increase/decrease in one of the Axie statistics. When multiple Cards that affect the same Axie are played, their effects are accumulated. More information about cards is provided in Appendix A .\n\n\nBattle of the Round\nBattle rules:\n\nthe order of attacks is determined by the highest Speed statistic (if there is a draw then it is decided by the lowest Health and then by lowest ID of an Axie)\nclosest row with a standing Axie to the enemy lines will take the damage\nif two Axies are inside the row taking the damage there’s a 50/50 chance which Axie will absorb the damage\n\nAll of this logic could be kept inside a contract. One of that things that would need to be taken into account is that Axies attack in the pre-defined order. So during a Round, an Axie that has not yet attacked could be knocked out of the game and so the Cards associated with it, that were played in the Round, should lose their effects. Also, when an Axie gets knocked out, all of their cards should be removed from the deck."
  },
  {
    "objectID": "documents/research/posts/ERFC-37.gfm.html#summary-of-proposed-matched-progression",
    "href": "documents/research/posts/ERFC-37.gfm.html#summary-of-proposed-matched-progression",
    "title": "Gaming DAPPs - Play to Earn model",
    "section": "Summary of proposed matched progression",
    "text": "Summary of proposed matched progression\n\nInital proposal\n\n\n\n\n\n\n\n\n\n\n\nStep\nPhase\nDescription\nNumber of transactions\nDone by\n\n\n\n\n1\nMatch Set Up\nStarting of a Match\n1\nPlayer 1\n\n\n2\nMatch Set Up\nJoining a Match\n1\nPlayer 2\n\n\n3\nMatch Set Up\nCommitting to a team formation and setting of a secret number\n2*1\nPlayer 1 and Player 2\n\n\n4\nMatch Set Up\nRevealing team formations\n2*1\nPlayer 1 and Player 2\n\n\n5\nRound\nCommitting to Cards that will be played in this Round\n2*1\nPlayer 1 and Player 2\n\n\n6\nRound\nRevealing Cards of the Round\n2*1\nPlayer 1 and Player 2\n\n\n7\nMatch Wrap Up\nRevealing secret numbers after the match has ended\n2*1\nPlayer 1 and Player 2\n\n\n\n\nTotal = (1+1+2+2) + N*(2+2) + (2) = 8 + 4*N ; where N is the number of rounds in the match\nEven with just 3 rounds the number of transactions per player would be 16, so this is a problem.\n\n\nImproved Match Progression\nSomething that could be done is the introduction of “fair play” where for one round’s commit stage, one player would transmit both of the commit messages to the chain after which the other player would transmit both reveal messages.\nIf the player transmitting commitment messages decides to not transmit them, then the match is stuck. However, the game can be structured in a way such that it is in the interest of both players to resolve the match regardless of the outcome. This could be done by staking some amount of tokens that the players will receive at the end of the match.\nIf the player transmitting the reveal messages realizes he will lose and decides to not transmit them, then the first player could transmit just their own reveal message that would start a countdown for the other player to transmit their reveal message. If the countdown is reached then the player that didn’t act in the spirit of fair play is penalized in some way.\n\n\n\n\n\n\n\n\n\n\n\nStep\nPhase\nDescription\nNumber of transactions\nDone by\n\n\n\n\n1\nMatch Set Up\nStarting of a Match\n1\nPlayer 1\n\n\n2\nMatch Set Up\nJoining a Match\n1\nPlayer 2\n\n\n3\nMatch Set Up\nTransmitting commitments to the team formations and setting of a secret number\n1\nPlayer 1\n\n\n4\nMatch Set Up\nTransmitting the Reveal of team formations\n1\nPlayer 2\n\n\n5\nRound\nTransmitting Commitment Messages\n1\nPlayer 1\n\n\n6\nRound\nTransmitting Reveal Messages\n1\nPlayer 2\n\n\n7\nMatch Wrap Up\nRevealing secret numbers after the match has ended\n1\nPlayer 1\n\n\n\n\nThis scheme would give the total number of transactions of:\nTotal = (1+1+2) + N*(2) + (1) = 5 + 2*N ; where N is still the number of rounds in the match\nWith the number of rounds equal to 3, the number of transactions per player is less or equal to 6.\nThis is an improvement but maybe it could be brought down further. Also, signing 6 transactions per match would still break the flow of the game, so the players would have to have a substantial financial interest in continuing to play the current match and the game itself."
  },
  {
    "objectID": "documents/research/posts/ERFC-37.gfm.html#appendix-a",
    "href": "documents/research/posts/ERFC-37.gfm.html#appendix-a",
    "title": "Gaming DAPPs - Play to Earn model",
    "section": "Appendix A",
    "text": "Appendix A\nEach Axie has a fixed set of attributes - a class, 4 stats and 4 cards.\n\n\n\n\nAxie\n\n\nPicture 2: Axie characteristics\n\n\nAxie Classes\n\n9 of them (Plant, Reptile, Dusk, Beast, Bug, Mech, Aqua, Bird, Dawn)\n6 main classes (Plant, Beast, Bug, Reptile, Aqua, Bird, Dawn)\n3 remaining are “secret” (and generally weaker in the game)\nto make the strengths of classes balanced, they are grouped in 3 groups, forming a “rock paper scissor” relationship\n\n\n\nAxie Stats\n\ncan be divided into “base” and “additional” Stats\ntotal of 140 points is distributed between 4 of the base Stats (with each class having its own base distribution)\n\nHealth — amount of damage Axie can take before getting knocked out\nSpeed — affects the order in which Axies attack in a match (higher Speeds attack first)\nSkill — increases damage dealt when the Axie performs multiple cards/moves (a.k.a. combo)\nMorale — increases chance to land a critical hit, as well as entering “last stand” which allows them to attack a few more times before getting knocked out\n\n\n\nBase Stats Distribution\n\n\n\nClass\nHealth\nSpeed\nSkill\nMorale\n\n\n\n\nAqua\n39\n39\n35\n27\n\n\nBeast\n31\n35\n31\n43\n\n\nBirds\n27\n43\n35\n35\n\n\nBug\n35\n31\n35\n39\n\n\nPlant\n61\n31\n31\n41\n\n\nReptile\n39\n35\n31\n35\n\n\n\n\n\nadditional Stats depend on Axie’s Body Parts\n\n\n\nAxie Body Parts\n\nthere are 6 of them (Eyes, Ears, Horns, Mouth, Back, Tail)\nonly Horns, Mouth, Back, Tail have an associated card with it (one card per body part)\nIn total, there are:\n\n4*6 types of Mouth\n6*6 types of Horns\n6*6 types of Back\n6*6 types of Tail\n\ninside groups of those types each of the 6 main classes is equally represented\nif the class of an Axie matches with the type of a body part, Axie’s stats will increase\n\n\nAdditional Stats Increase\n\n\n\nClass/Type\nHealth\nSpeed\nSkill\nMorale\n\n\n\n\nAqua\n+1\n+3\n0\n+0\n\n\nBeast\n0\n+1\n0\n+3\n\n\nBirds\n0\n+3\n0\n+1\n\n\nBug\n+1\n0\n0\n+3\n\n\nPlant\n+3\n0\n0\n+1\n\n\nReptile\n+3\n+1\n0\n0\n\n\n\n\n\nAn Axie is a ‘pure breed’ when its class and all of its body parts are of the same type.\n\n\n\nAxie Abilities (Cards)\n\nthere are 132 cards in total (first divided by 6 main classes and then by the body parts)\neach Axie has 4 cards associated with it\neach Card has\n\nan amount of Energy it costs to play it\nattack points - damage it does to the enemy Axie\ndefensive points - forms a shield that takes the damage instead of Axie’s Health\npotentially buffs/debuffs\nadditional effects (draw another card, steal some Energy from the opponent,…)\n\ncards realize a “combo” when 2 or more of them are played (for the same Axie)\ncards realize a “chain” when 2 or more Axies use 2 or more cards that are from the same class\n\n\n\nAxie Card Buffs/Debuffs (Card Effects)\n\nthere are 3 buffs (positive effects on the Axie) and 16 debuffs (negative effects on the enemy Axie)\nthey affect the Axie for one or more turns\nthey are “stackable” - their effects are accumulated\n\n\nList of Buffs\n\n\n\nName\nDescription\n\n\n\n\nAttack+\nIncrease next attack by 20%\n\n\nMorale+\nIncrease Moral by 20% for the following round\n\n\nSpeed+\nIncrease Speed by 20% for the following round.\n\n\n\nList of Debuffs\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nAroma\nTarget priority changes to this Axie until the next round.\n\n\nAttack-\nDecrease next attack by 20%.\n\n\nChill\nAffected Axie cannot enter Last Stand\n\n\nFear\nAffected Axie will miss their next attack\n\n\nFragile\nAffected Axie’s shield will take double damage from the next incoming attack\n\n\nJinx\nAffected Axie cannot land critical hits\n\n\nLethal\nNext incoming attack is a guaranteed critical strike\n\n\nMorale-\nDecrease Morale by 20% for the following round\n\n\nPoison\nAffected Axie will lose 2 HP for every card used\n\n\nSleep\nNext incoming attack will ignore shields\n\n\nSpeed-\nDecrease Speed by 20% for the following round\n\n\nStench\nAffected Axie will lose target priority for the following round\n\n\nStun\nAffected Axie’s first attack will miss.Next incoming attack will ignore shields\n\n\nCannot Be Healed\nThis Axie cannot be healed or recover health. This Debuff cannot be removed"
  },
  {
    "objectID": "documents/research/posts/ERFC-39.gfm.html#quick-recap-ecdsa",
    "href": "documents/research/posts/ERFC-39.gfm.html#quick-recap-ecdsa",
    "title": "BLS vs Schnorr vs ECDSA digital signatures",
    "section": "Quick recap: ECDSA",
    "text": "Quick recap: ECDSA\nModern cryptography is founded on the idea that the key that you use to encrypt your data can be made public while the key that is used to to decrypt your data can be kept private. As such, these systems are known as public-key cryptographic systems.\nECDSA stands for Elliptic Curve Digital Signature Algorithm. Elliptic curve cryptography is a form of public key cryptography which is based on the algebraic structure of elliptic curves over finite fields. Used by both Bitcoin and Ethereum.\nElliptic curve: secp256k1\n\nsecp256k1\nThe elliptic curve domain parameters over Fp associated with a Koblitz curve secp256k1 are specified by the sextuple T = (p, a, b, G, n, h) where the finite field Fp is defined by:\n\np = 0xFFFFFFFF FFFFFFFF FFFFFFFF FFFFFFFF FFFFFFFF FFFFFFFF FFFFFFFE FFFFFC2F = 2^256 − 2^32 − 2^9 − 2^8 − 2^7 − 2^6 − 2^4 − 1\n\nThe curve E: y^2 = x^3 + ax + b over Fp is defined by:\n\na = 0x00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000\nb = 0x00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000007\n\nThe base point G in compressed form is:\n\n0x02 79BE667E F9DCBBAC 55A06295 CE870B07 029BFCDB 2DCE28D9 59F2815B 16F81798\n\nand in uncompressed form is:\n\n0x04 79BE667E F9DCBBAC 55A06295 CE870B07 029BFCDB 2DCE28D9 59F2815B 16F81798 483ADA77 26A3C465 5DA4FBFC 0E1108A8 FD17B448 A6855419 9C47D08F FB10D4B8\n\n\n\n\nsecp256k1 curve\n\n\n\n\nPrivate and public keys\nPrivate keys are generated as random 256 bits or 64 random hex characters or 32 random bytes. The public key is derived from the private key using ECDSA. Public key is a point on secp256k1 elliptic curve, generated by formula K = k * G where K is public key, k is private key, G is the constant point on secp256k1 elliptic curve and * is the multiplication operator on secp256k1 elliptic curve. There is no inverse, “/” operator, therefore the relationship between k and K is fixed, but can only be calculated in one direction, from k to K. A private key can be converted into a public key, but a public key cannot be converted back into a private key, because the math only works one way. The multiplication of k * G is equivalent to repeated addition, so G + G + G + …​ + G, repeated k times.\n\n\nSigning and verification\nTo sign and verify ECDSA signature using OpenSSL, do next\n# Generate private key\nopenssl genpkey -algorithm rsa -out privatni.pem\n\n# Generate public key out of private key\nopenssl rsa -pubout -in privatni.pem -out javni.pem\n\n# Test message for signing\necho \"Test\" > message.txt\n\n# Sign the message (with Bitcoin's hashing alghorithm)\nopenssl dgst -sha256 -sign privatni.pem -out signature.bin message.txt\n\n# Verification\nopenssl dgst -sha256 -verify javni.pem -signature signature.bin message.txt\n\n\n\necdsa signing\n\n\n\n\nECDSA verification in Solidity\nimport \"@openzeppelin/contracts/utils/cryptography/ECDSA.sol\";\n\ncontract Example {\n  address public admin;\n\n  constructor() {\n    admin = msg.sender;\n  }\n\n  function verify(bytes32 _digest, bytes calldata _signature) public view returns(bool) {\n    return admin == ECDSA.recover(_digest, _signature);\n  }\n}\n\n\nECDSA verification in Javascript/Typescript\nimport { SignerWithAddress } from '@nomiclabs/hardhat-ethers/signers';\nimport { utils } from 'ethers';\nimport { expect } from 'chai';\n\n(async () => {\n    let admin: SignerWithAddress;\n\n    [admin] = await ethers.getSigners();\n\n    const message: string = 'Hello World';\n    const msgHash: string = utils.hashMessage(message);\n    const digest: Uint8Array = utils.arrayify(msgHash);\n\n    const signature: string = await admin.signMessage(message);\n\n    const address: string = utils.recoverAddress(digest, signature);\n    expect(address).to.equal(admin.address);\n})();"
  },
  {
    "objectID": "documents/research/posts/ERFC-39.gfm.html#bls",
    "href": "documents/research/posts/ERFC-39.gfm.html#bls",
    "title": "BLS vs Schnorr vs ECDSA digital signatures",
    "section": "BLS",
    "text": "BLS\nBLS stands for Boneh-Lynn-Shacham, it’s a signature scheme that is based on bi-linear pairings. A pairing, defined as e(,), is a bilinear map of 2 groups G1 and G2 in some other group, GT. e(,) takes as arguments points in G1 and G2.\nPairings that verifies a signature looks like this:\ne(g1, sig) = e(P, H(m))\n\n# or in expanded form like this\ne(g1, pk*H(m)) = e(pk*g1, H(m)) = e(g1, pk*H(m))\nH(m) is hashing a message to a point on an elliptic curve.\nBLS consists of:\n\nKeyGen — choose a random α. Given generator g1, P=α*g1\nSign — σ = α*H(m) ∈ G2 (in the case of ETH 2.0)\nVerify(P,m, σ) — if e(g1, σ) = e(P, H(m)) return true.\n\nElliptic curve: BLS12-381\n\nBLS12-381\nBLS12-381 is a pairing-friendly elliptic curve construction that is optimal for zk-SNARKs at the 128-bit security level.\nBarreto-Naehrig (BN) curves are a class of pairing-friendly elliptic curve constructions built over a base field Fp of order r, where r≈p. It is possible to construct a new BN curve that targets 128-bit security by selecting a curve closer to p≈2^(384). However, the larger group order r impairs the performance of multi-exponentiation, fast fourier transforms and other cryptographic operations.\nBarreto-Lynn-Scott (BLS) curves are a slightly older class of pairing-friendly curves which now appear to be more useful for targeting this security level. Current research suggests that with p≈2^(384), and with an embedding degree of 12, these curves target the 128-bit security level.\n\n\nPrivate and public keys\nThe private/secret key (to be used for signing) is just a randomly chosen number between 1 and r−1 inclusive. We’ll call it pk.\nThe corresponding public key is P=[pk]g1, where g1 is the chosen generator of G1. That is, g1 multiplied by pk, which is g1 added to itself pk times.\nThe discrete logarithm problem means that it is unfeasible to recover pk given the public key P.\n\n\nSigning\nOne can sign the message by calculating the signature σ=[pk]H(m). That is, by multiplying the hash point by our secret key. But what is H?\nTo calculate a digital signature over a message, we first need to transform an arbitrary message (byte string) to a point on the G2 curve. The initial implementation in Eth2 was “hash-and-check”:\n\nHash your message to an integer modulo q\nCheck if there is a point on the curve with this x-coordinate. If not, add one and repeat\nOnce you have a point on the curve multiply it by the G2 cofactor to convert it into a point in G2.\n\n\n\n\nbls signing\n\n\n\n\nVerification\nGiven a message m, a signature σ, and a public key P, we want to verify that it was signed with the pk corresponding to P. The signature is valid if, and only if, e(g1,σ)=e(P,H(m)).\n\n\n\nbls verification\n\n\n\n\nAggregation\nA really neat property of BLS signatures is that they can be aggregated, so that we need only two pairings to verify a single message signed by n parties, or n - 1 pairings to verify n different messages signed by n parties, rather than 2n pairings you might naively expect to need. Pairings are expensive to compute, so this is very attractive.\nTo aggregate signatures we just have to add up the G2 points they correspond to: σagg=σ1+σ2+...+σn. We also aggregate the corresponding G1 public key points Pagg=P1+P2+...+Pn.\nNow the magic of pairings means that we can just verify that e(g1,σagg)=e(Pagg,H(m)) to verify all the signatures together with just two pairings.\n\n\nBLS verification in Solidity\nBelow shows an example Solidity function that verifies a single signature. EIP-197 defined a pairing precompile contract at address 0x8 and requires input to a multiple of 192. This assembly code calls the precompile contract at address 0x8 with inputs.\n  // Negated genarator of G2\n  uint256 constant nG2x1 = 11559732032986387107991004021392285783925812861821192530917403151452391805634;\n  uint256 constant nG2x0 = 10857046999023057135944570762232829481370756359578518086990519993285655852781;\n  uint256 constant nG2y1 = 17805874995975841540914202342111839520379459829704422454583296818431106115052;\n  uint256 constant nG2y0 = 13392588948715843804641432497768002650278120570034223513918757245338268106653;\n\n\nfunction verifySingle(\n    uint256[2] memory signature, \\\\ small signature\n    uint256[4] memory pubkey, \\\\ big public key: 96 bytes\n    uint256[2] memory message\n) public view returns (bool) {\n    uint256[12] memory input = [\n        signature[0],\n        signature[1],\n        nG2x1,\n        nG2x0,\n        nG2y1,\n        nG2y0,\n        message[0],\n        message[1],\n        pubkey[1],\n        pubkey[0],\n        pubkey[3],\n        pubkey[2]\n    ];\n    uint256[1] memory out;\n    bool success;\n\n    assembly {\n        success := staticcall(sub(gas(), 2000), 8, input, 384, out, 0x20)\n        switch success\n            case 0 {\n                invalid()\n            }\n    }\n\n    require(success, \"\");\n    return out[0] != 0;\n}\n\n\nBLS verification in Javascript/Typescript\nconst bls = require('@noble/bls12-381');\n\n(async () => {\n    // keys, messages & other inputs can be Uint8Arrays or hex strings\n    const privateKey =\n        '67d53f170b908cabb9eb326c3c337762d59289a8fec79f7bc9254b584b73265c';\n    const message = '64726e3da8';\n    const publicKey = bls.getPublicKey(privateKey);\n    const signature = await bls.sign(message, privateKey);\n    const isValid = await bls.verify(signature, message, publicKey);\n    console.log({ publicKey, signature, isValid });\n\n    // Sign 1 msg with 3 keys\n    const privateKeys = [\n        '18f020b98eb798752a50ed0563b079c125b0db5dd0b1060d1c1b47d4a193e1e4',\n        'ed69a8c50cf8c9836be3b67c7eeff416612d45ba39a5c099d48fa668bf558c9c',\n        '16ae669f3be7a2121e17d0c68c05a8f3d6bef21ec0f2315f1d7aec12484e4cf5',\n    ];\n    const messages = ['d2', '0d98', '05caf3'];\n    const publicKeys = privateKeys.map(bls.getPublicKey);\n    const signatures2 = await Promise.all(\n        privateKeys.map((p) => bls.sign(message, p))\n    );\n    const aggPubKey2 = bls.aggregatePublicKeys(publicKeys);\n    const aggSignature2 = bls.aggregateSignatures(signatures2);\n    const isValid2 = await bls.verify(aggSignature2, message, aggPubKey2);\n    console.log({ signatures2, aggSignature2, isValid2 });\n})();"
  },
  {
    "objectID": "documents/research/posts/ERFC-39.gfm.html#schnorr",
    "href": "documents/research/posts/ERFC-39.gfm.html#schnorr",
    "title": "BLS vs Schnorr vs ECDSA digital signatures",
    "section": "Schnorr",
    "text": "Schnorr\nSchnorr signatures are generated slightly differently than ECDSA. Instead of two scalars (r,s) we use a point R and a scalar s. Similar to ECDSA, R is a random point on elliptic curve (R = k×G). Second part of the signature is calculated slightly differently: s = k + hash(P,R,m) ⋅ pk. Here pk is your private key, P = pk×G is your public key, m is the message. Then one can verify this signature by checking that s×G = R + hash(P,R,m)×P.\n\n\n\nschnorr signing\n\n\nThis equation is linear, so equations can be added and subtracted with each other and still stay valid. This brings us to several nice features of Schnorr signatures that we can use.\n\nBatch validation\nTo verify a block in Bitcoin blockchain we need to make sure that all signatures in the block are valid. If one of them is not valid we don’t care which one - we just reject the whole block and that’s it.\nWith ECDSA every signature has to be verified separately. Meaning that if we have 1000 signatures in the block we will need to compute 1000 inversions and 2000 point multiplications. In total ~3000 heavy operations.\nWith Schnorr signatures we can add up all the signature verification equations and save some computational power. In total for a block with 1000 transactions we need to verify that\n(s1+s2+…+s1000)×G=(R1+…+R1000)+(hash(P1,R1,m1)×P1+ hash(P2,R2,m2)×P2+…+hash(P1000,R1000,m1000)×P1000)\nHere we have a bunch of point additions (almost free in sense of computational power) and 1001 point multiplication. This is already a factor of 3 improvement - we need to compute roughly one heavy operation per signature.\n\n\n\nbatch validation\n\n\n\n\nKey aggregation\nWe want to keep our bitcoins safe, so we might want to use at least two different private keys to control bitcoins. One we will use on a laptop or a phone and another one - on a hardware wallet / cold wallet. So when one of them is compromised we still have control over our bitcoins.\nCurrently it is implemented via 2-of-2 multisig script. This requires two separate signatures to be included in the transaction.\nWith Schnorr signatures we can use a pair of private keys (pk1,pk2) and generate a shared signature corresponding to a shared public key P=P1+P2=pk1×G+pk2×G. To generate this signature we need to choose a random number on every device (k1,k2), generate a random point Ri=ki×G, add them up to calculate a common hash(P,R1+R2,m) and then get s1 and s2 from every device (si = ki + hash(P,R,m) ⋅ pki). Then we can add up these signatures and use a pair (R, s) = (R1+R2, s1+s2) as our signature for shared public key P. No one else won’t be able to say if it is an aggregated signature or not - it looks exactly the same as a normal Schnorr signature."
  },
  {
    "objectID": "documents/research/posts/ERFC-40.gfm.html#goals",
    "href": "documents/research/posts/ERFC-40.gfm.html#goals",
    "title": "Access NFT Tools - Overview And Space For Improvements",
    "section": "Goals",
    "text": "Goals\n\nIdentifying content gating tools (that works).\nUnderstanding how they work.\nRecognizing use cases that they are used for, the problems they aim to solve.\n\nIt is essential to overview current solutions in the field we aim to dive into. Both, short-term, for this research and experiment, ut also long-term, for being able to actively track all the innovation in this field from now on.\n\nVerify our presumption about how current solutions are used to gate Telegram and Discord communities: Through using centralized services and storing the user’s private information (wallet address and username), existing solutions periodically verify the user’s possession of specific NFT.\n\nThe main goal of this research is to verify our presumption that there is privacy concern and space for tech improvement within the Access NFT tool niche. If it is accurate, and if the current solution keeps track and connection between private users’ data, we can conclude that there are reasons to move forward with this idea and start working on technical specifications and research how it can be developed."
  },
  {
    "objectID": "documents/research/posts/ERFC-40.gfm.html#methodology",
    "href": "documents/research/posts/ERFC-40.gfm.html#methodology",
    "title": "Access NFT Tools - Overview And Space For Improvements",
    "section": "Methodology",
    "text": "Methodology\n\nTrying out and using existing solutions. We aimed to try each solution available on the market as we believe it is the only way to understand how it works and what problems it solves.\nConsult and discuss with team and community members via contact forms or community servers We want to confirm that our understanding of how a specific tool works and what lies under the hood is correct. Hence, the best way is to receive confirmation from the people building and using it, especially when those tools are not open-sourced.\nAnalyze other reviews There are blog posts, websites, and apps that have already analyzed and gathered information about the current state of Access NFT tools. Thus, we do not need to do all of this work again, as we can benefit from their insights and overviews. Still, this does not mean that we will take it for granted. We want to do our research as well.\n\n*We were not looking under the hood (going through the code). Most of the solutions are not open-sourced. Also, we could understand how they work by consulting with the community and team members behind these solutions."
  },
  {
    "objectID": "documents/research/posts/ERFC-40.gfm.html#collab.land",
    "href": "documents/research/posts/ERFC-40.gfm.html#collab.land",
    "title": "Access NFT Tools - Overview And Space For Improvements",
    "section": "Collab.Land",
    "text": "Collab.Land\nCollab.Land is a sovereign ruler and tool that is used by almost all Discord and Telegram Access communities.\nThe Collab.Land documentation is scarce and it is focused on explaining how to connect their bot rather than explaining how it works. Also, we could not find any other useful information and as their solution was not open-sourced, we reached out directly to them looking for answers that will help us test our hypothesis.\nBased on the answers we received, Collab.Land does store connections between users personal information and their wallet address.\nHowever, before we could conclude that our hypothesis was correct, we had to research Swordy-bot."
  },
  {
    "objectID": "documents/research/posts/ERFC-40.gfm.html#swordy-bot",
    "href": "documents/research/posts/ERFC-40.gfm.html#swordy-bot",
    "title": "Access NFT Tools - Overview And Space For Improvements",
    "section": "Swordy Bot",
    "text": "Swordy Bot\nSwordy-bot is a Discord bot used to verify and grant access to a specific Discord channel (server) if a user has required token(s).\nCompared to Collab.Land, Swordy-bot is built on top of decentralized Unlock Protocol. However, as well as Collab.Land it does store and keep a connection between user’s personal information (Discord id) and wallet address in their centralized database. We came to that conclusion (again) based on the information provided by their team members. .\nAnother common thing with Collab.Land is that Sowrdy-bot is not open-sourced as well.\nWorth mentioning is that, even though Collab.Land is sovereign ruler, Swordy-bot is used as a gate keeper by more than 100 Discord communities.\nFinally, we could confirm that our presumption was correct based on the fact that both of these solutions do store and keep track of the user’s wallet address and username centrally.\nThereafter we went a step further and analyzed other solutions used for content gating. All of them can be divided into 3 categories:\n\nProtocols\nPlatforms\nApps/Tools\n\nAlso each of them is either centralized or decentralized.\nWe have already covered Swordy-bot and Collab.land which are centralized tools."
  },
  {
    "objectID": "documents/research/posts/ERFC-40.gfm.html#unlock-protocol-decentralized-protocol",
    "href": "documents/research/posts/ERFC-40.gfm.html#unlock-protocol-decentralized-protocol",
    "title": "Access NFT Tools - Overview And Space For Improvements",
    "section": "Unlock Protocol [Decentralized Protocol]",
    "text": "Unlock Protocol [Decentralized Protocol]\nUnlock Protocol is an open-source protocol, not a centralized platform, used to create an underlying infrastructure for token gating content (communities, application, websites pages, and sections, images, videos, etc.)\nIt enables:\n\nuser (admin) to deploy a set of smart contracts (on Mainnet, xDAI, Polygon, BSC, or Optimism) and define gating details (number of keys, key price and key duration).\nregular users to purchase key (an NFT) and access content.\n\nUP is the underlying layer that allows other tools to build on top of it and utilize its functionalities. Bellow are listed use cases enabled through community-developed integrations (apps and plugins) build on top of Unlock Protocol:\n\nSwordy-bot - gating Discord communities (servers and channels).\n\nDiscourse plugin - gated content on Discourse.\nWP and Webflow plugins - gated website pages or section.\nDurap module - gating content on Durap.\nSlack plugin - gated Slack servers.\nShopify app - allows merchants to offer special memberships to their customers.\n\nPlugins and other integration tools are the ones that connect blockchain with specific apps (Discord, Slack, etc.). Hence, they are centralized solutions that monitor what’s happening on a blockchain (whether the wallet address still holds an NFT) and inform apps about that. Something like reverse oracles.\nAll in all, Unlock Protocol is a customized set of smart contracts with the following functionalities:\n\nMinting and sending NFTs (locks) to users.\nCollecting and withdrawing crypto on behalf of admin.\n\nAlso, if needed, it can support more traditional (web2) authentication methods by storing private keys on behalf of a user. Also, UP support CC payments.\nYou are right if you think that all of the above may be done without UP. However, it is much easier and faster to implement all of these functionalities by using Unlock Protocol than developing all of the smart contracts and features from scratch."
  },
  {
    "objectID": "documents/research/posts/ERFC-40.gfm.html#whale-room-centralized-platform",
    "href": "documents/research/posts/ERFC-40.gfm.html#whale-room-centralized-platform",
    "title": "Access NFT Tools - Overview And Space For Improvements",
    "section": "Whale room [Centralized Platform]",
    "text": "Whale room [Centralized Platform]\nWhale room is a centralized platform that enables users to create their chat rooms within the platform and gate them by setting up the access requirements (required tokens). Also, it supports token mining and distribution (to the community members).\nWhale room is an alternative for using Discord (or Telegram) in combination with Collab.Land (or Swordy-bot) as it offers both, chat rooms (as Telegram and Discord) and token gating functionality (as Collab.Land and Swordy-bot)."
  },
  {
    "objectID": "documents/research/posts/ERFC-40.gfm.html#mintgate-centralized-tool-platform",
    "href": "documents/research/posts/ERFC-40.gfm.html#mintgate-centralized-tool-platform",
    "title": "Access NFT Tools - Overview And Space For Improvements",
    "section": "MintGate [Centralized Tool & Platform]",
    "text": "MintGate [Centralized Tool & Platform]\nMintGate is a tool that allows users to hide content (URL) and present it only to the ones that possess specific NFT. Users can use MintGate to deploy new NFTs required to access content, but it also supports using other NFTs, minted in the other way.\nIn addition, MintGate recently created a centralized platform that offers the creation of your store and gating it with MintGate technology."
  },
  {
    "objectID": "documents/research/posts/ERFC-40.gfm.html#guild-centralized-platform",
    "href": "documents/research/posts/ERFC-40.gfm.html#guild-centralized-platform",
    "title": "Access NFT Tools - Overview And Space For Improvements",
    "section": "Guild [Centralized Platform]",
    "text": "Guild [Centralized Platform]\nIn combination with its Medusa bot, Guild gate the access to Discord private channels rather than the whole discord server. Compared to other solutions, it supports more complex requirements logic by combining the following requirement options: possession of an NFT, amount of an ERC20 token, and opportunity to whitelist wallet addresses.\nBesides this feature and difference, it offers the same options as using some of the previously mentioned bots combined with Discord or Telegram (that you also need if you want to use Guild)."
  },
  {
    "objectID": "documents/research/posts/ERFC-42.gfm.html",
    "href": "documents/research/posts/ERFC-42.gfm.html",
    "title": "Analysis of smart contract fuzzers",
    "section": "",
    "text": "Executive Summary\nWe are witness of the accelerated development of smart contracts, and more and more valuable assets are contained in them. For this reason, it is very important to write secure smart contracts. However, since people write smart contracts, and people make mistakes, it is desirable to have a tool that will be able to point out potential problems in the code. Fuzz testing is a technique that with early entry stresses our program to reveal errors. We will get to know with fuzing tools through this research.\n\n\nIntroduction\nSmart contracts often contain valuable assets, whether in the form of tokens or Ether. Smart contract’s source code is publicly available, every execution happens on a public network. If smart contracts have vulnerabilities that can lead to catastrophic damages, that is potentially measured in millions of dollars. For example, at 2017 attack on Parity Wallet cost ~30 million dollars1 , at the 2016 DAO Hack cost ~150 million dollars2 , at 2020 Harvest Finance was attacked using flash loans and stole ~30 million dollars3 . To prevent disasters like this, it is important to find any vulnerabilities in smart contracts before deployments. Some of the common vulnerabilities are integer overflow/underflow, race conditions, and also we could have logic mistakes that are hard to detect. Security is essential while developing smart contracts. There are some known hacker attacks and good practices to follow.4\nHence, in the development of smart contracts, testing is one of the most important techniques that require special time aside. Mostly we write unit tests, but unit tests are specific to one use case, and often some edge cases are not covered. There is a big research interest in developing testing tools, especially ones that are able to automatically detect as many problems in the code as possible, one such technique is fuzz testing.\n\n\nGoals & Methodology\nThere are many automatic bug-finding tools, and the purpose of this research is to introduce you to smart contracts fuzz testing. Fuzzing is a well-known technique in the security community it generates random, or invalid data as inputs to reveal bugs in the program, in one word it stress the program and causes unexpected behavior or crashes.\nConsidering the research interest in this area, a number of tools have been developed. Some popular open source tools are ContractFuzzer, ContraMaster, ILF, sFuzz, Smartian, and Echidna. Comparison of these tools is not easy task, the main reason is that each tool covers some specific set of bug classes.\nEchidna is a property-based tool, develop with the aim to check if the contract violates some user-defined invariants, while other tool tries to find crashes. Company Trails of Bits extended fuzz technique to the EVM by developing the Echidna tool. Echidna can test both Solidity and Vyper smart contracts, it is written in Haskell, and main design goals are:\n\nEasy to use and configure\nGood contract coverage\nFast and quickly results\n\nContraMaster have been developed to detect irregular transactions due to various types of adversarial exploits, detects 3 classes of bug: Reentrancy, Exception Disorder, Gasless Send and Integer overflow/underflow.\nContractFuzzer detects 7 classes of bug: Reentrancy, Exception Disorder, Gasless Send, Timestamp Dependency, Block Number Dependency, DelegateCall and Freezing Ether Contract.\nsFuzz detects all classes of bug as Contract Fuzzer plus Integer overflow/underflow. It has an extendable architecture which allows to easily support new bug classes as well. Also, sFuzz is effective in achieving high code coverage\nSmartian detects 13 classes of bug: Assertion Failure, Arbitrary Write Block state Dependency, Control Hijack, Ether Leak, Integer Bug, Mishandled Exception, Multiple Send, Reentrancy, Suicidal Contract, Transaction Origin Use, Freezing Ether, Requirement Violation.\nThe way of generating inputs is different, ContractFuzzer and Echidna generate test cases based on a set of predefined parameter values, and fail to cover deeper paths that expose some vulnerabilities. sFuzz has guided input generation based on a genetic algorithm to iteratively improve its branch coverage. ILF generates input based on AI, using neural networks.\nAll in all, only Smartian, ILF, and Echidna at the end show the path how we could reproduce the bug. As Smartian covers more bug classes than ILF, shown in bellow Figure5 , the focus in this research will be on Smartian and Echidna.\n\n\n\nResults & Discussion\nAll examples are run on MacOS Big Sur, version 11.6, processor 2,6 GHz 6-Core Intel Core i7 and 16GB of memory. As there is no upper bound on how long Echidna can run, but the goal is to find a bug in up to 5 minutes.6 Configuration for Smartian test timeout is set up to 5 minutes.\nLet’s first show and discuss few motivating smart contract examples:\ncontract MotivationExample {\n    function f(int256 a, int256 b, int256 c) public pure returns (int256) {\n        int256 d = b + c;\n        if (d < 1) {\n            if (b < 3) {\n                return 1;\n            }\n            if (a == 42) {\n                assert(false);\n                return 2;\n            }\n            return 3;\n        } else {\n            if (c < 42) {\n                return 4;\n            }\n            return 5;\n        }\n    }\n}\nVery fast both Smartian and Echidna find assertion failure in above smart contract, results with counterexample and information how to reproduce transaction are show in next Figures(Figure1 and Figure2):\n\nFigure1 : Smartian replayable test case\n\nFigure2 : Echidna replayable test case\nThe next two examples have some more complex math:\ncontract MotivationExample {\n    bool private value_found;\n\n    function f(uint256 a, uint256 b, uint256 c, uint256 d) public {\n        require(a == 42);\n        require(b == 129);\n        require(c == d+333);\n        value_found = true;\n        assert(value_found == false);\n    }\n}\nAbove one, the inputs must meet three requirements, and to the equality. Smartian and Echidna test it, and at bellow Figures(Figure3 and Figure4) are results:\n\nFigure3 : Smartian replayable test case\n\nFigure4 : Echidna failed to find assertion\nSmartian quickly found assertion failure and counterexample, while Echidna failed to find one. Take a look at hardest motivation example and result from fuzzers:\ncontract MotivationExample {\n    uint256 private stateA;\n    uint256 private stateB;\n    uint256 CONST = 32;\n\n    function f(uint256 x) public {\n      stateA = x;\n    }\n\n    function g(uint256 y) public{\n      if (stateA % CONST == 1) {\n        stateB = y - 10;\n      }\n    }\n\n    function h() public view {\n      if (stateB == 62) { \n        bug(); \n      }\n    }\n\n    function bug() private pure {\n      assert(false);\n    }\n}\n\nFigure5 : Smartian replayable test case\n\nFigure6 : Echidna failed to find assertion\nAgain, Smartian quickly finds assertion failure, while Echidna fails. The reason for failure is due to the way it generates inputs. Echidna is not smart to go in-depth when making input seeds and figure out the values in the deeper branches.\nIndeed, there is a way for Echidna to find assertion failure in the above examples, solution is in the configuration file.\nEchidna has a YAML configuration file, with configurable parameters, that can be turned on or off during the test. If config.yaml is not listed, the default YAML configuration file is called.7 Some of configuration parameters enable to blacklist function, compute maximum gas usage, the maximum number of transactions sequences to generate, number of test sequences to run, prefix for boolean functions that are properties to be checked, contract deployer address. Also it is possible to define set of addresses transactions originate from along with default balance for addresses. In case we have a complex contract, and we need to initialize the blockchain with some data -> tool Etheno helps here8 , after Etheno finishes the initialization JSON file is created, that is set as initialization inside configuration file. Additionally, if our contract uses some framework, for example, Hardhat or Truffle, Echidna then use crytic compile, and build directory of the framework is sent through crytic arguments inside Echidna configuration file.\nBack to the above example, to find assertion failure, it is enough inside configuration file to set corpus directory. After first run, inside the corpus directory we could see the generated input for contract properties, now is enough to modify the input to use suitable parameters that will cause assertion failure.\nAlthough Smartian beat Echidna in the above examples, the logical question, that arises, is what are the advantages of Echidna and why would we use Echidna rather than Smartian?\nEchidna’s advantage are invariants, Invariants are Solidity functions that can represent any incorrect state that contract can have, each invariant must be:\n\nPublic method that has no argument\nReturn true if it is successful\n\nor:\n\nPublic method that can have an argument\nUse assert in function\n\n\nFigure7 : Architecture of Echdina\nArchitecture9 is divided into preprocessing and fuzzing campaigns. In the preprocessing step, the static analyzer tool Slither10 is used with the purpose to find useful constants and functions for effective testing. In the fuzzing campaign step, using contract ABI(Application binary interface) the random transactions are generated, and also any previous transactions from the corpus are included. In case the vulnerability is detected, a counterexample is automatically minimized to the smallest and simplest sequence of transactions that cause failure.\nRunning Echidna:\n$ echidna-test contract.sol --constract TEST --config config.yaml,\nor if Truffle or Hardhat is used:\n$ echidna-test . contract.sol --constract TEST --config config.yaml\nEchidna can be run from the docker, the official image of the current 2.0.0 version is trailofbits/echidna. Inside docker, default version of solidity compiler is 0.5.7, so if we want to test contracts in another version we need to install solc-select. If the preferred method is to not worry about how to install additional tools, there is a docker image trailofbits/eth-security-toolbox, but currently there ecidna version is 1.7.2.\n\n\nConclusion\nAlthough Smartian is better at generating input, what should be noted is that Smartian still doesn’t have support for solc 0.8.x or greater. All examples from the section Results & Discussion have been tested with solc 0.4.25. For an experiment, if you take IB.sol from Smartian benchmark examples,11 and adapt it to work with solidity version 0.8.9, Smartian will fail to find Integer Bug, but Echidna will find it. If you take a look at comparison of fuzz tools in Goals & Methodology that in the previous version Echidna was not able to found these bugs, as the integer overflow/underflow is one of the features in Echidna 2.0.0 for solc 0.8.x or greater. In Appendix at Figure11 and Figure12 is shown Smartian output, and in Figure13 is shown Echidna output.\nIn general, Echidna and Smartian together cover bug classes: Assertion Failure and Integer Bug. Some comparison examples between Echidna 2.0.0, Smartian solc 0.4.25 and Smartian solc 0.8.9 are in research_examples.\nEchidna Assertion allows us to manage what property should test along with the input range value of testing property arguments, in contrary using explicit property we are not sure which function will be checked and which arguments should be used to call test property, explicit property check all method that is not private or internal.\nWhen asked which tool of the two to use, the simple answer is both. Both tools are promising. They cover different classes of problems and the use of both tools for testing smart contracts reduces the chance that our contract has potential flaws. Echidna is under active development, so it is reasonable that it is buggy. But, they work hard to reply & fix each issue.\nThe Echidna coverage report is a little bit confused, one is shown at Figure8. Some suggestion is to add some styled report(example) and the legend table of symbol meaning:\n\n\n\n\n\n\n\nLine marker\nMeaning\n\n\n\n\n’*’\nIf an execution ended with a STOP\n\n\nr\nIf an execution ended with a REVERT\n\n\no\nIf an execution ended with an out-of-gas error\n\n\ne\nIf an execution ended with any other error (zero division, assertion failure, etc)\n\n\n\n\nFigure8 : Echidna coverage\nUI for both tools should be improved, it would be nice to display emitted events. And, Smartian output could color counterexample and found bugs. The current UI is shown in the bellow Figures.\n\nFigure9\n\nSmartian output\n\n\n\nFigure10 : Echidna output\nIt would be nice to have integration with Remix IDE, which will help with debugging.\n\n\nAppendices\n\nFigure11 : Smartian found Integer Bug with 0.4.25\n\nFigure12 : Smartian not found Integer Bug with 0.8.9\n\nFigure13 : Echidna found Integer Bug with 0.8.9\n\n\nBibliography\n\n\n\n\n\nReferences\n\n‘Building-Secure-Contracts/Workflow.md at Master  Crytic/Building-Secure-Contracts’, GitHub <https://github.com/crytic/building-secure-contracts> [accessed 14 February 2022]\n\n\nChoi, Jaeseung, Doyeon Kim, Soomin Kim, Gustavo Grieco, Alex Groce, and Sang Kil Cha, ‘SMARTIAN: Enhancing Smart Contract Fuzzing with Static and Dynamic Data-Flow Analyses’, in 2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE) (Melbourne, Australia: IEEE, 2021), pp. 227–39 <https://doi.org/10.1109/ASE51524.2021.9678888>\n\n\n‘Etheno’ (Crytic, 2022) <https://github.com/crytic/etheno> [accessed 28 February 2022]\n\n\nFoxley, William, ‘Harvest Finance: $24m Attack Triggers $570m ’Bank Run’ in Latest DeFi Exploit’, 2020 <https://www.coindesk.com/tech/2020/10/26/harvest-finance-24m-attack-triggers-570m-bank-run-in-latest-defi-exploit/> [accessed 11 February 2022]\n\n\nGrieco, Gustavo, Will Song, Artur Cygan, Josselin Feist, and Alex Groce, ‘Echidna: Effective, Usable, and Fast Fuzzing for Smart Contracts’, in ISSTA 2020, 2020\n\n\nSiegel, David, ‘Understanding The DAO Attack’, 2016 <https://www.coindesk.com/learn/2016/06/25/understanding-the-dao-attack/> [accessed 11 February 2022]\n\n\n‘Slither, the Solidity Source Analyzer’ (Crytic, 2022) <https://github.com/crytic/slither> [accessed 11 February 2022]\n\n\n‘The Parity Wallet Hack Explained’, OpenZeppelin Blog, 2017 <https://blog.openzeppelin.com/on-the-parity-wallet-multisig-hack-405a8c12e8f7/> [accessed 11 February 2022]\n\nFootnotes\n\n\n‘The Parity Wallet Hack Explained’, OpenZeppelin Blog, 2017 <<https://blog.openzeppelin.com/on-the-parity-wallet-multisig-hack-405a8c12e8f7/>> [accessed 11 February 2022].↩︎\nDavid Siegel, ‘Understanding The DAO Attack’, 2016 <<https://www.coindesk.com/learn/2016/06/25/understanding-the-dao-attack/>> [accessed 11 February 2022].↩︎\nWilliam Foxley, ‘Harvest Finance: $24m Attack Triggers $570m ’Bank Run’ in Latest DeFi Exploit’, 2020 <<https://www.coindesk.com/tech/2020/10/26/harvest-finance-24m-attack-triggers-570m-bank-run-in-latest-defi-exploit/>> [accessed 11 February 2022].↩︎\n‘Building-Secure-Contracts/Workflow.md at Master  Crytic/Building-Secure-Contracts’, GitHub <<https://github.com/crytic/building-secure-contracts>> [accessed 14 February 2022].↩︎\nJaeseung Choi and others, ‘SMARTIAN: Enhancing Smart Contract Fuzzing with Static and Dynamic Data-Flow Analyses’, in 2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE) (Melbourne, Australia: IEEE, 2021), pp. 227–39 <https://doi.org/[10.1109/ASE51524.2021.9678888](https://doi.org/10.1109/ASE51524.2021.9678888)>.↩︎\nGustavo Grieco and others, ‘Echidna: Effective, Usable, and Fast Fuzzing for Smart Contracts’, in ISSTA 2020, 2020.↩︎\nCryticEchidnaEthereum?↩︎\n‘Etheno’ (Crytic, 2022) <<https://github.com/crytic/etheno>> [accessed 28 February 2022].↩︎\nGrieco and others.↩︎\n‘Slither, the Solidity Source Analyzer’ (Crytic, 2022) <<https://github.com/crytic/slither>> [accessed 11 February 2022].↩︎\nSmartian2022?↩︎"
  },
  {
    "objectID": "documents/research/index.html",
    "href": "documents/research/index.html",
    "title": "Research",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n         \n          File Name\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nBLS vs Schnorr vs ECDSA digital signatures\n\n\nBitcoin and Ethereum both use elliptic-curve cryptography for generating keys and signing transactions. The algorithm they both use is called Elliptic Curve Digital Signature Algorithm (ECDSA), which represents a secure way of signing a message (a transaction for example) using Elliptic Curve Cryptography (ECC). This is a comparative analysis of BLS, Schnorr, and ECDSA signatures. The goal is to understand why are Ethereum and Bitcoin migrating from ECDSA, why BLS and Schnorr are superior to ECDSA and what are the key differences between them, how to use these signatures in the development and which Elliptic curves they are using and why.\n\n\n\n\n\n\n3/4/2022\n\n\nAndrej Rakic\n\n\n12 min\n\n\nERFC-39.gfm.md\n\n\n\n\n\n\n\n\nBuilding on Filecoin and Filecoin Proofs\n\n\nWhen looking to build dApps that utilize decentralized storage Filecoin seems like the best option even with its flaws like : absent proof of deletion for the client, absent encryption, impossible modifying of the stored data and the durability problem of Filecoin’s Proof of Replication. This research gives an overview of competitors in decentralized storage solution field with a focus on Filecoin protocol. It also shows how its competitor Storj differs from Filecoin and the current grant opportunities for potential building on it. Considering the results of the research Filecoin seems to be the best option considering the popularity and the size of its ecosystem. Currently there is no interest for building on and with Filecoin and this is a purely explorative work without experiments, in order to test the researcher’s methodology and the approach to research. However, it proposes a question about a potential way of improving Filecoin, or creating both safer protocol for the user and cheaper for the storage miners.\n\n\n\n\n\n\n2/25/2022\n\n\nAleksandar Damjanovic\n\n\n19 min\n\n\nERFC-38.gfm.md\n\n\n\n\n\n\n\n\nSolidity++ (S++)\n\n\nWriting efficient code in modern languages is mostly reflected in writing efficient algorithms and business logic. Writing efficient code in Solidity is mostly reflected in thinking as an assembler, moving focus from logic implementation to memory optimisations and low level hacks. Highly optimised code may become unreadable and unmaintainable with possible bugs hidden between the lines of bit level operations. To solve this problem, this project proposes multiple automatic optimisation techniques that will be all summed up into a transpiler and Solidity language extensions called Solidity++.\n\n\n\n\n\n\n2/24/2022\n\n\nAleksandar Veljković\n\n\n6 min\n\n\nERFC-57.gfm.md\n\n\n\n\n\n\n\n\nGaming DAPPs - Play to Earn model\n\n\nBlockchain games, also called Gaming DAPPs, are an emerging area in the Web3 space. With their gameplay, Gaming DAPPs currently cannot compete with the 3D AAA games and they mostly resemble 2D Hyper Casual Mobile games. Developers often need to make compromises in separating on-chain activity from the actions that are taken off-chain and so the line is blurred between what is truly a blockchain game and what is not. However, strong advantage of these games is that they almost always offer some form of an economic incentive to the players with an opportunity to “own” part of the game in order to influence the game’s further development.\n\n\n\n\n\n\n2/24/2022\n\n\nMilos Bojinovic\n\n\n32 min\n\n\nERFC-37.gfm.md\n\n\n\n\n\n\n\n\nAccess NFT Tools - Overview And Space For Improvements\n\n\nNFT Access tokens are primarily used to join private Discord and Telegram communities. Still, a significantly larger opportunity lies in many online and offline communities and events that might utilize this technology.[^1]\n\n\n\n\n\n\n2/21/2022\n\n\nMilos Novitovic\n\n\n10 min\n\n\nERFC-40.gfm.md\n\n\n\n\n\n\n\n\nAnalysis of smart contract fuzzers\n\n\nWe are witness of the accelerated development of smart contracts, and more and more valuable assets are contained in them. For this reason, it is very important to write secure smart contracts. However, since people write smart contracts, and people make mistakes, it is desirable to have a tool that will be able to point out potential problems in the code. Fuzz testing is a technique that with early entry stresses our program to reveal errors. We will get to know with fuzing tools through this research.\n\n\n\n\n\n\n2/17/2022\n\n\nMarija Mijailovic\n\n\n14 min\n\n\nERFC-42.gfm.md\n\n\n\n\n\n\nNo matching items"
  }
]