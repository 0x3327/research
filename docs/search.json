[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "documents/research/posts/ERFC-37.hugo.html",
    "href": "documents/research/posts/ERFC-37.hugo.html",
    "title": "Gaming DAPPs - Play to Earn model",
    "section": "",
    "text": "Blockchain games, also called Gaming DAPPs, are an emerging area in the Web3 space. With their gameplay, Gaming DAPPs currently cannot compete with the 3D AAA games and they mostly resemble 2D Hyper Casual Mobile games. Developers often need to make compromises in separating on-chain activity from the actions that are taken off-chain and so the line is blurred between what is truly a blockchain game and what is not. However, strong advantage of these games is that they almost always offer some form of an economic incentive to the players with an opportunity to “own” part of the game in order to influence the game’s further development.\nThe most widespread model in such games is the “Play to Earn” (P2E) model. In a P2E model, players compete against each other in earning either ERC721 or ERC20 tokens. The mechanism of token distribution varies from game to game but in general it favors those that invest more into the game and so the “Pay to Win” element is introduced. This is currently the most popular model but it has negative connotations of lacking the play aspect and focusing on just the earning potential. Games now try to pivot to what they call “Play and Earn” model with the intention of promoting the community first but also keeping the earning part going. In either way, the economy of such games seems unsustainable as it relies on the never ending stream of new players.\nOnboarding process is also almost always a bit complex and pricy. In order to start playing, the player initially needs to bare some cost that is not constant and is decided by the market (i.e. buying ERC721 tokens used in the game).\nFor the gamers to accept those low quality games, those tradeoffs and a pricy onboarding process there would need to be an economic incentive and whatever the future models will be called the earning aspect needs to be present."
  },
  {
    "objectID": "documents/research/posts/ERFC-37.hugo.html#ronin-sidechain",
    "href": "documents/research/posts/ERFC-37.hugo.html#ronin-sidechain",
    "title": "Gaming DAPPs - Play to Earn model",
    "section": "Ronin sidechain",
    "text": "Ronin sidechain\nThe game was developed on Ethereum in 2018 by the Sky Mavis company located in Vietnam. Due to high Ethereum fees, the game’s creators moved to their own EVM compatible sidechain called Ronin which uses Proof of Authority with validators being chosen by the company5 .\nTo start playing a player needs to :\n\ncreate their own Ronin wallet\ncreate their user account on the Axie Marketplace and connect their wallet to it\ntransfer some amount of ETH and buy at least 3 Axies with a floor price of ~42 dollars per Axie\ninstall a PC or Mobile app and login with their user account\n\nDisregarding the difficulty of the onboarding process, the whole point of a blockchain game is to have the players take actions on the blockchain. Some arguments could be made that at the time the game was developed this was needed but with the introduction of Ronin it is unclear why the whole game was not ported to it.\nThe only actions that are taken on the Ronin sidechain are dedicated to trading - buying, minting and gifting of Axies. There are no fees on Ronin, but the number of transactions per wallet address per day is limited. Deploying on Ronin, also, requires company’s permission so the development of new games and the whole Ronin ecosystem is slowed down."
  },
  {
    "objectID": "documents/research/posts/ERFC-37.hugo.html#scholarships",
    "href": "documents/research/posts/ERFC-37.hugo.html#scholarships",
    "title": "Gaming DAPPs - Play to Earn model",
    "section": "Scholarships",
    "text": "Scholarships\nMarket decides on the price of Axies and with a high entry cost of starting with the game, a new model of onboarding has emerged. Newcomers (scholars) can “rent out” the Axies for a certain period of time and negotiate with the owners (managers) the terms of the profit distribution. This is not a official in-game feature and is enabled by having the managers controlling the Ronin wallet and scholars controlling the user account associated with it. This leads to a bad position for the scholar as the profits are claimed by the manager and then the scholar’s share is sent to the their wallet’s address. Scholars are essentially at the mercy of the managers as their earnings and the scholarship itself can be revoked at any time.\nMore fairer way of enabling Scholarships, would be to have a smart contract that would have complete control of the Axies and through which the scholars would make in-game actions. The profits would go to the contract’s address that would perform a fair split. The terms of the agreement (the minimum amount of profits to be earned by the scholar) and the scholarship’s time period would be embedded in the contract. If both parties agree, the terms could be changed later on."
  },
  {
    "objectID": "documents/research/posts/ERFC-37.hugo.html#game-breakdown",
    "href": "documents/research/posts/ERFC-37.hugo.html#game-breakdown",
    "title": "Gaming DAPPs - Play to Earn model",
    "section": "Game Breakdown",
    "text": "Game Breakdown\nThe game itself is organized in 1v1 matches where players do not have any influence against who they will get matched. Matching is done by the server based on players’ Match Making Rankings (MMR) which is determined by the win/lose ratio of those players. All of this could be implemented in a smart contract which would keep track of all of the players’ MMRs and update them after each match. Players could start a new match or join an existing one if the absolute difference in their MMRs is under a certain threshold. The benefits of this approach is that they could also choose what match they will join or challenge a specific player.\nWinner of each match gets some amount of game’s “Smooth Love Potion” (SLP) tokens. That amount is dependent on the MMR of that player (the higher the MMR the more SLP tokens they will win). SLP is an inflationary ERC20 token that gets minted after each match. Even though there are SLP burning mechanisms through some in-game actions, most players opt to cash out their winnings so this might not be a viable economic model. One alternative would be to have both players stake some amount of tokens in a match with the winner taking the sum of those stakes. The problem is that it introduces betting connotations and games go to extreme lengths in order to not be considered a betting game as it potentially introduces regulation.\nInside the game, there is also “Axie Infinity Shard” (AXS) token. AXS is an ERC20 token which has a fixed total supply. The company behind the game has roughly 20% of the total AXS supply and small amounts of AXS is distributed to the top players of the month.\nThe game makes heavy use of Axies which have certain characteristics. Players do not completely own Axies as it was discovered that the company can freeze them, making them useless. Once an Axie is frozen, it cannot be used in the game nor it can be traded on the Axie Marketplace. This possess a major concern as the players’ assets are constantly under a threat of those players being banned from the game by the Sky Mavis company. Full list of violations that will result in a ban can be seen here6 .\nEach Axie has one of the 9 classes, 4 statistics and 4 cards associated with it. Classes are grouped into three groups that form a “rock paper scissor” relationship. Meaning that, group G1 does 15% extra damage to group G2 but takes 15% extra damage when attacked by group G3. Statistics determine the Health, Skill, Speed and Morale of an Axie. This statistics affect the matches and their state transitions as the Speed for example determines the order of attacks. Cards can have positive or negative effects on an Axie as well additional effects that affect the players. Four cards associated with an Axie are added to the player’s deck when that Axie is used inside a match. More information about Axies is provided in Appendix A and the way new Axies are created is provided in the Breeding section of the Whitepaper7 ."
  },
  {
    "objectID": "documents/research/posts/ERFC-37.hugo.html#matches",
    "href": "documents/research/posts/ERFC-37.hugo.html#matches",
    "title": "Gaming DAPPs - Play to Earn model",
    "section": "Matches",
    "text": "Matches\nOne match consists out of:\n\nBoth players choosing a team of 3 Axies and deciding on their positions on the field (creating team formations)\nRounds being played out until one of the players doesn’t have a standing Axie\n\n\n\n\n\nAxieMatch\n\n\nPicture 1 : Axie Infinity Match View\n\n\nTeam formations\nThe positioning of an Axie matters because it determines what Axie will take the damage from the enemy. Each player has 5 rows where an Axie can be positioned. The closest row to the enemy lines will be attacked first. The Axies cannot change their position and they stay where they initially were until they get knocked out. Choosing of a team formation would require two transactions per player. Those transactions would be organized in the commit-reveal scheme so that the players wouldn’t have an advantage against the opponent that naively sent the transaction revealing their team and their positions.\n\n\nRounds\nEach Round is carried out in the following order:\n\nPlayers randomly draw 3 cards* from their own decks (consisting of 24 Cards**) and decide on what cards they will play\nCards are revealed and their affects are applied to Axies\nBattle of the Round takes place\nIf one of the players doesn’t have a standing Axie then the match is over\n\n* Exception is that in the first Round, players draw 6 cards.\n** There are 3 Axies per team, each Axie adds two copies for each of the 4 cards. So in total there are 3*2*4 = 24 cards in one player’s deck.\n\n\nDrawing of Cards\nOnce the teams have been revealed both players know each other decks. However, they should not know what cards the opponent has in their hands and so they should not know what cards they have yet to draw.\nOne scheme that could be applied is the following:\n\nHave both players choose their secret “random” number and commit to it with a hash\nHave the game’s contract ask for a random number from Chainlink’s VRF8\nOnce the randomness has been fulfilled, both players know what is the order of cards they will be drawing as the seed for shuffling their decks will be some function of their own number and the received number from Chainlink’s VRF\n\nThe drawing order of cards in the deck is now known only to the players. During the match, each player can play any card inside their deck and until a match ends, players trust each other that the played card was in the opponent’s hand. However, when the match ends both players would need to reveal their secret numbers as the contract needs to verify if they honored the drawing order and if the card they played at a certain moment was one of the cards they were holding. The first discrepancy would end the verification process and the cheater would be penalized.\nOne additional subproblem is that after each round the remaining cards inside the deck should be reshuffled. This could be done with requesting another random number which would help form a seed for random shuffling of the remaining cards. The verification process would need to be modified to support this.\nNote: after all of the cards have been drawn, the deck resets.\n\n\nReveal of the Cards\nThe reveal of the cards would also need two transactions per player so that one player wouldn’t just wait for the opponent to reveal their cards and then change their strategy accordingly.\nOnce cards have been revealed, before the battle takes place, the effects of the played Cards are applied. In total there are 3 constant effects (Attack damage, Defensive points and the cost of playing that Card) and 19 additional effects that a Card may have. For example, one of those additional effects are an increase/decrease in one of the Axie statistics. When multiple Cards that affect the same Axie are played, their effects are accumulated. More information about cards is provided in Appendix A .\n\n\nBattle of the Round\nBattle rules:\n\nthe order of attacks is determined by the highest Speed statistic (if there is a draw then it is decided by the lowest Health and then by lowest ID of an Axie)\nclosest row with a standing Axie to the enemy lines will take the damage\nif two Axies are inside the row taking the damage there’s a 50/50 chance which Axie will absorb the damage\n\nAll of this logic could be kept inside a contract. One of that things that would need to be taken into account is that Axies attack in the pre-defined order. So during a Round, an Axie that has not yet attacked could be knocked out of the game and so the Cards associated with it, that were played in the Round, should lose their effects. Also, when an Axie gets knocked out, all of their cards should be removed from the deck."
  },
  {
    "objectID": "documents/research/posts/ERFC-37.hugo.html#summary-of-proposed-matched-progression",
    "href": "documents/research/posts/ERFC-37.hugo.html#summary-of-proposed-matched-progression",
    "title": "Gaming DAPPs - Play to Earn model",
    "section": "Summary of proposed matched progression",
    "text": "Summary of proposed matched progression\n\nInital proposal\n\n\n\n\n\n\n\n\n\n\n\nStep\nPhase\nDescription\nNumber of transactions\nDone by\n\n\n\n\n1\nMatch Set Up\nStarting of a Match\n1\nPlayer 1\n\n\n2\nMatch Set Up\nJoining a Match\n1\nPlayer 2\n\n\n3\nMatch Set Up\nCommitting to a team formation and setting of a secret number\n2*1\nPlayer 1 and Player 2\n\n\n4\nMatch Set Up\nRevealing team formations\n2*1\nPlayer 1 and Player 2\n\n\n5\nRound\nCommitting to Cards that will be played in this Round\n2*1\nPlayer 1 and Player 2\n\n\n6\nRound\nRevealing Cards of the Round\n2*1\nPlayer 1 and Player 2\n\n\n7\nMatch Wrap Up\nRevealing secret numbers after the match has ended\n2*1\nPlayer 1 and Player 2\n\n\n\n\nTotal = (1+1+2+2) + N*(2+2) + (2) = 8 + 4*N ; where N is the number of rounds in the match\nEven with just 3 rounds the number of transactions per player would be 16, so this is a problem.\n\n\nImproved Match Progression\nSomething that could be done is the introduction of “fair play” where for one round’s commit stage, one player would transmit both of the commit messages to the chain after which the other player would transmit both reveal messages.\nIf the player transmitting commitment messages decides to not transmit them, then the match is stuck. However, the game can be structured in a way such that it is in the interest of both players to resolve the match regardless of the outcome. This could be done by staking some amount of tokens that the players will receive at the end of the match.\nIf the player transmitting the reveal messages realizes he will lose and decides to not transmit them, then the first player could transmit just their own reveal message that would start a countdown for the other player to transmit their reveal message. If the countdown is reached then the player that didn’t act in the spirit of fair play is penalized in some way.\n\n\n\n\n\n\n\n\n\n\n\nStep\nPhase\nDescription\nNumber of transactions\nDone by\n\n\n\n\n1\nMatch Set Up\nStarting of a Match\n1\nPlayer 1\n\n\n2\nMatch Set Up\nJoining a Match\n1\nPlayer 2\n\n\n3\nMatch Set Up\nTransmitting commitments to the team formations and setting of a secret number\n1\nPlayer 1\n\n\n4\nMatch Set Up\nTransmitting the Reveal of team formations\n1\nPlayer 2\n\n\n5\nRound\nTransmitting Commitment Messages\n1\nPlayer 1\n\n\n6\nRound\nTransmitting Reveal Messages\n1\nPlayer 2\n\n\n7\nMatch Wrap Up\nRevealing secret numbers after the match has ended\n1\nPlayer 1\n\n\n\n\nThis scheme would give the total number of transactions of:\nTotal = (1+1+2) + N*(2) + (1) = 5 + 2*N ; where N is still the number of rounds in the match\nWith the number of rounds equal to 3, the number of transactions per player is less or equal to 6.\nThis is an improvement but maybe it could be brought down further. Also, signing 6 transactions per match would still break the flow of the game, so the players would have to have a substantial financial interest in continuing to play the current match and the game itself."
  },
  {
    "objectID": "documents/research/posts/ERFC-37.hugo.html#appendix-a",
    "href": "documents/research/posts/ERFC-37.hugo.html#appendix-a",
    "title": "Gaming DAPPs - Play to Earn model",
    "section": "Appendix A",
    "text": "Appendix A\nEach Axie has a fixed set of attributes - a class, 4 stats and 4 cards.\n\n\n\n\nAxie\n\n\nPicture 2: Axie characteristics\n\n\nAxie Classes\n\n9 of them (Plant, Reptile, Dusk, Beast, Bug, Mech, Aqua, Bird, Dawn)\n6 main classes (Plant, Beast, Bug, Reptile, Aqua, Bird, Dawn)\n3 remaining are “secret” (and generally weaker in the game)\nto make the strengths of classes balanced, they are grouped in 3 groups, forming a “rock paper scissor” relationship\n\n\n\nAxie Stats\n\ncan be divided into “base” and “additional” Stats\ntotal of 140 points is distributed between 4 of the base Stats (with each class having its own base distribution)\n\nHealth — amount of damage Axie can take before getting knocked out\nSpeed — affects the order in which Axies attack in a match (higher Speeds attack first)\nSkill — increases damage dealt when the Axie performs multiple cards/moves (a.k.a. combo)\nMorale — increases chance to land a critical hit, as well as entering “last stand” which allows them to attack a few more times before getting knocked out\n\n\n\nBase Stats Distribution\n\n\n\nClass\nHealth\nSpeed\nSkill\nMorale\n\n\n\n\nAqua\n39\n39\n35\n27\n\n\nBeast\n31\n35\n31\n43\n\n\nBirds\n27\n43\n35\n35\n\n\nBug\n35\n31\n35\n39\n\n\nPlant\n61\n31\n31\n41\n\n\nReptile\n39\n35\n31\n35\n\n\n\n\n\nadditional Stats depend on Axie’s Body Parts\n\n\n\nAxie Body Parts\n\nthere are 6 of them (Eyes, Ears, Horns, Mouth, Back, Tail)\nonly Horns, Mouth, Back, Tail have an associated card with it (one card per body part)\nIn total, there are:\n\n4*6 types of Mouth\n6*6 types of Horns\n6*6 types of Back\n6*6 types of Tail\n\ninside groups of those types each of the 6 main classes is equally represented\nif the class of an Axie matches with the type of a body part, Axie’s stats will increase\n\n\nAdditional Stats Increase\n\n\n\nClass/Type\nHealth\nSpeed\nSkill\nMorale\n\n\n\n\nAqua\n+1\n+3\n0\n+0\n\n\nBeast\n0\n+1\n0\n+3\n\n\nBirds\n0\n+3\n0\n+1\n\n\nBug\n+1\n0\n0\n+3\n\n\nPlant\n+3\n0\n0\n+1\n\n\nReptile\n+3\n+1\n0\n0\n\n\n\n\n\nAn Axie is a ‘pure breed’ when its class and all of its body parts are of the same type.\n\n\n\nAxie Abilities (Cards)\n\nthere are 132 cards in total (first divided by 6 main classes and then by the body parts)\neach Axie has 4 cards associated with it\neach Card has\n\nan amount of Energy it costs to play it\nattack points - damage it does to the enemy Axie\ndefensive points - forms a shield that takes the damage instead of Axie’s Health\npotentially buffs/debuffs\nadditional effects (draw another card, steal some Energy from the opponent,…)\n\ncards realize a “combo” when 2 or more of them are played (for the same Axie)\ncards realize a “chain” when 2 or more Axies use 2 or more cards that are from the same class\n\n\n\nAxie Card Buffs/Debuffs (Card Effects)\n\nthere are 3 buffs (positive effects on the Axie) and 16 debuffs (negative effects on the enemy Axie)\nthey affect the Axie for one or more turns\nthey are “stackable” - their effects are accumulated\n\n\nList of Buffs\n\n\n\nName\nDescription\n\n\n\n\nAttack+\nIncrease next attack by 20%\n\n\nMorale+\nIncrease Moral by 20% for the following round\n\n\nSpeed+\nIncrease Speed by 20% for the following round.\n\n\n\nList of Debuffs\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nAroma\nTarget priority changes to this Axie until the next round.\n\n\nAttack-\nDecrease next attack by 20%.\n\n\nChill\nAffected Axie cannot enter Last Stand\n\n\nFear\nAffected Axie will miss their next attack\n\n\nFragile\nAffected Axie’s shield will take double damage from the next incoming attack\n\n\nJinx\nAffected Axie cannot land critical hits\n\n\nLethal\nNext incoming attack is a guaranteed critical strike\n\n\nMorale-\nDecrease Morale by 20% for the following round\n\n\nPoison\nAffected Axie will lose 2 HP for every card used\n\n\nSleep\nNext incoming attack will ignore shields\n\n\nSpeed-\nDecrease Speed by 20% for the following round\n\n\nStench\nAffected Axie will lose target priority for the following round\n\n\nStun\nAffected Axie’s first attack will miss.Next incoming attack will ignore shields\n\n\nCannot Be Healed\nThis Axie cannot be healed or recover health. This Debuff cannot be removed"
  },
  {
    "objectID": "documents/research/posts/ERFC-105.hugo.html",
    "href": "documents/research/posts/ERFC-105.hugo.html",
    "title": "Transaction splitting",
    "section": "",
    "text": "Introduction\nThis paper tends to explain and show how the need for trust can be eliminated when it comes to donations and setting up business models in general. Idea is to have a platform that will allow you to set up a use case in which you want to allow users to buy a product from you where a specific percentage of the purchase will go to a different wallet(s).\n\nX% of purchase transparently goes to verified charity wallet\na split transaction between different co-creators or marketplace and author\n\nWe can develop a fully transparent set of smart contracts and protocols on some general-purpose blockchain and introduce to sellers and customers a brand new way for selling goods and doing business in general.\nAdditionally, if there are too many different wallet recipients, the splitting can be done through merkle tree claim model.\n\n\nGoals & Methodology\n\nGoals\n\nHow to make charity donations and spending transparent\nEliminate the need for trusting the brand, and provide customers a way to easily verify their spendings\nSplit input transaction to several different wallets by desired percentage\n\n\n\nMethodology\n\nWrite a set of smart contracts in Solidity for this purpose.\n\nPros:\n\nPortable to any EVM general purpose blockchain\nNative support for cryptocurrencies\nPossibility for including ERC-20 tokens, as well\nTooling (Hardhat, OpenZeppelin, etc.)\n\nCons:\n\nLack of native percentage operator in Solidity, one need to decide up to which decimal the result is reliable\nTransaction costs\n\n\nIntegrate it with Bizzswap\n\nPros:\n\nOut of the box solution for paying/swapping coins and tokens\n\nCons:\n\nThird-party dependency\n\n\nTesting\n\nUnit tests\nIntegration tests\nEnd to end tests\nStatic analasys\nCode coverage\nGas usage\nFuzz testing (optional)\nFormal verification (optional)\nAudit\nDeployment to testnet\n\n\nThe beauty of the end result is that anyone can port a frontend app to this decentralized protocol and incorporate it in their business models, already existing platforms, etc.\n\n\n\nResults & Discussion\nTransaction can be split among several parties in Solidity like this:\n\nStep 1)\nSmart contract should have the ability to receive native coin or any other token. There is a need for a receive() or fallback() function, depends on the implementation of the smart contract.\nreceive() is called if msg.data is empty, otherwise fallback() is called.\n    /**\n    Which function is called, fallback() or receive()?\n\n           send Ether\n               |\n         msg.data is empty?\n              / \\\n            yes  no\n            /     \\\nreceive() exists?  fallback()\n         /   \\\n        yes   no\n        /      \\\n    receive()   fallback()\n    */\n\n    // Function to receive Ether. msg.data must be empty\n    receive() external payable {}\n\n    // Fallback function is called when msg.data is not empty\n    fallback() external payable {}\n\n\nStep 2)\nSmart contract then needs to split the received amount among other wallets by defined percentage. Sending funds is trivial and we won’t focus on that. The most challenging part is proper ratio calculations in a language with no native support for decimal arithmetics.\nIn Solidity, one must assume that all numbers have 18 decimal precision. For example:\n\n1 is 1000000000000000000,\n0.5 is 500000000000000000, and\n100 is 100000000000000000000\n\nSince there are no native language support for percentage arithmetics in Solidity, devs are using Basis Points as a unit of measurement equal to 1/100th of 1 percent. This metric is commonly used for loans and bonds to signify percentage changes or yield spreads in financial instruments, especially when the difference in material interest rates is less than one percent.\n\n0.01% = 1 BPS\n0.05% = 5 BPS\n0.1% = 10 BPS\n0.5% = 50 BPS\n1% = 100 BPS\n10% = 1 000 BPS\n100% = 10 000 BPS\n\nPossible implementation:\n    function calculateFee(uint256 _amount) public pure returns(uint256) {\n        // for example 1.85% is fee\n\n        return _amount * 185 / 10000;\n    }\n\n\n\nConclusion\nThere are no blockers for implementing this idea in Solidity. The tooling is stable, the math is not complex and can be handled by the language, and the blockchain technology itself is capable for storing this type of applications.\n\n\nBibliography"
  },
  {
    "objectID": "documents/research/posts/ERFC-57.hugo.html",
    "href": "documents/research/posts/ERFC-57.hugo.html",
    "title": "Solidity++ (S++)",
    "section": "",
    "text": "Introduction\nJavascript introduced classes in ES6 which are very useful construct for writing clean and readable code. Browser, however, could not understand ES6 code so the code needed to be transpiled into ES5. Programmers could use classes to write logic and transpiler took care of transforming the code into an optimised ES5. The same parallel could be made with Solidity. Programmers should be focused on writing business logic in Solidity-like S++ code and the code should be automatically transpiled into efficient Solidity code. S++ should not drastically modify basic Solidity syntax, but introduce new annotations and helper functions that would enable efficient transpiling to pure Solidity code.\nThere are multiple ways to improve the smart contract code on bytecode level. This project will focus on improvements on Solidity code level but the end product - optimised Solidity code will be compiled into bytecode and further optimisations on bytecode level can be performed.\nOptimisations on code level include variable declaration ordering (state variables, local variables and structs), which correlates with the order of memory block stacking and directly influences the costs of storing the data. Some variables might occupy more space than needed to store values in ranges that require less bits than the closest Solidity data type. The most extreme example is boolean data type which stores true/false values, that could be stored in 1 bit of memory, but really use an entire byte. Accessing storage structs requires more operations than accessing a local variable, which becomes obvious problem when it is done in loops. Any fixed sized data type, like, int64, bytes32 and so on, are always cheaper than dynamic data types, such as string, and those dynamic types should be replaced with fixed ones wherever is possible (wherever the value range of the variable is known). Cost reduction can even be achieved by deleting variables - freeing blockchain space. Execution of a code that performs delete command on some variable/mapping value/… rewards executor with a refund of up to 50% of transaction cost, depending on the amount of freed space. There were some attempts to remove this feature (https://eips.ethereum.org/EIPS/eip-3298) but so far it is still valid and exploitable. Usage of lazy evaluation is, an in other languages, preferred way of saving execution steps, which for Solidity directly equals saved money.\n\nExisting solutions\nThere are optimisations on loop level [1] that do not include variable level optimisations but recognise code patterns that are classified into several categories that can be automatically optimised. Also, there are papers [2] that uncover additional space for optimisation, such as removing conditions that always equal the same value or values in the loops that never change. There are also blog advices for writing optimised smart contracts, such as [3], that can and will be referenced when implementing automatisation methods.\n\n\n\nGoals & Methodology\nSo far, there is no tool or IDE that either automates these optimisations or includes multiple improvements in one software. The goal of this project is to evaluate feasibility of implementing such tool and Solidity language extensions that could allow its easier implementation. End PoC toll planned as a result of this project focuses on space optimisation techniques and includes:\n\nImplementing parser for Solidity language that enables language semantic and syntax analysis\nImplementing optimisation methods:\n\nVariable declaration reordering\nStruct values declaration reordering\nDefining data type extensions for storing custom-sized data (i.e. integers with values between 20 and 40) and implementing language parser extensions\n\nImplementing generator for optimised code\n\n\n\nResults & Discussion\nProto-parser for Solidity code, which includes basic PoC constructs was implemented, as well as variable declaration reordering to enable more efficient space usage; The plan is to extend the parser and to include other optimisation techniques listed in the previous segment.\n\nVariable declaration reordering\nVariable declaration reordering is a problem synonymous to binning problem, where the goal is to pack objects of a certain size into bins of fixed size in any order using the least number of bins. The problem is a known optimisation problem which comes from NP (non-deterministically polynomial) class and has no known efficient solution. The approach for implementing variable declaration reordering was using branch-and-bound algorithm which resulted in decent reordering time of less than 10 seconds for up to 16 variables in one block. The reordering was performed per block. The same algorithm will be performed on structs.\n\n\nData type extensions\nTo enable extensions of data types there are two steps:\n\nDefining language extensions\nMap variables to bits of memory blocks and create getters and setters\n\nNew data types will be mapped to memory blocks, variables of 256 bits, on bit level using bit masks. Packing of those values will be optimised using the previously defined variable stacking algorithm\nProposed language extension of integer data types consists of using the existing base type (int, uint) followed by value range given in brackets. Example custom type uint<10, 1030> represents unsigned integer values between 12 and 98. The optimiser would determine the closest known data type that covers the range (in this case uint16). The real number of bits required to store value 1030 is 11 and the closest data type is 16 bits long which means that 5 bits are redundant. First level of optimisation may be using 11 bits mapped to a memory block of 256 bits and implementing getters and setters to interact with the value, which is casted to uint16 when used. But there is another hidden optimisation that can further improve space management. Notice that the upper limit of the interval is indeed 1030 which required 11 bits to store but the lower limit is 10 which means that total number of values is 1030 - 10 = 1020 and that many values can be stored in 10 bits of memory, so the optimisation will take into account the overall interval range and generate getters / setters that will include offsets before storing/loading values.\nBoolean type optimisation will be straight forward and, while it will keep the type name, it will be mapped to only one bit of memory.\nString data types would be extended with indicator of the number of characters as string<10> will represent fixed sized string that will be mapped to bytes.\n\n\nCode generation\nThe parser generates three-like structure of the Solidity code. Every statement or declaration is stored as an object with parameters related to the nature of the statement/declaration. For example, declaration uint256 private var1 is stored as VariableDeclaration object with attributes type=uint256, modifier=private identifier=var1. Each object contains information to reconstruct itself back to string from. This type of organisation enables easy traversal through code, easy recognition of higher-level constructs, such are infinite loops, and easy reconstruction to Solidity code (in this case - optimised code).\n\n\n\nConclusion\nThe feasibility of the proposed project is confirmed by implementing PoC prototype, the optimisation evaluation on the existing smart contracts is yet to come but even the simple reordering of variables that reduces the number of used memory blocks clearly shows benefits of money savings (saved memory) and time savings (eliminated thinking about such generic problem that can and should be automatised). The borderline is - it makes no harm to use it, it can only be an improvement.\nThe value of this project is assumed, but still has to be confirmed. That is why the next step of this research should be further assessment by target user group, which should include developers that are actively using Solidity in their everyday work and domain experts.\n\n\nAppendices\n\n\nBibliography\n[1] B Mariano, Y. Chen, Y. Feng Demystifying Loops in Smart Contracts https://fredfeng.github.io/papers/ase20-consul.pdf [2] T. Brandstaetter Optimisation of Solidity Smart Contracts https://repositum.tuwien.at/bitstream/20.500.12708/1428/2/Brandstaetter%20Tamara%20-%202020%20-%20Optimization%20of%20solidity%20smart%20contracts.pdf [3] https://mudit.blog/solidity-gas-optimization-tips/ ::: {#refs} :::"
  },
  {
    "objectID": "documents/research/posts/ERFC-261.hugo.html",
    "href": "documents/research/posts/ERFC-261.hugo.html",
    "title": "SBT - Soulbound Tokens",
    "section": "",
    "text": "This paper explores the SBTs (Soulbound Tokens) as a concept, their potential use cases, and the difficulties of implementing this solution. It also covers concepts of social and community recovery and briefly covers designated-verifier proofs and verifiable delay functions as they are potential enablers of this solution.\nSBTs as a concept have major potential, although they face major obstacles like legacy systems, privacy and cold start issues. Nevertheless there is a positive sentiment towards this solution."
  },
  {
    "objectID": "documents/research/posts/ERFC-261.hugo.html#some-of-the-current-problems-in-web3",
    "href": "documents/research/posts/ERFC-261.hugo.html#some-of-the-current-problems-in-web3",
    "title": "SBT - Soulbound Tokens",
    "section": "Some of the current problems in Web3",
    "text": "Some of the current problems in Web3\nIf we look at the Web2 as it is now, the online identities of various users are a major part of various processes. Users can even use their identity on other platforms to easily register to others with a click or two. For example, a Google account In the future, unique identity linked to a user (in this case a “Soul”) could be a solution that would bring onboard more people to Web3, unlock new potential uses-cases, and potentially create a standardized reputation system. Currently Web3 is dependent on various Web2 structures regarding representing social identity. Examples:\n\nNFT platforms rely on centralized platforms for a proof of scarcity and initial provenance.\nDAOs. If DAOS want to prevent Sybil attacks, they usually rely on social media profiles for proof of personhood.\nWeb3 participants rely on centralized custodial wallets. Wallets that offer decentralized key management are not user-friendly.\nGenerally, it’s hard to establish reputation systems and manage blacklisting as anyone can create another address and access your protocol, for example\nLack of native web3 identity. This makes undercollateralized lending virtually impossible.\nGovernance vulnerability to financial attacks.\n\nThese problems could, in theory, as the paper mentioned earlier says, be solved by introducing SBTs in Web3."
  },
  {
    "objectID": "documents/research/posts/ERFC-261.hugo.html#what-are-souls-exactly",
    "href": "documents/research/posts/ERFC-261.hugo.html#what-are-souls-exactly",
    "title": "SBT - Soulbound Tokens",
    "section": "What are souls exactly?",
    "text": "What are souls exactly?\nSouls are accounts or wallets that hold publicly visible, non-transferable tokens. Those tokens could pottentially also be revocable by the issuer.2\nThe tokens that the Soul account or wallet owns should be SBTs. Soul could, in theory, hold various types of tokens, but that possibility isn’t explored in the paper. SBTs could be used to represent affiliations, credentials, memberships, etc. but more on that in the following paragraphs. The true power of these tokens lies if there was a possibility for SBTs to be issued and verified by other Soul accounts that are counterparties in the relationship. These counterparty Souls could be individuals, companies, or institutions.\nAnother essential property of Soul accounts or wallets is the abundance of a requirement for a soul to be linked to a legal name or a need to ensure that there is one soul account per human. Soul accounts or wallets could also be possibly transferred across humans.\nSouls can also be a type of reputation signal of the user to the ecosystem. Depending on the SBTs that the soul account has, the user could have a positive or negative reputation. A positive reputation could give the user various benefits regarding products and services, and a negative one could prevent users from accessing them. This property can also pose a problem protocols could “redline” (discriminate) owners of some SBTs and prevent them from using their product."
  },
  {
    "objectID": "documents/research/posts/ERFC-261.hugo.html#possible-use-cases-of-sbts",
    "href": "documents/research/posts/ERFC-261.hugo.html#possible-use-cases-of-sbts",
    "title": "SBT - Soulbound Tokens",
    "section": "Possible use cases of SBTs",
    "text": "Possible use cases of SBTs\n\nSBTs and Lending\nIn traditional finance, reputation is a significant factor in uncollateralized lending. This system often relies on centralized credit scores of borrowers to gauge creditworthiness. However, this has flaws like not providing lending services if there is insufficient data on the borrower and discrimination.\nIn Web3, users must overcollateralize in the token of their choice to receive a loan. This is where the SBTs could, in theory, provide a solution.\n“Implementation and adoption of SBTs have a potential to unlock a censorship-resistant, bottom-up alternative to top-down commercial and”social” credit systems.” - E. Glen Weyl, Puja Ohlhaver, Vitalik Buterin\nIn the case of lending, SBTs could represent education credentials, work history, and rental contracts, which could signal “creditworthiness”.\nThe loans themselves could be represented by the SBTs, which could be burnable by the institution that has given the loan. After the burning of the token, an institution could send another SBT to the borrower. This time it would be a proof of timely repayment SBT. This token could serve as a “signal” to other lenders that this borrower returns his/hers loans on time, which would impact the borrower’s “credit score” and provide the borrower with better loan conditions. Non-transferability prevents transferring or hiding outstanding loans. A rich ecosystem of SBTs ensures that borrowers who try to escape their loans (creating a new soul) will have insufficient SBTs to stake their reputation.\n\nCommunity lending market\n“SBTs would offer a substrate for community lending practices similar to those pioneered by Muhammad Yunus and the Grameen Bank, where members of a social network agree to support one another’s liabilities. Because a Soul’s constellation of SBTs represents memberships across social groups, participants could easily discover other Souls who would be valuable co-participants in a group lending project.Whereas commercial lending is a”lend-it-and-forget-it” until repayment model, community lending might take a “lend-it-and-help-it” approach—combining working capital with human capital with greater rates of return.” - E. Glen Weyl, Puja Ohlhaver, Vitalik Buterin\n\n\nWhat are the first steps?\nThere are a couple of “requirements” for this type of lending to be true:\n\nSoul accounts/wallets would carry SBTs they are comfortable sharing publicly. This could be an excellent first step for the adoption of social/intra-community lending in Web3.\nSocial relationships and credentials would play a significant role in this type of lending.\n\n\n\n\nSBTs and NFTs\nIn terms of NFTs , Souls could play a major role in terms of artist’s reputation. When issuing NFTs artist could issue them from their Soul.\n“The more SBTs the artist’s Soul carries, the easier it would be for buyers to identify the Soul as belonging to that artist, and thereby also confirm the NFT’s legitimacy. Artists could go a step further to issue a linked SBT stored in their Soul that attests to the NFT’s membership to a”collection” and vouches for whatever scarcity limits the artist wishes to set. Souls would thus create a verifable, on-chain way to stake and build reputation on the provenance and scarcity of an object.” - E. Glen Weyl, Puja Ohlhaver, Vitalik Buterin\nThe application of SBTs in this market extends beyond art. Some examples of potential use cases:\n\nvarious services\nrentals/property\nauthentication\nsocial provenance\nretail\ngaming\nand many more, SBTs unlock the use cases where NFTs cannot be applicable\n\n\n\nSoul Accounts in Airdrops and DAOs\nSoulbound Tokens could also enable communities to be convened at the intersection of souls and to form a DAO, for example. Drops of SBTs or “Souldrops” can be given based on SBTs and other tokens within a Soul (soul account/wallet). Some examples:\n\nconference attendees\ncertified programmers\nearly members\netc\n\n“Souldrops could also introduce novel incentives to encourage community engagement. Dropped SBTs could be engineered to be soulbound for a period but eventually”vest” into transferable tokens over time. Or the reverse could be true. Transferable tokens held for some period could unlock the right to SBTs that confer further governance rights over a protocol. SBTs open a rich possibility space to experiment with mechanisms that maximize community engagement and other goals, like decentralization” - E. Glen Weyl, Puja Ohlhaver, Vitalik Buterin\nIn DAOS, SBTs could be used to mitigate Sybil attacks in various ways:\n\ncomputing over a Soul’s constellation of SBTs to differentiate between unique Souls and probable bots and denying any voting power to a Soul that appears to be a Sybil.\nconferring more voting power to Souls with more reputable SBTs — like work or educational credentials, licenses, or certifications.\nissuing specialized “proof-of-personhood” SBTs could help other DAOs bootstrap Sybil resistance.\nchecking for correlations between SBTs held by Souls who support a particular vote and applying a lower vote weight to highly correlated voters.\n\nSouls and SBTs could also be used to estimate the decentralization degree in the governance of DAOs and protocols.\n\n\nProperty\nSo far, NFTs could not effectively be applied to property rights, considering their ease of transfer. Using SBTs, owners could set different rights and limitations for the same property (vehicles, real estate, events, etc.)."
  },
  {
    "objectID": "documents/research/posts/ERFC-261.hugo.html#recovery-of-soul-accountswallets",
    "href": "documents/research/posts/ERFC-261.hugo.html#recovery-of-soul-accountswallets",
    "title": "SBT - Soulbound Tokens",
    "section": "Recovery of Soul Accounts/Wallets",
    "text": "Recovery of Soul Accounts/Wallets\nSoul accounts would probably be recoverable by using Social Recovery.\n\nSocial recovery\nA social recovery system/wallet works as follows:\n\nThere is a single “signing key” that can be used to approve transactions\nThere is a set of at least 3 (or a much higher number) of “guardians”, of which a majority can cooperate to change the signing key of the account. The signing key can add or remove guardians, though only after a delay.\n\nA social recovery wallet can be used as a regular wallet. In case of losing a key, the user can reach out to their guardians and ask them to sign a particular transaction to change the signing pubkey registered in the wallet contract to a new one.\nGuardians can be:\n\nother devices\nfriends and family members\ninstitutions (they can verify your identity by phone number, e-mail, etc.)3\n\n\n\nSBT community recovery\nIn this proposed solution, Soul recovery is tied to the Soul’s memberships across communities.\n“In a community recovery model, recovering a Soul’s private keys would require a member from a qualified majority of a (random subset of) Soul’s communities to consent.” - E. Glen Weyl, Puja Ohlhaver, Vitalik Buterin.\nThis recovery implies secure, off-chain communication channels where authentication can occur.\n\n\n\nSBT rec\n\n\nPicture 1: Social recovery vs SBT community recovery\nBy embedding security in sociality , a Soul can always regenerate their keys through community recovery, which deters Soul theft (or sale): because a Seller would need to prove selling the recovery relationships, any attempt to sell a Soul lacks credibility. - E. Glen Weyl, Puja Ohlhaver, Vitalik Buterin.\nThis recovery solution is just a proposition and requires more experimentation."
  },
  {
    "objectID": "documents/research/posts/ERFC-261.hugo.html#implementation-and-adoption-challenges",
    "href": "documents/research/posts/ERFC-261.hugo.html#implementation-and-adoption-challenges",
    "title": "SBT - Soulbound Tokens",
    "section": "Implementation and adoption challenges",
    "text": "Implementation and adoption challenges\n\nPrivacy\nOne of the biggest challenges in the adoption of SBTs is privacy. Too many public SBTs that a soul possesses can reveal too much information about a soul.\nBlockchain systems are public by default, and every transaction and relationship recorded on-chain is available for everyone in the world to see. One possible solution is to have separate souls for professional and private life. These souls can easily be linked if there are no serious privacy solutions.\nAnother solution is to have SBTs that could store data off-chain, leaving only the hash of the data on-chain.\n\n\n\nPrivacy with SBTs\n\n\nPicture 2: A way to keep some of the SBTs private\nThe choice of how to store data is left to the person. Possible solutions are:\n\ntheir own devices\na trusted cloud service\nIPFS or other decentralized networks\n\nZero-knowledge proofs are another possible solution that could help kickstart the adoption of SBTs. They also can allow people to prove arbitrary statements without revealing any more information beyond the statement itself. They can be computed over SBTs to prove characteristics about a Soul. Privacy could be extended further by introducing multi-party computation techniques like garbled circuits.\nOther possible solutions for privacy problems include designated-verifier proofs and verifiable delay functions.\nFor example: If user A wants to prove some property about its SBTs to user B, they can make a zero-knowledge proof of the statement “I hold SBTs that have the property Z.” User B can then be sure as he didn’t make the proof. But what about passing somebody else’s proof? Users can mitigate this by using verifiable delay functions. Using verifiable delay functions, user A can make and present a proof that can only be made with required SBTs at the moment, but anyone else will be able to make five minutes from now.\n\n\nBribing the owners of the SBTs\nOwners of the SBTs could be bribed by various parties in order to influence their voting or to exploit their other SBTs.\nIn the research paper, writers mention these ways of exploits mitigations:\n\n“The ecosystem of SBTs could bootstrap of”thick” community channels , where SBTs signal authentic off-chain community membership with strong social bonds and repeat interactions. This would make it easier for communities to alter and revoke SBTs of impersonators and bots. Such thick channels—which we often and in churches, workplaces, schools, meet-up groups, and organizations in civil society—would provide a more sybil-resistant social substrate to police gaming (e.g., through bots, bribes, impersonation) in more “thin” social channels.\nNested communities could require SBTs to force context on potential collusion vectors “just below” them . For example, if a state were holding a funding round or vote, the state might require every participating citizen to also hold an SBT of a defined county and municipality.\nThe openness and cryptographic provability of the SBT ecosystem could itself be used to actively detect collusive patterns and penalize inauthentic behavior —perhaps discounting the voting power of collusive Souls, or obliging Souls to accept SBTs representing negative attestations.\nZK technology (eg. MACI ) could cryptographically prevent some attestations made by a Soul from being provable.\nEncouraging of whistleblowers\nMechanisms from peer-prediction theory\nCorrelation scores that focus on correlations where there is a large incentive to be honest if a group of Souls share a common interest.” - E. Glen Weyl, Puja Ohlhaver, Vitalik Buterin. ‘Decentralized Society: Finding Web3’s Soul by E. Glen Weyl, Puja Ohlhaver, Vitalik Buterin :: SSRN’4\n\n\n\nLegacy systems\nCurrent identity systems tend to concentrate power on the issuer of identity proofs. If we look at the government IDs, for example, the user doesn’t own their identity. Government can track users’ movement (passports), revoke licenses (driving licenses), and put an “expiration date” on your ID card. In Web3 idendity is often handled by identity protocols like: Litentry, ORE network and IDX. When it comes to identity, SBTs and their DeSoc property could, in theory, replace the existing legacy system. However, changes in ID systems usually take a very long time.\n\n\nCold start challenge\nThe research paper asks a question: What comes first SBTs or social recovery?\nWhen it comes to SBTs currently revokable tokens could be created and minted to wallets. They are referred to as “Proto SBTs”, allthough they are not as practical as SBTs proposed they could be a step in the right direction.\nCommunity recovery wallets like Argent and Loopring also show that social recovery wallets can work in practice.\n\n\n\nsoc rec wallets\n\n\nPicture 3: Social recovery wallets explained\n“Norms can also shepherd Souls into existence. As we rethink tokens and wallets, we can also reframe how we think about certain classes of NFTs and tokens that are intended to signal membership. In particular, we can introduce a norm of not transferring NFTs and POAPs issued by reputable institutions that reflect attendance to a conference, work experience, or education credentials. Such transfers of membership tokens—if traded for value—could diminish the reputation of a wallet and perhaps discourage issuers from further issuing membership or POAP tokens to that wallet.” - E. Glen Weyl, Puja Ohlhaver, Vitalik Buterin"
  },
  {
    "objectID": "documents/research/posts/ERFC-261.hugo.html#sentiment",
    "href": "documents/research/posts/ERFC-261.hugo.html#sentiment",
    "title": "SBT - Soulbound Tokens",
    "section": "Sentiment",
    "text": "Sentiment\nWhen the paper came out on May 11th 2022, it didn’t gain mainstream attention initially. Around 20th of May the paper and the ideas in it caught the attention of media outside Web3 space with magazines like Fortune covering the ideas presented.\nIn an interview held by Jason Levin with E. Glen Weyl , the author predicted that that SBTs will be available for early uses by the end of 2022 and that the 2024 up cycle will focus on SBTs.\nOverall response to the paper was very positive all across the board and many potential use cases are discussed."
  },
  {
    "objectID": "documents/research/posts/ERFC-270.hugo.html",
    "href": "documents/research/posts/ERFC-270.hugo.html",
    "title": "[ERFC - 270] ZK NFT Mixer",
    "section": "",
    "text": "NFTs are growing in popularity, and so are the numbers of transactions for transferring the NFTs on the Blockchain. One of the main features of the Blockchain is privacy, but only in terms of pseudo-anonymity. The pseudo-anonymity implies that the user creating the transaction is hidden, as he is represented using an opaque, pseudo-random address. Still, the transactions between the addresses are transparent. By analyzing the transaction graph and external inputs, such as social network posts putting the address in a specific context, it is possible to deduce the address owner and reveal all his transactions. This research aims to analyze the possibility of creating a solution that can enable NFT transfers in an environment that supports complete anonymity but is still compatible with Ethereum. The proposed infrastructure represents a zero-knowledge mixer supporting shielded transfers of the NFTs on the side chain with the ability to withdraw NFTs back on the main chain."
  },
  {
    "objectID": "documents/research/posts/ERFC-270.hugo.html#depositing-nfts-from-ethereum-to-the-side-chain",
    "href": "documents/research/posts/ERFC-270.hugo.html#depositing-nfts-from-ethereum-to-the-side-chain",
    "title": "[ERFC - 270] ZK NFT Mixer",
    "section": "Depositing NFTs from Ethereum to the side chain",
    "text": "Depositing NFTs from Ethereum to the side chain\nUser deposits NFTs on the Ethereum smart contract by sending a transaction allowing the smart contract to take over the NFT. On the side chain, \\(N\\) commitments are minted and added to a Merkle tree. The smart contract can’t say which commitment resembles the NFT, so the commitment values need to be constructed in a specific way by the user and submitted to Ethereum smart contract. The NFT commitment should be a hash of NFT ID, NFT address, owner’s ephemeral public key, and some secret value. Other, \\(N-1\\), commitments should be hashes of a string value NULL, the owner’s ephemeral public key, and some other secret value. The user must provide ZK proof that exactly one commitment is the NFT commitment while all other \\(N-1\\) values are decoys. Each commitment is tied to a nullifier, generated as a hash of the commitment ID, a Merkle path of the commitment inside the Merkle tree, and the same secret value used for the specific commitment. The secrets should be different for each commitment-nullifier pair. By doing this setup, the public still doesn’t know which commitment is the NFT, but anyone can verify that one of the commitments indeed is the NFT, while others are blanks."
  },
  {
    "objectID": "documents/research/posts/ERFC-270.hugo.html#transfering-nfts-on-the-sidechain",
    "href": "documents/research/posts/ERFC-270.hugo.html#transfering-nfts-on-the-sidechain",
    "title": "[ERFC - 270] ZK NFT Mixer",
    "section": "Transfering NFTs on the sidechain",
    "text": "Transfering NFTs on the sidechain\nUsers on the side chain are identified using their new ephemeral keys, meaning the keys should be changed frequently. The keys are the private key, public key, and encryption key. The public key is a private key hash, while the encryption key is “the real” public key, in the sense of public-key cryptography, derived from the private key. Transferring NFTs is similar to tornado cash logic with the addition of decoys. For each NFT transfer, the user also transfers \\(N-1\\) other decoys to \\(N-1\\) different addresses. Received decoys from other users can (and should) be used in further transfers of NFTs, thus enabling further obfuscation of the trading paths. Each transfer on the sidechain is a shielded transfer, meaning that it is not performed as a regular token transfer but as a protocol backed by zero-knowledge proofs. Before explaining the protocol, let us first see how the commitment is constructed. The commitment contains two values, the commitment value, defined during deposit, and a secret value used as a salt while generating the first value. Both values are stacked in a byte array and encrypted using the owner’s public key. When the user wants to transfer the commitment, a new commitment and decoys are sent to the sidechain. ZK proof is also sent on the chain, proving that all the computations were correct. The sender proves that the new commitment is derived from the proper input commitments and nullifiers, and the output commitments are generated in the way that only one output commitment contains the NFT. The new commitment is encrypted using the recipient’s public key, so only the recipient can decrypt it and use it later. The recipient listens to the sidechain events to see if any new commitment can be decrypted using his private key and notes down all the commitments that pass the test."
  },
  {
    "objectID": "documents/research/posts/ERFC-270.hugo.html#withdrawing-nfts-from-the-side-chain-to-ethereum",
    "href": "documents/research/posts/ERFC-270.hugo.html#withdrawing-nfts-from-the-side-chain-to-ethereum",
    "title": "[ERFC - 270] ZK NFT Mixer",
    "section": "Withdrawing NFTs from the side chain to Ethereum",
    "text": "Withdrawing NFTs from the side chain to Ethereum\nThe user who initiates withdrawal must prove ownership of the commitment representing the NFT by providing ZK proof. The user proves the knowledge of the secret preimage used for generating the initial commitment. When the proof is verified and valid, the commitment is nullified. The smart contract on the Ethereum transfers the locked NFT to the Ethereum address provided within the withdrawal request."
  },
  {
    "objectID": "documents/research/posts/ERFC-246.hugo.html",
    "href": "documents/research/posts/ERFC-246.hugo.html",
    "title": "[ERFC - 246] Token Engineering and Design of complex systems",
    "section": "",
    "text": "This is the first research paper in the series of research exploring Token Economics and Token Engineering tools and processes. It provides an introduction to the practice and the Process of Token Engineering. It also covers a portion of systems theory and the three most prevalent tools in the field of Token Engineering. These tools are:\n\nMachinations - tool token engineers can use in designing Crypto Economic systems in all stages, from System Mapping to Evaluation and improvements on the running system.\ncadCad - the package most often used in designing, testing, and validating complex systems through simulation.\nTokenSPICE - an EVM Agent-Based Token Simulator written in Python which simulates tokenized ecosystems via an agent-based approach, with EVM “in the loop”."
  },
  {
    "objectID": "documents/research/posts/ERFC-246.hugo.html#steps-in-token-engineering-process",
    "href": "documents/research/posts/ERFC-246.hugo.html#steps-in-token-engineering-process",
    "title": "[ERFC - 246] Token Engineering and Design of complex systems",
    "section": "Steps in Token Engineering process",
    "text": "Steps in Token Engineering process\nBelow we will present some general steps in the process of Token Engineering.\n\nSystem Mapping\nIdentifying what concepts and constructs are relevant to our model and goals for the system\nBefore starting the process, we need to take systems thinking approach. In this stage of the design, the engineer:\n\nBuilds stakeholder taxonomies by identifying stakeholder groups, their possible actions, and the form of their incentives\nLays out the system dynamics and agent goals.\n\nSome of the tools for system mapping are Cluster Maps and Ecosystem Canvas:\nCluster Maps\nThe system’s goal is set in the middle of a cluster map while the associated nodes are drawn around it. This is a not-so-rigorous approach and is often used to get an outline of the system engineer is creating.\n\nFigure 2: Cluster map example@acarogluToolsSystemsThinkers2017\nEcosystem canvas\nWhen using this method, the purpose of the system is put at the center while key players are laid out in circles radiating outwards.\n\nFigure 3: Ecosystem canvas Author stephenyo3\n\n\nFormalising the design\nAfter the initial phase is the phase of formalizing the design using causal loop diagrams and stock and flow diagrams, there is another tool called Machinations that Token Engineers could use in this step. We will cover Machinations later in the paper.\nCausal Loop Diagrams\n“A causal loop diagram is a”snapshot of all relationships that matter.” It visualizes key variables (i.e., factors, issues, processes) and how they are interconnected. These diagrams show variables represented as texts and causal relationships between them as arrows.” ‘What Is a Causal Loop Diagram and What Is It Good For? | Marketlinks’4\n\nFigure 4: Causal Loop Diagram\nIn a Causal Loop Diagram positive relationships are labeled with a plus sign while negative relationships are labeled with a minus sign. Above example is often used to explain this type of diagrams. It shows how market saturation and word of mouth impact potential adopters and adoption rate.\nStock and flow diagrams\nStock and flow diagrams are a more complex way of formalizing the design. You can see below a representation of the simple system using this diagram.\n\nFigure 5: Stock and Flow Diagram\n\n\nModularising the logic and model building\nAfter formalizing the design, system modeling is done using an open-source python package, cadCad. We will cover cadCad in detail in the next section of the paper.\n\n\nRefining the model\nAs the name suggests, this is the part in the process where the model is refined using quantitative and qualitative backtesting.\n\n\nEvaluation and improvements on the running system\nAfter the system is up and running cadCad model can be used as a “digital twin,” which allows token engineers to:\n\nevaluate proposed changes to the system\ntest the sensitivity of parameters\nexplore the success criteria and failure modes\nevaluate behaviors and policies\nmake recommendations to governance bodies stephenyo5"
  },
  {
    "objectID": "documents/research/posts/ERFC-246.hugo.html#machinations",
    "href": "documents/research/posts/ERFC-246.hugo.html#machinations",
    "title": "[ERFC - 246] Token Engineering and Design of complex systems",
    "section": "Machinations",
    "text": "Machinations\nMachinations is a browser-based tool to design and balance game systems. This tool has seen use from game designers, consultants, developers, and analysts. Token Engineers can also use this tool in designing Crypto Economic systems in stages from System Mapping to Evaluation and improvements on the running system. Machinations shines in pre-production.\nUsing Machinations, Token Engineer can:\n\nMap systems in an interactive diagram\nSet parameters that define resource flow\nPlot and analyze the results in real-time using the chart option\nSimulate outcomes for one player journey or stochastically using batch plays\nExport outcomes in CSV\nExport design parameters to Google Sheets\n\nMachinations is uses these types of nodes:\n\nPools that collect Resources\nSources that create Resources\nDrains that consume/destroy resources\nConverters that transmute resources\nGates that redistribute resources\n\nAnd two types of connections:\n\nResource connections that determine how the Resources flow\nState connections that modify the state of diagram elements Dana6\n\nBelow you will find a basic diagram created in machinations that can be used to see the flow of an AMM system:\n\nFigure 6: Machinations diagram example\nThis design was done in less than 20 minutes and represents a basic flow of the user exchanging tokens for crypto using our AMM. The Key takeaway is that machinations, even though its main focus is on game design can be used for the stages of System Mapping and Formalising the design."
  },
  {
    "objectID": "documents/research/posts/ERFC-246.hugo.html#cad-cad",
    "href": "documents/research/posts/ERFC-246.hugo.html#cad-cad",
    "title": "[ERFC - 246] Token Engineering and Design of complex systems",
    "section": "cad Cad",
    "text": "cad Cad\n                  ___________    ____\n  ________ __ ___/ / ____/   |  / __ \\\n / ___/ __` / __  / /   / /| | / / / /\n/ /__/ /_/ / /_/ / /___/ ___ |/ /_/ /\n\\___/\\__,_/\\__,_/\\____/_/  |_/_____/\nby cadCAD                  ver. 0.4.28\n======================================\n       Complex Adaptive Dynamics       \n       o       i        e\n       m       d        s\n       p       e        i\n       u       d        g\n       t                n\n       e\n       r\ncadCad is the package most often used in designing, testing, and validating complex systems through simulation. It supports Monte Carlo methods, A/B testing, and parameter sweeping. cadCad can model systems from agent-based modeling to system dynamic modeling. It can easily be integrated with other Python modules and data science workflows. But first, let’s briefly explain what these methods are:\n“Monte Carlo simulations are used to model the probability of different outcomes in a process that cannot easily be predicted due to the intervention of random variables. It is a technique used to understand the impact of risk and uncertainty in prediction and forecasting models.” ‘What Is a Monte Carlo Simulation?’7\n“A/B Testing is a method to compare two (or more) variations of something and determine which one works better. In this method, users are randomly assigned to one of two variants. A statistical analysis is performed to determine which variation performs better for a defined business goal.”8\n“In modeling, parameter sweeps are an important method for fine-tuning parameter values, exploring parameter space, and calibrating simulations to data. A parameter sweep is an iterative process in which simulations are run repeatedly using different values of the parameter(s) of choice. This process enables the modeler to determine a parameter’s”best” value (or range of values), or even where in parameter space the model produces desirable (or non-desirable) behaviors.” ‘Parameter Sweeps and Model Iteration Idmtools Documentation’9\nAlthough it can be used to simulate any system that can be described as state variables that evolve over time according to a set of equations, cadCad has seen the most use in the Token Engineering process.\nThe first step in modeling using cadCad is the Visual System mapping we mentioned earlier.\nAfterward, the next step is Mathematical specification using differential equations. For example, we will use the simple system of sheep from the cadCad introduction course:\n\nHere we set the initial differential equations for our model. It is self explanatory. Population growth depends on food sources.\nNote: This is a simplified model used to for demonstration, Crypto Economy equations are much more complex\nAfterwards, the Modelling and simulation process in general works like this:\n\nFigure 7: cadCad processes\nWe will briefly explain the steps without going into code examples as we will cover plenty of code examples in future research papers:\n\nFirst, the engineer defines all the state variables in the system and their initial values (they can be of any Python data type)\nAfterwards, the variables that impact the behavior of the model.\nPolicy functions compute one or more signals to be passed to state update functions. They are used to describe the logic and behavior of a system component.\nState update functions are then designed to define how the model changes over time.\nPartial state update blocks are used for composing state update functions and policy functions in series or parallel.\n\nSimulation process steps:\n\nIn the configuration stage, engineer ties all the model compenents using “config_sim” and chooses how the simulation should run:\n\nThe number of times it will run\nThe number of timesteps the simulation will run for\nThe parameters of the system\n\nExecution computes the simulation output\nOutput preparation is the process in which data is manipulated and analyzed in order to answer questions about the model.\nAnalysis is self explanatory - engineer evaluates the model performance and if there is a need, improves the model. Usually that is the case as the first model is almost always not the optimal one.\n\nAs we can see cadCad covers the entire Token Engineering process with help from stock and flow diagrams and system mapping tools. We will cover this library and its functionalities in great detail in future research papers."
  },
  {
    "objectID": "documents/research/posts/ERFC-246.hugo.html#tokenspice",
    "href": "documents/research/posts/ERFC-246.hugo.html#tokenspice",
    "title": "[ERFC - 246] Token Engineering and Design of complex systems",
    "section": "TokenSPICE",
    "text": "TokenSPICE\nTokenSPICE is an EVM Agent-Based Token Simulator written in Python, which simulates tokenized ecosystems via an agent-based approach, with EVM “in the loop”.\nEthereum Virtual Machine (EVM) is a computation engine which acts like a decentralized computer that has millions of executable projects. It acts as the virtual machine which is the bedrock of Ethereum’s entire operating structure. ‘Ethereum Virtual Machine (EVM) | CoinMarketCap’10\nTokenSPICE has been mainly used in later stage analysis and for verifying and tuning the system designs.\nAgent-based modeling focuses on the individual active components of a system. Agents in tokenSPICE can be DAOs, unique users, and other protocols, making it a versatile tool.\nIt can be used in Token Engineering flows to design, tune and verify tokenized ecosystems.\nIf the engineer wants to model on the smart contract code directly and skip the equations set up like in cadCad, then the tokenSPICE is the tool of choice.\nIt uses Brownie, which treats smart contracts as classes, making it easier to run simulations. It also requires less work upfront in contrast to cadCad. Write the contracts in Solidity, then simulate with tokenSPICE.\nWhen you run the simulator, the run function in SimEngine.py is triggered and starts the run loop in it:\ndef run(self):\n        \"\"\"\n        @description\n          Runs the simulation!  This is the main work routine.\n\n        @return\n           <<none>> but it continually generates an output csv output_dir\n        \"\"\"\n        log.info(\"Begin.\")\n        log.info(str(self.state.ss) + \"\\n\")  # pylint: disable=logging-not-lazy\n\n        while True:  \n            self.takeStep()\n            if self.doStop():\n                break\n            self.state.tick += 1 \n            chain.mine(blocks=1, timedelta=self.state.ss.time_step)\n        log.info(\"Done\")\nEvery single time it loops, every single agent inside the state takes a step:\n\ndef takeStep(self) -> None:\n        \"\"\"Run one tick, updates self.state\"\"\"\n        log.debug(\"=============================================\")\n        log.debug(\"Tick=%d: begin\", (self.state.tick))\n\n        if (self.elapsedSeconds() % self.state.ss.log_interval) == 0:\n            s, dataheader, datarow = self.createLogData()\n            log.info(\"\".join(s))\n            self.logToCsv(dataheader, datarow)\n\n        # main work\n        self.state.takeStep()\n\n        log.debug(\"=============================================\")\n        log.debug(\"Tick=%d: done\", self.state.tick)\nThe rest of the file is dedicated to logging the results into CSV format, and we will not examine it further.\nSimilarly, the simulation engine also takes a step when the Agents take a step in the simulation.\nSimulation with tokenSPICE can be done using the following command in the terminal and the results can saved in CSV or a plot created in png:\n  tsp plot netlists/scheduler/netlist.py outdir_csv outdir_png\nNote: Netlists are just agents “wired-up”\nHere we ran a vesting simulation in the Ocean protocol and this is the resulting plot:\n\nFigure 8: Vesting simulation in tokenSPICE\nYou can run all kinds of netlists in order to get a “feel” of it, in the tokenSPICE official GitHub repo.\nIn contrast to cadCad, which can be used for any type of system, tokenSPICE is focused on EVM systems and incentives. The main difference is that cadCad is mainly used in the early-stage design of systems."
  },
  {
    "objectID": "documents/research/posts/ERFC-147.hugo.html",
    "href": "documents/research/posts/ERFC-147.hugo.html",
    "title": "OWT - Omni Web Token",
    "section": "",
    "text": "JSON Web Tokens, or JWT, are the format of JSON encoded data structures for authorizing clients on the Web. Some JWT payloads have a specific standardized structure, defined by the protocol, such as OAuth(2) tokens. Others, however, are designed specifically for particular apps.\nAuthorization on the Blockchain is done mainly through explicitly whitelisting addresses that are allowed to perform specific actions. Whitelisting introduces high costs when the number of whitelisted addresses is large. A good example is ICO whitelisting, where hundreds or even thousands of participants need to get whitelisted.\nThis research aims to find an efficient, more cost-effective solution for authorizing users on the Blockchain using a system of authorization tokens issued and received off-chain, without the Blockchain transaction fees, and which will be valid on-chain and off-chain. In addition, the token structure should be transferrable off-chain as a JWT token, compatible with the OAuth2 standard, and reusable in both Web 2.0 and Web 3.0 worlds."
  },
  {
    "objectID": "documents/research/posts/ERFC-147.hugo.html#oauth-standard",
    "href": "documents/research/posts/ERFC-147.hugo.html#oauth-standard",
    "title": "OWT - Omni Web Token",
    "section": "OAuth standard",
    "text": "OAuth standard\nOpen Authorization, or OAuth for short, is an authorization standard followed by many APIs worldwide. The standard specifies the protocol between client and authorization server and data transferred in protocol messages. There are two versions of OAuth standard, 1 and 2.\nOAuth standard doesn’t specify the transfer method for sending messages, but one widely adopted standard is using JWT tokens as the authorization data container."
  },
  {
    "objectID": "documents/research/posts/ERFC-147.hugo.html#jwt-tokens",
    "href": "documents/research/posts/ERFC-147.hugo.html#jwt-tokens",
    "title": "OWT - Omni Web Token",
    "section": "JWT Tokens",
    "text": "JWT Tokens\nIn 2015, Michael Jones, John Bradley and Nat Sakimura introduced JSON Web Tokens (JWT) through RFC-7519 as a compact structure for representing claims transferred between two parties [3]. Since then, JWTs have been used for client authentication on the Web. Initially, the authentication was performed between APIs but quickly found use in client authentication following the growth of client-heavy applications.\nJWT has a general structure, made of a header, body, and signature segments. The header segment includes the token type and a label of the algorithm used for creating signatures.\nExample JWT header\n{\n    \"type\": \"JWT\",\n    \"alg\": \"ES256\"\n}\nThe body segment is a container segment used for storing protocol-specific information. OAuth 2.0 standard, introduced with RFC-6749 [4], specifies several fields required for authorization that are members of the body of a JWT token. Some of those fields are:\n\naud (Audience) - Identifier of the user to whom the client will present the token for executing an action\nexp (Expiration) - Token expiration timestamp\niss (Issuer) - Identifier of the token issuer, authorization service provider\nscope (Action scope) - List of actions for which the token owner would be authorized to perform\nsub (Subject) - Identifier of the token owner\niat (Issued at) - Isusing timestamp\n\nExample JWT Body\n{\n  \"sub\": \"user1\",\n  \"name\": \"John Doe\",\n  \"iat\": 1516239022,\n  \"aud\": \"server1\",\n  \"scope\": [\"data.fetching\"]\n}\nField name in the JWT body example represents a custom, application-specific data field.\nThe signature part of the JWT token contains the hash or signature of the token provided by the token issuer.\nThe three token parts are put together into one base64 encoded string (without trailint = symbols), having the tree parts separated by the dot . symbol.\nExample of a complete JWT string\neyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiaWF0IjoxNTE2MjM5MDIyfQ.SflKxwRJSMeKKF2QT4fwpMeJf36POk6yJV_adQssw5c"
  },
  {
    "objectID": "documents/research/posts/ERFC-147.hugo.html#general-authorization-protocol",
    "href": "documents/research/posts/ERFC-147.hugo.html#general-authorization-protocol",
    "title": "OWT - Omni Web Token",
    "section": "General Authorization Protocol",
    "text": "General Authorization Protocol\nA general protocol for client authorization on the Web using JWT tokens consists of two steps: - Requesting token from the authorization server for executing some action - Presenting the authorization token received from the authorization server to the server which would perform the requested action"
  },
  {
    "objectID": "documents/research/posts/ERFC-147.hugo.html#existing-approaches",
    "href": "documents/research/posts/ERFC-147.hugo.html#existing-approaches",
    "title": "OWT - Omni Web Token",
    "section": "Existing Approaches",
    "text": "Existing Approaches\nThere are some existing approaches for combining the Blockchain and OAuth tokens. In one such research [5], the authors used NFTs as authorization tokens generated on-chain that would be verifiable using the OAuth 2.0 protocol. That approach is inverse to this research: it doesn’t reduce costs but puts authorization tokens on the chain."
  },
  {
    "objectID": "documents/research/posts/ERFC-147.hugo.html#chain-token-ct-representation",
    "href": "documents/research/posts/ERFC-147.hugo.html#chain-token-ct-representation",
    "title": "OWT - Omni Web Token",
    "section": "Chain token (CT) representation",
    "text": "Chain token (CT) representation\nThe essential requirements for the token that would be used on the chain are: - Efficient verifiability; token should be efficiently verifiable on the chain - Authenticity; the probability of token forgery should be negligible - Succinctness; token should be small in byte size - Non-transferability; token should be used only by the user who received the token\nAs an example throughout this research, we will use the money checkout allowance problem, where a client needs to be authorized to payout a certain amount of money from the account of the authorizing entity. An existing approach for solving this problem is calling the approval method on the smart contract or whitelisting clients and explicitly approving the total allowed amount for each client. Method signature without a token argument would look like this:\npayout(address account, address receiver, uint256 amount)\nA naive approach for constructing a token that would follow JWT logic would be providing all the values bound by the token. In our example, the values that are required for money checkout from the account are: - account owner identifier; 20 bytes address value - authorized client identifier; 20 bytes address value - allowed amount; 32 bytes value\nNext, the apparent problem is a forgery. Everyone can create a token with listed values and submit it on the Blockchain. The solution for this problem is to provide a signature made by the authorization entity, which confirms the provided values. Ethereum signatures contain three segments, v, r, and s, 65 bytes long. All summed up to 137 bytes of memory. Even if this doesn’t look like a significant sum, the main issue is that the token size asymptotically grows by \\(O(n)\\), linearly with the number of arguments n. In other words, it would become costly, or even unusable, for methods with more arguments. It is not very efficient, but it is a start.\n\nToken size optimization\nThe problem with the naive approach is linear growth with the number of arguments. Solving this problem requires looking closely at the method that is being called. The method payout already contains the token values as a method argument, and it would be redundant to provide them again in the token. The same pattern is visible with different use-cases. This observation suggests that we can avoid providing the values within the token but use only a hash of the approved values and check if the hash of the provided input values matches the token hash. Using the hashing method clears the linear growth of the token as the size of the keccak256 hash is fixed to 32 bytes in length, achieving a constant \\(O(1)\\) size of the token. The hash can also cover other values that could be hard-coded into the smart contract without a token size increase, which will become an essential feature in later sections.\n\n\nSignature size optimization\nThe Ethereum signature size is fixed to 65 bytes. That means that it passes the size of 2 memory words (32 bytes in size) and requires three blocks of memory. The solution for this problem comes with EIP-2098 [6], which proposes a simple technique for compact signature representation, reducing its size by 1 byte and allowing it to fit into two memory blocks.\nThe total token size is now fixed to 32 bytes of token hash plus another 64 bytes of signature data totaling 96 bytes or three blocks of memory.\n\n\nMapping OAuth2.0 parameters\nNow that we have a token structure, we need to figure out how to standardize token parameters to comply with the OAuth2.0 standard.\nThe audience parameter refers to the smart contract containing the payout method. It doesn’t need to be provided explicitly as a method argument as it is already encoded in the smart contract.\nThe token issuer can be deduced from the signature and doesn’t need to be explicitly provided.\nThe subject parameter is provided both as the input argument of the payout method. It doesn’t have to be provided explicitly. It should not be provided even as the argument, as it is already given as the message sender value.\nThe scope parameter describes the action that should be executed. It should not be explicitly provided as it should be hardcoded in the method.\nThe token expiration time is a tricky one. It doesn’t naturally belong to method arguments, so it should be provided explicitly. To prevent the token size increase, we can do a simple modification of the token hash. The value of the timestamp can be stored in 8 bytes. A straightforward solution is to provide another method argument with an 8-byte value. A more elegant solution is to transform the keccak256 value into “pseudo-keccak224” (SHA224 [7] value, with unchanged initialization value) by truncating the hash size to its 224 bytes prefix and appending 8 bytes extension with expiration timestamp as the last 8 bytes of the token hash. This transformation returns us to the previously proposed token size without extra arguments.\nAs we can see, all the crucial parameters of the OAuth2.0 protocol can be successfully mapped to the Blockchain transactions and the chain token."
  },
  {
    "objectID": "documents/research/posts/ERFC-147.hugo.html#token-reusability",
    "href": "documents/research/posts/ERFC-147.hugo.html#token-reusability",
    "title": "OWT - Omni Web Token",
    "section": "Token reusability",
    "text": "Token reusability\nSome authorization tokens are reusable many times until the expiration date. However, some use-cases require that the tokens may be used only once. An example of such a use case is exactly the example we have used so far. Once the payout method has been executed, the user should not be able to perform double-payout transactions. This problem can be solved the same way as in Web 2.0 - by introducing a payment identifier. The payment identifier should be included as an extra method argument and included in the token hash. This solution also solves the issue of the lost token, as the token can be reissued with the same payment identifier and used for the same payment only once. Specific use cases may require the existence of only one token. In that case, the token (and method arguments) may include jti parameter or JWT token ID as a token identifier or use the token hash as an identifier. The smart contract should implement mechanisms for preventing double payments and a potential blacklisting of tokens."
  },
  {
    "objectID": "documents/research/posts/ERFC-147.hugo.html#action-scope-schemas",
    "href": "documents/research/posts/ERFC-147.hugo.html#action-scope-schemas",
    "title": "OWT - Omni Web Token",
    "section": "Action scope schemas",
    "text": "Action scope schemas\nOAuth2.0 proposes a parameter that includes the action scope or label of the action for which the client can use the token. Action scopes are not specified as they can be represented using any string value. The problem here is the cost of using string data types in smart contracts. This research proposes restriction for action type values to numerical data types of 4 bytes. This restriction allows \\(2^32\\) possible scope values that could represent many use cases.\nA library of token schemas can help developers properly format their tokens based on scope number schemas. Furthermore, enumeration of action types can introduce standardization for tokens where a smart contract may require, for example, “scope 42” tokens for executing a method. We present the first two token scopes that would be used in the following use-cases:\n\nScope 1: Generic Identity\nGeneric identity scope should be used for verification of the client’s identity. The verification is based on the client’s wallet address and unique identifier in the issuer’s database. The hash for “scope 1” tokens be made by hashing the following array of values in their respective order: - address iss; token issuer address aud; smart contract address or 0 address for general identification - address sub; client’s wallet address - uint4 scope; action scope with value 1 for generic identification - bytes32 uuid; unique identifier of the client in the issuer’s - uint8 exp; token expiration timestamp in UNIX timestamp format database ### Scope 2: Allowance Allowance tokens, used for crypto cheques, should have hashes made by hashing the following array of values in their respective order: - address iss; token issuer - address aud; smart contract address - address sub; money receiver - uint4 scope; action scope with value 2 for allowance - bytes32 paymentId; payment identifier (NOTE: token hash may be used as payment ID but it can introduce new problems) - uint256 amount; amount to transfer - uint8 exp; token expiration timestamp in UNIX timestamp format\nThe reader may notice that in both cases, there are two logical groups of parameters: - general parameters; iss, aud, sub,scopeandexp- application-specific parameters;userId,paymentId,amount`\nThe general parameters are the same for all tokens, and application-specific parameters are different for each scope."
  },
  {
    "objectID": "documents/research/posts/ERFC-147.hugo.html#from-chain-token-ct-to-omni-web-token-owt",
    "href": "documents/research/posts/ERFC-147.hugo.html#from-chain-token-ct-to-omni-web-token-owt",
    "title": "OWT - Omni Web Token",
    "section": "From Chain Token (CT) to Omni Web Token (OWT)",
    "text": "From Chain Token (CT) to Omni Web Token (OWT)\nNow that we have the complete definition of the chain token, we can go one more step and make it usable and transferrable in the Web 2.0 world. We can do this by packing chain token data as part of theJWT token, following the OAuth 2.0 schema.\n\nOWT Body\nThe token scopes define OAuth 2.0 parameters, so the only remaining thing is appropriately packing the chain token into the JWT body and creating a proper JWT signature. The verifier needs to know the CT hash value and the parameters used in the construction of the token.\nThe proposed OWT body schema is:\n{\n    aud: <smart contract address>,\n    iss: <issuer's wallet address>,\n    scope: <readable name of the scope>,\n    exp: <token expiration timestamp>\n    chain_token: {\n        token_hash: <CT hash with expiration timestamp>,\n    r: <signature r value>,\n    sv: <compact representation of s and v signature values>\n        params: [<\n                ordered list of token parameters\n                in form of:\n                 {\n                    param: <parameter name>,\n                    value: <parameter value>,\n                    type: <parameter data type>,\n                 }\n            >]\n    }\n}\n\n\nOWT Signature\nThe JWT standard allows using the Ethereum secp256k1 signatures by providing the EC256 algorithm type value in the token header. The signature of the OWT token is created using the issuer’s private key. To verify the signature, the verifier needs access to the issuer’s public key, which should be available from the issuer’s /.well-known/jwks.json route of the authorization API."
  },
  {
    "objectID": "documents/research/posts/ERFC-147.hugo.html#owt-issuing",
    "href": "documents/research/posts/ERFC-147.hugo.html#owt-issuing",
    "title": "OWT - Omni Web Token",
    "section": "OWT Issuing",
    "text": "OWT Issuing\nThe OAuth 2.0 protocol allows using client id and client secret parameters when requesting the authorization tokens. OWT requests can be made using wallet address as client’s identity and signature of the clients wallet address as the client secret parameter"
  },
  {
    "objectID": "documents/research/posts/ERFC-147.hugo.html#allowance-use-case",
    "href": "documents/research/posts/ERFC-147.hugo.html#allowance-use-case",
    "title": "OWT - Omni Web Token",
    "section": "Allowance Use Case",
    "text": "Allowance Use Case\nThe allowance use case tested the token issuing protocol for payout allowances, evaluated the costs of performing payouts using the token, and compared the results with the basic whitelisting protocol.\n\nTest setup\nAuthorization entity submitted allowance amount for the client using the whitelist smart contract method. The cost of the whitelisting transaction was 44,484 gas.\n\n\nTest 1: Payout by Whitelisting\nThe client executed the payoutOld method of the smart contract with a propper allowance amount. The cost of the payout transaction was 22,389 + T gas.\n\n\nTest 2: Payout with Token\nThe client requested an OWT from the authorization server using OAuth 2.0 request schema. The authorization server verified the client’s credentials and issued a “scope 2” token to the client with a specified allowance amount. The client verified the OWT data and extracted the chain token and its parameters from the OWT. After the OWT verification step, the client submitted them to the smart contract using the payoutNew method. The payout was successful, and the execution cost was 59,935 + T gas.\n\n\nTest 3: Payout with Token using Token Hash as Payment Identifier\nThe protocol for issuing and verifying the OWT was done the same way as in the previous test. The only difference was that the token did not follow the “scope 2” schema but left out the payment identifier. The client performed a payout using the payoutNewShort method. Again, the payout was successful, and the execution cost was 59,189 + T gas.\n\n\nCost Analysis\nThe test results show that the cost of the payout protocol using whitelisting was 44,484 gas for the issuer and 22,389 gas for the client, resulting in 66,873 gas for the entire protocol. Test 2 showed that using the token has reduced the issuer’s costs to 0, but the client’s cost was increased to 59,935 gas which is 37,546 gas more than in test 1. Interestingly, the overhead cost for the client is lower than the costs of the issuer’s whitelisting, which indicates that using the token reduces the issuer’s fees, compared to whitelisting, even if the issuer compensates the overhead client’s costs by increasing the allowance amount. Test 3 showed that using token hash as payment identifier reduces the costs for extra 746 gas. However, this introduces problems with token reissuing. The exact token needs to be reissued every time the original one gets lost, requiring the expiration timestamp to be the same as the original one for the hashes to match and thus have the same payment identifier. This situation can cause the issuer to be unable to reissue the token as its timestamp has already expired. The results of the cost analysis are presented in Table 1. \n\n\n\n\nMethod\n\n\nIssuer’s costs (gas)\n\n\nClient’s costs (gas)\n\n\nTotal protocol costs (gas)\n\n\n\n\nWhitelisting\n\n\n44,484\n\n\n22,389\n\n\n66,873\n\n\n\n\nOWT allowance\n\n\n0\n\n\n59,935\n\n\n59,935\n\n\n\n\nShort OWT allowance\n\n\n0\n\n\n59,189\n\n\n59,189\n\n\n\n\n\nTable 1. Cost anlysis of the allowance use case methods"
  },
  {
    "objectID": "documents/research/posts/ERFC-147.hugo.html#generic-identity-use-case",
    "href": "documents/research/posts/ERFC-147.hugo.html#generic-identity-use-case",
    "title": "OWT - Omni Web Token",
    "section": "Generic Identity Use Case",
    "text": "Generic Identity Use Case\nThe generic identity use case tested the issuing and verification of the “scope 1” tokens and assessed the costs of using identification tokens on the chain.\nThe client acquired OWT from the authorization server and submitted the token with the received user id from the issuer’s database. The client submitted the token to the verifyIdentity method of the smart contract and successfully verified the identity token. The cost of the verification was 36,816 gas, which suggests that the verification proces cost was 15,817 gas, leaving out the base transaction cost.\n\nUse case analysis\nThe test showed that the costs for verifying the identity tokens are low and open a new path for the identity representations valid on multiple chains. Additional use cases may specify scope schemas for more specific identity tokens and introduce low-cost decentralized identities to a multi-chain environment."
  },
  {
    "objectID": "documents/research/posts/ERFC-147.hugo.html#general-purpose-token-verification-service-gptvs",
    "href": "documents/research/posts/ERFC-147.hugo.html#general-purpose-token-verification-service-gptvs",
    "title": "OWT - Omni Web Token",
    "section": "General Purpose Token Verification Service (GPTVS)",
    "text": "General Purpose Token Verification Service (GPTVS)\nThe entire verification protocol can be summed up into a token verification smart contract that can be deployed and used by multiple users to verify the authorization tokens for their smart contracts. Blacklisting can be introduced for tokens or clients. This service should allow listing approved issuers and smart contract addresses from which the requests may come. Also, the service can be monetized by requiring a certain amount of Ethers or ERC-20 tokens to be submitted monthly by the users to the verification smart contract, or the service will become unavailable for the requests coming from the user’s smart contracts."
  },
  {
    "objectID": "documents/research/posts/ERFC-147.hugo.html#appendix-1-chain-token-verification-smart-contract",
    "href": "documents/research/posts/ERFC-147.hugo.html#appendix-1-chain-token-verification-smart-contract",
    "title": "OWT - Omni Web Token",
    "section": "Appendix 1: Chain token verification smart contract",
    "text": "Appendix 1: Chain token verification smart contract\n// SPDX-License-Identifier: GPL-3.0\npragma solidity >=0.7.0 <0.9.0;\n\ncontract VerifierContract {\n        mapping(address => uint256) whitelisted;\n        mapping(bytes32 => bool ) usedPaymentIds;\n        bytes prefix = \"\\x19Ethereum Signed Message:\\n32\";\n        uint32 genericIdentityScope = 1;\n        uint32 payoutScope = 2;\n\n        function checkSignature(bytes32[3] calldata token, address signer) public returns (bool) {\n                // Decode r, s, v values\n                bytes32 hash = token[0];\n                bytes32 sv = token[2];\n                bytes32 r = token[1];\n                bytes32 s = sv & bytes32((uint((1 << 255) - 1)));\n                uint8 v = uint8(uint(sv >> 255) + 27);\n\n                // Create signature hash\n                bytes32 prefixedProof = keccak256(abi.encodePacked(prefix, hash));\n\n                // Verify signer\n                address recovered = ecrecover(prefixedProof, v, r, s);\n                return recovered == signer;\n        }\n        \n        function whitelist(address client, uint256 amount) public {\n                whitelisted[client] = amount;\n        }\n\n        function payoutOld(uint256 amount) public {\n                require(amount <= (whitelisted[msg.sender]));\n                whitelisted[msg.sender] -= amount;\n        }\n\n        function payoutNew(address sender, uint256 amount, bytes32 paymentId, bytes32[3] calldata token) public{\n                // Check if token has already been used\n                require(usedPaymentIds[paymentId] == false);\n\n                // Check token expiration\n                uint32 exp = uint32(uint256(token[0]));\n                require(exp > block.timestamp, \"Token expired\");\n\n                // Check token signature\n                require(this.checkSignature(token, sender) == true, \"Invalid signature\");\n\n                // Check token values\n                bytes32 prefixedProof = keccak256(abi.encodePacked(sender, address(this), msg.sender, payoutScope, exp, paymentId, amount));\n                require (bytes32((uint256(prefixedProof >> 32 << 32) | uint256(exp))) == token[0]);\n\n                usedPaymentIds[paymentId] = true;\n        }\n\n        function payoutNewShort(address sender, uint256 amount, bytes32[3] calldata token) public{\n                // Check if token has already been used\n                require(usedPaymentIds[token[0]] == false);\n\n                // Check token expiration\n                uint32 exp = uint32(uint256(token[0]));\n                require(exp > block.timestamp);\n\n                // Check token signature\n                require(this.checkSignature(token, sender) == true);\n\n                // Check token values\n                bytes32 prefixedProof = keccak256(abi.encodePacked(sender, address(this), msg.sender, payoutScope, exp, amount));\n                require (bytes32((uint256(prefixedProof >> 32 << 32) | uint256(exp))) == token[0]);\n\n                usedPaymentIds[token[0]] = true;\n        }\n\n        function verifyIdentity(address issuer, bytes32 userId, bytes32[3] calldata token) public {\n                // Check token expiration\n                uint32 exp = uint32(uint256(token[0]));\n                require(exp > block.timestamp);\n\n                // Check token signature\n                require(this.checkSignature(token, issuer) == true);\n\n                // Check token values\n                bytes32 prefixedProof = keccak256(abi.encodePacked(issuer, address(this), msg.sender, genericIdentityScope, exp, userId));\n                require (bytes32((uint256(prefixedProof >> 32 << 32) | uint256(exp))) == token[0]);\n        }\n}"
  },
  {
    "objectID": "documents/research/posts/ERFC-91.hugo.html",
    "href": "documents/research/posts/ERFC-91.hugo.html",
    "title": "Explorative Research on Crypto Insurance - Current state, problems and possibilities of creating new products",
    "section": "",
    "text": "In this paper we covered 5 of the main players in DeFi insurance market in order to determine the products offered, the problems with these products, the way the claims are handled and the possibilty of creating new insurance protocols. Initially we were not familiar with this field and the effort needed for creating these products, so we conducted this explorative research.\nAfter the research we came to these conclusions:\n\nIn creating these kind of products there needs to be significant effort both in developing and inital investment. Protocols covered utilize Advisory Boards of insurance experts in order to create their products.\nState regulation is a big factor in insurance in order to protect the policyholders from malicious insurance offerers. This can be a problem, depending on the states’ attitude towards cryptocurrency.\nHandling claims is often left to the community incentivizing just behavior by staking.\nCollecting adequate capital initially is also one of the major problems.\nProtocols with less staked pools have higher premiums in most cases which shows us that the risk assessment is usually hard to do with new protocols.\nThere is a limited cover capacity.\nUsually there is no cross-chain coverage which limits the protection capability of DeFi protocols on other chains.\nLack of protection diversity: most products offered are limited when compared to the broad coverage of risk types in the traditional insurance market.\nInsuring real world Events is almost non-existent. Etherisc offers 2 products using Oracles but our assumption is that there is still no market need for this kind of products thus there is not much movement in this direction. However utilizing Oracles in traditional products is an interesting thing and we think should be looked more into.\n\nHowever, if You would like to go deeper to understand how these protocols work we recommend reading the entire paper."
  },
  {
    "objectID": "documents/research/posts/ERFC-91.hugo.html#what-is-insurance",
    "href": "documents/research/posts/ERFC-91.hugo.html#what-is-insurance",
    "title": "Explorative Research on Crypto Insurance - Current state, problems and possibilities of creating new products",
    "section": "What is Insurance?",
    "text": "What is Insurance?\nInsurance has a long history, there are claims that it was created around 2000 BC in Babylon, merchant receiving a loan paid the lender extra money in exchange for exemption of loan payment if the merchant’s shipment were stolen. Hovewer, the importance of insurance field cannot be presented without a mention of London’s Lloyd’s. In the 17th century, a London coffeehouse was a meeting place for people seeking marine cargo protection and people willing to take those risks in exchange for premium. The coffeeshop now is the world famous Lloyds. A sheet of cargo and ship information would be filled and the individuals who accepted that risk would sign with their names under it’s description.1 That brings us to a first term in insurance underwriting.\n\nUnderwriting is risk accessment process to determine whether to accept or reject the risk we will come into contact with this term a lot in the later paragraphs.\nAs we previously mentioned the point of insurance is to transfer and share risks.\nThe individuals or companies that would like to transfer risk to other parties by paying a certain fee (premium) are called insured. The reason why the insured avoid the risk is because the loss is too volatile to bear.\nThe party that accepts such risks and and associated fee (premium) is the insurer. Insurers are not averse to exposing themselves to the same risks as insured because of something called pooling and the law of large numbers. The essence of pooling risk is to spread losses of the few over the entire group. The law of large numbers states that the greater the number of exposures the more closely will the actual results approach to the expected average value"
  },
  {
    "objectID": "documents/research/posts/ERFC-91.hugo.html#benefits-of-insurance-and-the-nature-of-insurance",
    "href": "documents/research/posts/ERFC-91.hugo.html#benefits-of-insurance-and-the-nature-of-insurance",
    "title": "Explorative Research on Crypto Insurance - Current state, problems and possibilities of creating new products",
    "section": "Benefits of Insurance and the nature of Insurance",
    "text": "Benefits of Insurance and the nature of Insurance\nInsurance allows the insureds to “trade” the risk of loss for the certainty of smaller payments. As a result this ensured the stable cash flow since there are no extreme losses, and if they happen, they are covered by the insurance. As a result of this “stability” provided by insurance there is less need for governments assistance which saves public resources.2\n\n\n\n\n\n\n\n\n\nApplication\nUnderwriting\nPolicy Issuance\nClaim if a loss occurs\n\n\n\n\n\nFigure 1: The process of issuing insurance\n\nThe application for insurance often starts with quoting process where the amount to be paid in premiums are estimated according to the risk the client would like to manage. After the application the underwriting process occurs.\nUnderwriter evaluates the information of the application and then accepts and then “fine-tunes” the policy using the rating tables from the actuaries. Actuaries calculate premiums, in DeFi, this is done in a different way, more words on that in the later paragraphs.\nAfter the underwriter accepts the application the policy is issued.\nIf a loss occures the claims department examines the claim and asks the insured for the proof of loss before they pay the insured amount. The payment depends on the amount of damage suffered and the decision of the claim department.\n\nThe big part of insurance also is its regulation. Insurance is one of the most actively regulated fields, especially after the financial crisis in 2008. The regulation aims to ensure solvency of the insurers. One of the ways the state regulates Insurance companies is limiting their investment tacticts and portfolio allocation , in other words they don’t let them invest in risky assets which is de-facto the norm in cryptocurrency investments. This is one of the first issues encountered if we were to cooporate with existing insurance companies or create our own protocol that is regulated.3"
  },
  {
    "objectID": "documents/research/posts/ERFC-91.hugo.html#the-current-state-of-defi-insurance-and-insurance-with-cryptocurrency-and-insurance-processes",
    "href": "documents/research/posts/ERFC-91.hugo.html#the-current-state-of-defi-insurance-and-insurance-with-cryptocurrency-and-insurance-processes",
    "title": "Explorative Research on Crypto Insurance - Current state, problems and possibilities of creating new products",
    "section": "The current state of DeFi Insurance and Insurance with cryptocurrency and insurance processes",
    "text": "The current state of DeFi Insurance and Insurance with cryptocurrency and insurance processes\nWhen it comes to crypto-insurance with traditional insurers the market is non-existent, because of the regulation, lack of awareness and the lack of crypto adoption among general public. That’s why we will be covering DeFi insurance field.\nDeFi Insurance refers to buying coverage against losses cause by events in Decentralized Finance. With various hacks and exploits over the years the need for insuring users from the results of these events emerged. Contrary to the layman’s belief DeFi Insurance field is big and growing with different protocols emerging in it. However only 2% of all DeFi value is insured at the moment.4\nMain protections offered is capital protection against protocol hack/exploit risk, smart contract failures or stablecoin crashes. The premium user pays for a cover depends on the type of the cover, insurance provider and the duration of the cover.\nThe Decentralized part in this type of Insurance is that anybody can act as a coverage provider, which supports the initial writers assumption. They become providers by locking up capital in a capital pool of the insurance protocol thus providing needed liquidity. As coverage providers they choose for which protocols or events they want to provide coverage, for example: If they are certain that a protocol is safe from exploits they will prefer providing liquidity to the pool that covers that event.\nAnother big part of DeFi Insurance is verifying claims. This is often done by the Insurance protocol’s community. Considering the nature of insurance and pooling of risk and collecting coverage from providers they are often assembled as DAOs (Decentralized Autonomous Organizations). This means that governance token holders participate in verifying claims. There are several ways of doing that and we will be covering it in the next paragraph.\n\nMain players in this market\nThe 5 main competitors in DeFi insurance are:\n\nNexus Mutual\nBridge Mutual\nInsurAce\nNsure\nEtherisc\n\nNote: There are more insurance protocols but in order to keep this research a short overview we will showcase five\nWe will be covering them in detail to explore how they work and the type of products offered.\n\n\nNexus mutual\nNexus mutual is an Ethereum-based platform that offers insurance products led by community management and financials. Nexus mutual is set up as a DAO. Nexus offers three kinds of products:\n\nProtection against failures in any protocol used by users yield bearing token (Ethereum only)\nProtection against failures in the individual protocol user has funds in, on any chain, but not in other protocols it uses.\nProtection against hacks and halted withdrawals on exchanges or custodial wallets5\n\nSimply put, Nexus cover protects against loss of funds, not loss of value, except in the Yield Token Cover. In the Yield Token Cover Nexus may pay a claim if:\n\nDuring the cover period the face value of the covered token and the market value of the covered token differ in price by more than 10% for a continuous period of four hours or more; and\nThe Covered Member contributes to the mutual, one unit of face value of the covered token in exchange for 0.90 units of cover amount they wish to claim; and\nThe Covered Member redeems their claim payment during the cover period or within 14 days of the cover period ending.\n\nNexus does not provide Cover where the covered tokens and the cover amount are not denominated in the same refference currency. Nexus also doesn’t provide cover for any material goods loss.\nClaim assessing process:\n\nAll Covered members for a particular covered token will be assessed together for each claim event; and\nThe face value of the covered token immediately prior to the claim event shall be set as part of the claims assessment process; and\nFollowing a successful claim vote all Covered Members will be able to contribute their covered tokens and redeem their claim payment on a proportional basis up to the cover amount.6\n\nWe will not cover the other 2 products into great detail, as they are pretty straight forward. More info on them can be found here: https://nexusmutual.gitbook.io/docs/welcome/faq/cover-products\nAll protocols and custodial accounts can be covered by the platform provided that risk assesesors staked enough value against them. Risk Assessors (experienced auditors, capital providers) can stake value in the form of NXM token, thereby vouching for the security of the protocol/custodian and dropping the price of the cover. NXM can be unstaked at any time subject to a 30-day withdrawal period. When cover is subsequently sold on a protocol or custodian, Risk Assessors earn proportional rewards in NXM equivalent to 50% of the cover premium. If a claim is accepted and a payout occurs, Risk Assessors staked against the protocol/custodian will have their staked NXM burnt on a proportional basis to facilitate the payout of the cover amount. This may result in a Risk Assessor having some or all of their NXM staked against the protocol/custodian burnt to provide capital for the payout of the claim.\nCover becomes available through one of two ways:\n\nWhen Risk Assessors stake NXM against a protocol, custodian or cover product more cover is made available. The mutual places limits on the amount of cover to protect the mutual from being too exposed to any single risk. There are two limits a Specific Risk Limit and a Global Capacity Limit.\n\nSpecific Risk limit means capacity on any particular risk is limited by the amount of staking on that risk. If there is no staking the mutual cannot offer any cover. Specific Risk limit is equal to : capacity factor x net_staked_NXM .\nGlobal Capacity Limit is based on the financial resources of the Mutual and is there to ensure the mutual is not overly exposed to any particular risk, regardless of how much is staked. Global Capacity Limit = Minimum Capital Requirement In ETH (MCReth) x 20%\n\nAs cover policies expire cover becomes available. User can check Nexus Tracker for info on cover expiry.7\n\nMembership issue regarding privacy\nMembership in Nexus requires a one-off membership fee of 0.0020 ETH (~$5.50). However, to become a member users need to verify their identity following their Know Your Customer process. They also cannot accept members from 17 countries, Serbia included, thus limiting the usage of the mutual.\nTransparency\nAll deployed contracts of Nexus Mutual can be found here: https://api.nexusmutual.io/version-data/\nAll info regarding cover, staking and claims approvals/denies can be found here: https://nexustracker.io/\nHow are Cover purchases taken care of by Nexus?\nUsers specify which smart contract address they want cover for. They specify the cover amount , currency (ETH or DAI) and Cover Period. Quote will be generated and they need to make the transaction with Metamask. Users can currently pay with ETH, DAI or NXM (nexus mutual tokens). Cover Holders can submit a claim for material loss that occured within the cover period. They can also submit a claim up to 35 days after expiry. A loss that ocurs after cover policy is ended won’t be covered\nHow are claims taken care of?\nClaims are filed by submission. Members must provide cryptographic evidence of the loss (proof of loss) and their claim is later assessed by Claim Assessors by voting. Assessors are financially incentivised to take a longer-term view as they are required to lockup a stake. This stake is then burned if there is evidence of fradulent voting, which is addressed by Advisory Board. Advisory board consists of five members of founding team of the Nexus Mutual and insurance industry experts. They are said to have :\n\nTechnical Expertise on Smart Contract Security and blockchain\nTechnical Expertise on Insurance and Mutuals\nGeneral Expertise\n\nAdvisory board is there to provide techniqual guidance to the members of the mutual as well as to exercize the emergency functions if they are required.\nThis proposes a question: How do they keep the Advisory Board “in check” with Nexus’s decentralization principles?\nNexus does that by enabling members to kick-vote the Advisory Board members that they think are working maliciously. Board member can be replaced by another member if the membership base agrees. These proposals cannot be interfered with by the existing Advisory Board.\n\n\nBridge Mutual\nBridge Mutual has a similar business model as Nexus Mutual (DAO model). They provide coverage for smart contracts, stablecoins and other services. Bridge allows users to purchase coverage, provide coverage in exchange for yield, vote on policy claims and their payouts. We will not go into great detail as there are various similarities with Nexus Mutual to avoid repeating ourselves.\nThe main difference between Bridge and Nexus Mutual is that that Bridge doesn’t check customer’s ID and is available for residents all countries\nAny user on the platform can create any pool for any project as because the system is permissionless by design.\n\nInitial capital (USDT) must be put into the Project Coverage Pool by the user that is creating the pool.\nThat project can create incentives for coverage providers to provide coverage to their pool by depositing any number of Token into its designated Shield Mining pool which gets distributed to Coverage Providers alongside the typical yield.\n\nCoverage Provider examines the risk of providing Coverage Capital to the Project’s Coverage Pool. When they provide capital they are de-facto telling users that they are sure in the security and stability of the project. They recieve yield from the users purchasing policies and the BMI(Bridge Mutual) token staking. When coverage providers supply capital to the coverage pool they receive a token (bmi(project name)Cover) representing their share in the capital within this capital pool. Coverage providers can then stake those tokens in the bmiCover Staking Contract pool to get additional BMI rewards. They also recieve a BMINFT Bond that represent the amount of for example DAI staked in Cover Staking Contract pool. These NFT Bonds are tradable on any NFT marketplace. The purpose of these NFT bonds is to give the user a way out of their position without removing DAI from the ecosystem. When the Coverage Provider sells his NFT he also transfers the ownership of the staked bmiDAIx.\nPolicy Holder pays a premium for Coverage to protect against the Coverage Event that could affect the insured Project and cause them to lose funds. The total cost of the Premium is split : 80% to coverage 20% to the Reinsurance Pool. They can buy cover for minimum of 1 week and maximum of 52 weeks. Three factors determine the price of cover (premium):\n\nThe utilization ratio of the pool (ratio between cover bought and cover provided, pools that have higher utilization ratio are riskier and more expensive)\nDuration of the cover\nAmount of cover which user wants to buy\n\nThe Reinsurance pool consists of protocol owned funds that are used to provide liquidity to Coverage Pools. The reinsurance pool is funded through 20% of all premiums paid as mentioned above.\nThe Capital Pool aggregates USDT from the Coverage Pools and provides additional revenue to the protocol. Capital Pool sends USDT to yield generating platforms they deem to have low risk. We coulnd’t find exactly what those platforms are. This is a similarity with classic insurance companies which are limited in what way they invest their funds.\nProving a loss\nPolicy holders should submit any or all of the following:\n\nTransaction IDs proving that Policy Holder’s wallet deposited assets into the protocol and transaction IDs of the Coverable Event\nPosts from Protocol team, or an auditing team confirming that there was an exploit and providing additional information\nA description of the Coverable Event\nIf the address affected was not the same address that purchased coverage, any evidence that proves the Policy Holder is the bonafide owner of the address that suffered a Permanent Loss.\n\nThey also need to deposit 1% of the claim’s value in BMI to prevent spam claims. If the claim is valid, USDT is issued to the Claimant. If a claim is denied they can try again by depositing 1% again.\nA successful claim can only recieve up to the policy’s maximum coverable amount. If the DAO determines that the loss suffered was less then the maximum coverable amount, policy owners may recieve less funds. The DAO is incentivized to pay the claimant the exact amount that was lost.\nVoting process\nVotes for claim approval are anonymous and any user holding vBMI can vote. Voters can wote on multiple Claims before submitting them in a batch send. This is done to save time and gas. Only users that vote in the majority are rewarded with BMI for voting. Users that vote in the minority can lose “reputation” which decreases voting power. If there is a large diference in voting yes or no (80% to 20%) users that voted no will lose a portion of their stake.\n\nFigure 2. High Level Overview of Bridge Mutual’s Mechanism\n\n\nInsureAce.io\nInsurAce.io is a multi-chain mutual insurance protocol created in April 2021. It offers products that cover 100+ protocols, 3 CEX and 1 IDO platform. Currently they are depoloyed on Ethereum, Binance Smart Chain, Polygon and Avalanche. InsurAce hasn’t yet adopted the DAO governance mechanism, although they are working on it.\nCurrent state of Insurace.io (Capital pool size, Active cover amount, Capital Ratios etc ) can be found here: https://app.insurace.io/Data/Insurance\nThis protocols has 4 unique selling propositions :\n\n“0” Premium - Which means that the premiums are lower for their products. Their team designed portfolio-centric products to embrace risk diversification, developed models to optimize the cover cost. They did so by using advisors that are experts in the Insurance domain.\nEnriched Product Line - InsurAce.io also offers products that covers non-Ethereum DeFi protocols.\n\nTypes of protocols and smart contract systems covered:\n\nLending Protocols\nDecentralized Exchanges\nDerivative (e.g. Synthetix, Nexus Mutual)\nAsset (e.g. Badger, RenVM)\n\n\nSCR Mining - The participants earn $INSUR tokens by staking into the mining pool. The mutual capitals injected through staking will be managed with rigorous risk control models to adjust the Solvency Capital Requirement (SCR) dynamically and use the secured free capital for investment to control the mining speed accordingly.\nInsurAce tries to combat the low investment returns. Nexus mutual offers capital return to their providers from the premiums paid by users which is low compared to the yield on Compound and Aave. This problem makes users prefer putting their funds elsewhere, instead of the Insurance Protocol. Insurace combats this with offering users:\n\nOption to invest directly in the investment product depending on their risk aversion\nOption to stake in the mutual pool and get the investment carries and $INSUR tokens as rewards\nShares of the premium income\n\n\nInsurAce is operates similarly like the traditional insurance company using the insurance arm and the investment arm.\n“The insurance arm maintains reserve pools which maintain the solvency for claim coverage based on risk exposure. The investment arm maintains investment pools that generate carry to subsidize claims and attracts investors with risk appetite. The free capital in the insurance capital pool can be placed into the investment pool to gain a higher yield, while the insurance arm will protect the investment activities. Meanwhile, the investment arm’s yield will complement the premium on the insurance side and reduce the cover cost for customers.” ‘Whitepaper’8\nPricing model\nWhen it comes to before mentioned protocols they rely heavily on the value staked on individual protocols: the higher value staked the lower the premium will be priced. InsurAce tries to combat this with adopting the new actuary-based pricing model to mitigate this in order to assess the expected loss of insurance products fairly, reduce costs and enhance capability.\n“The model’s main inputs are the number/amount of claims and number/amount of exposures in a given time period, which will be used for selecting and training two separate models - the frequency model and the severity model. Frequency modeling produces a model that calibrates the probability of a given number of losses occurring during a specific period, while severity modeling produces the distribution of loss amounts and sets the level of deductible and limit of the coverage amount.” These models are then combined to solve aggregate loss. After that the decided aggregate loss is incorporated into the risk factors of protocols and the premiums are then calculated. The model’s parameters rely on historical data to devise and validate. They plan on taking this further with new Machine Learning methodologies.\nCapital model\nInsurAce’s capital model refers to EIOPA’s Solvency II, this regime is used for insurance and reinsurance in the European Union. It sets requirements needed for insurance products in order to protect policyholdes and beneficiaries.\n“Solvency II is an economic risk-based approach that should assess the”overall solvency” of insurance and reinsurance undertakings through quantitative and qualitative measures. Under Solvency II, the undertaking’s solvency requirements are determined based on their risk profiles and how such risks are managed, providing the right incentives for sound risk management practices and securing enhanced transparency.”\nSolvency II has different tiers of which the SCR (Solvency Capital Requirement) and MCR (Minimum Capital Requirement) are the two most important ones.\n“The SCR is the capital required to ensure that the insurance company will be able to meet its obligations over the next 12 months with a probability of at least 99.5%, while MCR represents the threshold to correspond to an 85% probability of adequacy over 12 months and is bound between 25% and 45% of the SCR. For supervisory purposes, the SCR and MCR can be regarded as”soft” and “hard” floors.”\nInsurAce uses SCR to calculate the minimum requirement funds set aside to pay all the potential claims considering all quantifiable risks. SCR is calculated with the following inputs:\n\nAll the active covers\nAll the outstanding claims\nThe potential incurred but not reported claims\nThe market currency shock risk\nThe non-life premium & reserve, lapse and catastrophe risks\nThe potential operational risk\n\nSCR% = Capital Pool Size / SCR\nA high ratio means the insurance company is financially strong with sufficient available funds to cover potential claims and other risks so the company is less likely to be insolvent . The lowest acceptable ratio is 100%.\nInsurAce also offers information of their Capital Efficiency ratio which shows the company’s current success in deploying capital.\nCER% = Active Cover Amount / Capital Pool Size\nA high ratio means the insurance company is increasing the productivity of its assets to generate income. Desired ratio is between 100% and 300%.\nRisk Assesment by InsurAce.io\nInsuraAce’s Advisory Board performs a preliminary risk assesment on the new protocols at first. InsurAce will also work with auditing firms if there is extra complexity or challenges. After that Advisory Board provides a report and rates the protocol 1 to 5. After they rate it protocol will go through the community risk assesment. Members who participate in the assesment get INSUR tokens as incentive.\nClaims Assesment by InsurAce.io\n\nFigure 3: InsureAce’s Claim Assesment process\nThis diagram shows us the whole system of claims Assesment in a clear way. The main difference is the inclusion of the Advisory board members which consists of various industry experts including the CTO of InsurAce.\n“$INSUR token holders can stake the $INSUR tokens to become the community Claim Assessor. Claim Assessor will be entitled to the right to vote in each claim assessment and earn $INSUR tokens as reward if their votes match with the voting result. During each voting session, the more tokens the user stake, the more voting tickets they will get (* capped at 5% of the total votes), and the more rewards they will receive.”\n\n\nNsure\nNsure was targeted to be a platform for users to trade risks borrowing the operation model of previously mentioned Lloyds London. With Nsure information is transparent and users are allowed not only to be outsorcing risk but also to become risk takers, capital providers, governance actors and auditors of the system.\nMore data on Nsure performance can be found here: https://app.nsure.network/#/cover/my\nProduct\nNsure offers Smart contract cover like previously mentioned protocols. The coverage and exclusions are identical so we will not go into great detail. More info can be found at: https://docs.nsure.network/nsure-network/docs/nsure-smart-contract-protect-policy-wording\nCapital model\nCapital is sourced from capital mining, with return of Nsure tokens for the miners to ensure a continuous capital support to the underwriting. Minimum Capital Requirement (MCR) is calculated based on the volume of each project and the correlation between them. A low MCR% below a pre-determined threshold will result into a lock of assets in capital pool, so as to protect the solvency the business.\nPricing model\nNsure uses a Dynamic Pricing Model to set the price. In this model capital supply and demand from the entire platform determines the price jointly similar to the pricing mechanism in the free market, by having Nsure tokens backing the policies bought. The price is self-adjustable to the movement of supply and demand, subject to the model, moderately stabilising the price change.\nRating System\nNsure uses its N-SCOSS rating system to quantify the code security of projects by assessing:\n\nHistory and Team\nExposure\nAudit\nCode quality\nDeveloper community\n\nClaim process\n\nFigure 4: Nsure claim process\nFor each policy sold there is one chance of claim filling for free. If the first claim was rejected a claim assesment fee of 10% is requested before new assesment.\nPolicy holder must provide evidence of loss on the designated project within the insurance period. Proof of loss must include:\n\nProof of ownership of affected account - After identifying his affected account, policyholder may prove his ownership over the account by signature or making a 0 amount payment to a specified address.\nEvidence of loss - Policy holder should provide:\n\nthe snapshot of the affected address’s balance at blocks before and after attack (to assist claim assessors quickly quantify the amount of lost\ntransaction of selling the damaged assets (loss is only recognised when it is realised)\ndescription of the attack from project team or security specialist\n\n\nClaim assesment\nNsure introduced a similar voting mechanism as previously mentioned protocols. Its features are:\n\nTo be registered as a claim assessor candidate, user must deposit a considerable amount of Nsure token. At launch, the deposit is set at () Nsure token.\nClaim assessors are randomly picked from registered candidate. For each claim, there will be 5 claim assessors.\nAs claim assessors’ reward is proportion to premium, users tend to register for larger size policy. To get each policy equal and fair tender, users do not know the premium of the policy at registration.\nThe token will be slashed if the claim assessor’s judgment is different from the majority.\n\nAfter claims assessors make a decision, policyholder and other Nsure token holders can challenge this decision. This will lead to a public vote for the final conclusion on the issue.\n\n\nEtherisc\nSo far we have been covering only protocols that offer smart contracts protection. Etherisc tries to include material goods into the story. Etherisc is a protocol to collectively build insurance products. Common infrastructure, product templates and insurance license-as-a-service make a platform that allows anyone to create their own insurance products. The first product Etherisc offered was FlightDelay Insurance. Products currently licensed are: Crop Insurance and FlightDelay Insurance. Products currently in design: Hurricane Protection, Crypto Wallet Insurance, Collateral Protection for Crypto backed Loans, Social Insurance (death, heavy illness). They are also open for product requests. Users have the option to build their own insurance product, but more information about the user needs to be provided.9\nThey also launched a Joint Grant Program with Chainlink to accelerate the adoption of data-driven decentralized insurance products, so we think that special attention should be paid towards potential building with Etherisc.10\nEtherisc Token\nDIP Tokens act as the native internal currency that is inseparable from the protocol and network of its users. DIP tokens are needed to earn transaction fees (% of insurance premiums or fixed cost), incentivize and reward platform users to bring risk to the network, build and maintain risk transfer products. The total supply of Etherisc Tokens is 1 Billion.\nDIP tokens give users access to the Decentralized Insurance Platform. By staking DIP token, participants provide collateral (bond) to guarantee future performance, availability, and service levels. Staking also signals quality and reputation. As a result, participants can earn money monetizing their skills, software (for example risk models or UI/UX), risk capital, insurance licenses, claim processing, or regulatory compliance/reporting services.\nFlightDelay Insurance example from the Whitepaper - Launched on January 20 2022\nIn their whitepaper’s FlightDelay Insurance product they use oraclize to obtain data from their data provider. Oraclize charges Etherisc for calling their contract. Etherisc incentivize Oraclize to provide their service correctly by :\n\nIn the buyers market (market with many oracles) - Demanding of Oraclize to put some tokens in a staking contract which will then returns tokens if they deliver in time and forward the tokens to Etherisc in case they miss their obligations.\nIn the sellers market (market with only one or few oracles) - Oraclize will earn an additional profit, again by staking tokens in a “staking contract”, but with reversed roles: Etherisc will stake tokens, and Oraclize will earn these tokens if they deliver, and in case they don’t deliver, the tokens are returned to Etherisc.\nBoth options can be combined with both parties staking tokens from historical flight delay\n\nTheir experience with the Flight Delay DApp confirmed that insurance applications need plenty of capital to be able to scale. But that entry barrier can be overcome with cryptographic tokens that enable highly customized economics. Their goal is to allow the tokenization of risks on the platform and to make them available on a global open-access marketplace.\nIn this kind of insurance the probability is calculated from historical flight data. They used flight delay initially as a POC because of low premiums assosciated with them and under normal circumstances flight dellays are well-aproximated by independent probability events. Etherisk leverages ChainLink data.\nOn January 20th 2022 Etherisc launched FlightDelay on Gnosis Chain Mainnet. It uses Chainlink Data Feeds to autonomously issue policies and execute payouts for travelers who experience flight delays or cancellations. The result of this is insurance policies that are quicker to settle, cheaper to provision thanks to decreases in human and technical overhead and more transparent given the blockchain backend.\nFlightDelay is now available for passenger flights globally. The insurance policies are purchasable with USDC using the Gnosis Chain on Etherisc’s FlightDelay portal. More payment options are in the works.\nParticipants in the Etherisc Protocol\nParticipants in the protocol are:\n\nCustomers - Customers can buy insurance using the token. For convenience, third parties can offer payment gateways and integrations which remove the necessity to own cryptocurrency from the end customer. Furthermore, participants can choose to offer insurance products in any native currency - be it a cryptocurrency, a token or a fiat currency. Use of token: Universal currency to buy insurance products.\nRisk model Providers and Actuaries - “Risk models are fundamental for any insurance product. The correctness of the model is precondition for the economic success of the product.Generally, because of the magnitude of value affected by errors and deviations in the model, a Risk Model Provider won’t take responsibility for the economic outcome of his model, but rather for his adherence to principles and established guidelines in his trade.” Use of token: Staking/Reward for providing or updating risk models\nData providers and oracles - Currently, data is collected together with the application for an insurance, and the insurance company “owns” the data - even after the insurance contract is no longer valid. In a blockchain decentralized environment, the collection of data could be separated. Customers could get paid for voluntarily offering their data to a data pool, which in turn can sell this to interested parties, leaving the ownership of the data completely with the customer. This is an interesting take on handling events in the real world and the real world application of crypto insurance. Use of token: Reward for giving data. Reward for giving access to data pools. Staking / Reward for providing reliable oracles.\nSales Agents - Sales agents are responsible for offering insurance products like in the traditional insurance. Use of token: Reward for distribution of products.\nClaim Agents - There are still many cases where automatic detection and processing the claims is not possible. Specialized and sometimes independent claims agents already exist that can be somewhat utilized e.g. in the area of car insurance, where they help insurers to process claims in shorter time. These claims agents can immediately use a decentralized platform, as soon as adequate products are available. Use of token: Reward for the provided service.\nLicense providers - Insurance in most countries depends on a proper license which can be difficult and costly to obtain. There is also a model where a license provider can act as an intermediary to regulators which is interesting if we are to build a new kind of insurance product. Use of tokens: Staking tokens to provide capital for a license provider, paying fees for licenses.\nProduct managers - Use of token: Reward for service.\n\nEtheriscs’ approach to crypto insurance is interesting but majority of their products are still in the works.Their first product FlightDelay Insurance was launched on January 10th 2022.11"
  },
  {
    "objectID": "documents/research/posts/ERFC-103.hugo.html",
    "href": "documents/research/posts/ERFC-103.hugo.html",
    "title": "ROTOKEN",
    "section": "",
    "text": "Organisations may include non-founder members with specific roles (treasurer, secretary, board members…) with higher permission levels, compared to the regular members. Members that have permission levels which include control over organisation finances represent a high risk for the organisation as there is a high risk of losing money if malicious members perform illegal transfers. That problem is especially present in the world of cryptocurrency where transfers cannot be revoked, once transaction is executed there is no rollback. The use of multisig might be an obvious solution, but the high overhead cost of writting multiple signatures on blockchain, even if the transaction is valid, may not be acceptable in many situations where there is a high frequency of transactions.\nThis research proposes a solution for this problem, restricted to the world of crypto-financial transactions, by introducing a rotating token which guarantees randomised selection of members that would be granted specified roles over fixed period of time while allowing blockchain money-transferring transactions to be executed only by the members that are holding the token. For improved prevention of unwanted transactions, the transactions may be disputed before their execution, which allows higher control over money transfers and enables improved protection of organisation funds. All transactions are transparent to all approved members, which adds extra layer of transparency over the entire organisation budget."
  },
  {
    "objectID": "documents/research/posts/ERFC-103.hugo.html#potential-markets",
    "href": "documents/research/posts/ERFC-103.hugo.html#potential-markets",
    "title": "ROTOKEN",
    "section": "Potential markets",
    "text": "Potential markets\nPotential markets that might be interested to use ROTOKEN include: - DAOs - Companies with frequent audits by the investors, board members or shareholders, mostly startups - Banks - Government organisations related with public government funds"
  },
  {
    "objectID": "documents/research/posts/ERFC-38.hugo.html",
    "href": "documents/research/posts/ERFC-38.hugo.html",
    "title": "Building on Filecoin and Filecoin Proofs",
    "section": "",
    "text": "When looking to build dApps that utilize decentralized storage Filecoin seems like the best option even with its flaws like : absent proof of deletion for the client, absent encryption, impossible modifying of the stored data and the durability problem of Filecoin’s Proof of Replication. This research gives an overview of competitors in decentralized storage solution field with a focus on Filecoin protocol. It also shows how its competitor Storj differs from Filecoin and the current grant opportunities for potential building on it. Considering the results of the research Filecoin seems to be the best option considering the popularity and the size of its ecosystem. Currently there is no interest for building on and with Filecoin and this is a purely explorative work without experiments, in order to test the researcher’s methodology and the approach to research. However, it proposes a question about a potential way of improving Filecoin, or creating both safer protocol for the user and cheaper for the storage miners."
  },
  {
    "objectID": "documents/research/posts/ERFC-38.hugo.html#goals-methodology",
    "href": "documents/research/posts/ERFC-38.hugo.html#goals-methodology",
    "title": "Building on Filecoin and Filecoin Proofs",
    "section": "Goals & Methodology",
    "text": "Goals & Methodology\nThe aim of this research is to explore Filecoin protocol and show its fallacies as much as writer is possible with a goal of inspiring building applications that utilize Filecoin, new protocols or improving Filecoin via building for it. The paper will compile the list of all fallacies and possibilities for improvement for Filecoin, opportunities for building on it, and the current state of decentralized storage market mainly comparing Storj and Filecoin."
  },
  {
    "objectID": "documents/research/posts/ERFC-38.hugo.html#introduction",
    "href": "documents/research/posts/ERFC-38.hugo.html#introduction",
    "title": "Building on Filecoin and Filecoin Proofs",
    "section": "Introduction",
    "text": "Introduction\nUnlike a centralized server operated by a single company, decentralized storage systems consist of a peer-to-peer network of user-operators who hold a portion of the overall data. The platforms can store any data sent by the user, with some platforms more focusing on encryption. There is no official data disclosing the types of data stored. This research will be covering contract-based persistence platforms with a focus on Filecoin.\nContract-based persistence means that data cannot be replicated by every node and stored forever, and instead must be upkept with contract agreements. These are agreements made with multiple nodes that have promised to hold a piece of data for a period of time. They must be refunded or renewed whenever they run out to keep the data persisted. Platforms with contract-based persistence currently present on the market are:\n\nFilecoin\nSkynet\nStorj\n0Chain\n\nFilecoin is a peer-to-peer network that stores files with built in economic incentives to ensure files are stored reliably over time. Users pay to store their files on storage providers. Storage providers are computers responsible for storing files and proving they have stored the files correctly over time. Available storage and the price of it is not controlled by any company. Anyone who wants to store their files or get paid for storing other users files can join Filecoin is written in its documentation, but is that really the case? It will be further explored in later sections. Filecoin’s native currency is FIL. Storage providers earn units of FIL for storing user’s data. Its blockchain records transactions along with proofs from storage providers that they are storing files correctly.\nCurrently Filecoin stores over 40.0453 PiB of users data over 1,848,292 deals.1\nWhen users want to store their files on Filecoin they use terminal or different guis that have been built by developers to choose between cost, redundancy and speed and they select the storage provider whose storage offer is best suited for their needs. Applications that implement filecoin negotiate storage with storage providers. There is no need for different API for each provider.2\n“While interacting with IPFS does not require using Filecoin, all Filecoin nodes are IPFS nodes under the hood, and (with some manual configuration) can connect to and fetch IPLD-formatted data from other IPFS nodes using libp2p. However, Filecoin nodes don’t join or participate in the public IPFS DHT. IPFS alone does not include a built-in mechanism to incentivize the storage of data for other people. This is the challenge Filecoin aims to solve. Filecoin is built on IPFS to create a distributed storage marketplace for long-term storage.”3"
  },
  {
    "objectID": "documents/research/posts/ERFC-38.hugo.html#filecoins-proof-system",
    "href": "documents/research/posts/ERFC-38.hugo.html#filecoins-proof-system",
    "title": "Building on Filecoin and Filecoin Proofs",
    "section": "Filecoin’s Proof System",
    "text": "Filecoin’s Proof System\nFilecoin uses Proof of Replication (PoRep) and Proof of Spacetime (poSt).\nIn a Proof of Replication, a storage miner proves that they are storing a physically unique copy, or replica, of the data. Proof of Replication happens just once, at the time the data is first stored by the miner. As the storage miner receives each piece of client data they place it into a sector, fundamental unit of storage in Filecoin. Sectors can contain pieces from multiple deals and clients. Steps in PoRep: ### Proof of Replication 1. Filling sectors and generating the Commd\nOnce a sector is full a Commitment of Data (CommD) is created, representing the root node of all the piece CIDs contained in the sector.\n\nSealing sectors and producing the Commitment of Replication\nSector data is encoded through a sequence of graph and hashing processes to create a unique replica. The root hash of the merkle tree of the resulting replica is called CommRLast. CommRLast is then hashed together with the CommC(another merkle root output from PoRep). This generates the CommR (Commitment of Replication) which is then recorded on Filecoin’s Blockchain. CommR, last is saved privately by the miner for future use in Proof of Spacetime but is not saved to the chain.\nEncoding process is slow and computationally heavy. Filecoin doesn’t provide encryption by default so users must encrypt data before adding it to the Filecoin network. This is the first issue encountered with Filecoin : Filecoin is optimized for public data and doesn’t yet support access controls.\nThe CommR offers clients the proof that the miner is storing a physically unique copy of the client’s data. If a client stores the same data with multiple storage miners, or makes multiple storage deals for the same data with the same miner, each deal will yield a different CommR. The sealing process also compresses the Proof of Replication using zk-SNARKs to keep the chain smaller so that it can be stored by all members of the Filecoin network for verification purposes.\nUnlike PoRep which is run once to prove that a miner stored a physically unique copy of the data at the time the sector was sealed. PoSt is run repeteadly to prove that the miners are continuing to dedicate storage space to that same data over time.4 ### Proof of Spacetime PoSt builds on several elements created during PoRep: the replica, the private CommRLast and public Commr. PoSt then selects some leaf nodes of the encoded replica and runs merkle inclusion proofs on them to prove that the miner has the specific bytes that indicate that he still holds the clients data. The miner then uses the privately stored CommRLast to prove that they know of a root for the replica which both agrees with the inclusion proofs and can be used to derive the CommR. As the final step PoSt compresses these proofs into a zk-Snark.\nIf the miners fail the Proof of Spacetime at any point they will lose their staked collateral. Aside for this fine, there is no other incentive to keep the miners storing the data. That becomes a problem if client’s storing private data or data of great significance.5"
  },
  {
    "objectID": "documents/research/posts/ERFC-38.hugo.html#results-discussion",
    "href": "documents/research/posts/ERFC-38.hugo.html#results-discussion",
    "title": "Building on Filecoin and Filecoin Proofs",
    "section": "Results & Discussion",
    "text": "Results & Discussion\n\nCritique From the users perspective:\nAs of today the price of 1 Filecoin token (FIL) is $22.05. The users can store a gigabyte of data for as little as 0.0000006157315278020465 FIL (0.01% the cost of Amazon S3) which means the user can store 100GB of data for $0.00135.6 From the price perspective the upside of Filecoin is it’s cheap storage but there are various downsides such as:\n\nAccesibility: if the user is not tech-savvy there is a big barrier to entry even with GUIs currently available. They are often not simple to install and it is hard to get them to work. Hovewer there are various Apps that make that somewhat easier like ChainSafe Files and Web3.Storage.When it comes to being a storage miner there minimal needs for hardware are:\n\n\n8+ core CPU\n128 GiB of RAM atleast\nA strong GPU for SNARK computations\n1TiB NVMe-based disk space for cache storage is recommended\nThis data shows that there needs to be a significant investment on storage miners part which is nothing out of the ordinary but significantly reduces the accessibility to the average person, ofcourse assuming the person wants to be a storage miner.\n\n\nAs the price of FIL tokens fluctuate the price of storage fluctuates as well. There is also a risk if any extra FIL is left in the customers wallet after the storage contract then token could potentially drop/rise in value, not to mention the fees of converting fiat into cryptocurrency.\nThere is no built in encryption. Users need to encrypt their data on their own. Encryption/decryption of files cost compute resources (RAM, CPU), and therefore money. Most end users would prefer the implementation of this functionality to be handled, optionally, by the Filecoin web, cli or desktop client software they choose to make use of. This problem is seemingly taken care of with apps like ChainSafe Files.7\nChainSafe Files is an online platform to store, view, and share files. Its main focus is data privacy of the users and self-reliance. When it comes to self-reliance ChainSafe Files makes sure that the users can access the stored files even if the Files platform becomes unavailable. It also offers authentication flow using a decentralized login provider called tKey, by Torus. tKey is a private key generator that can link keys to social accounts among other functions. The Files’ backend is built on top of ChainSafe Storage, and any file that is uploaded to Files is also pinned by a node on its infrastructure (each file has an CID) thus making the data retrieval possible even in the case of ChainSafe files app outage. In the case of retrieval users still have to decrypt the data, since all the data stored by ChainSafe Files is encrypted by default. ‘ChainSafe Files - Decentralized Cloud Storage’8\nIf the storage provider doesn’t respect his end of the deal he will be penalized and lose his staked FIL. Unless negotiating a great number of deals for the same data and storing a lot of copies in Filecoin, there is no guarantee that the data will be safely stored. These deals for the same data increase the cost of the service if the user wants to have somewhat durable data. Filecoin tries to mittigate this by having storage miners put 100+ FIL in collateral which also lowers the accessibility to the average person to become a storage miner. Currently most of the storage miners are located in China.\nFilecoin storage is cold storage. There is no way to modify data. If the user needs to change data , new data must be written.9\nIf the user issues a deletion command there is no guarantee that the client performs the operation. There is no way yet to Construct a formal Proof of Deletion.\n\n\n\nThe issue with Replication and Filecoin\nWhen decentralized storage network is utilized any storage node could go offline thus the stored data would be at risk of getting lost. To achieve a somewhat reliable storage many decentralized providers use replication, which means the only way to keep the users data reliably besides penalizing storage miners is to store multiple copies of it. Replication is not good for the network expansion factor. If Filecoin wants more durability for its data it needs more copies. For every increase of durability (storing or repairing the data) another multiple of the data size in bandwith is needed. Eg. If the durability level requires a replication strategy that makes 10 copies of the data this yields and expansion factor of 1000%. This data needs to be stored on the network, using bandwith in the process. The more replication the bigger the bandwith usage. Hovewer, if the node goes offline , only one of the storage nodes is needed to bring a new replacement node in, which again means that the 100% of the replicated data must be transferred. Excessive expansion factors produce an inefficient allocation of resources.10\nAnother issue with replication is churn (nodes joining and leaving the network). Quoting Patrick Gerbes and John Gleeson: “Using replication in a high-churn environment is not only impractical, but inevitably doomed to fail. Distributed storage systems have mechanisms to repair data by replacing the pieces that become unavailable due to node churn. However, in distributed cloud storage systems, file repair incurs a cost for the bandwidth utilized during the repair process. Regardless of whether file pieces are simply replicated, or whether erasure coding is used to recreate missing pieces, the file repair process requires pieces to be downloaded from available nodes and uploaded to other uncorrelated and available nodes.”11\nCurrently the circulating supply of FIL token is 162,302,978.00 FIL. The potential circulating FIL could reach 1.977 Billion tokens if the network hits a Yottabyte of storage capacity in under 20 years which is brave considering the current data stored in data centers today is less then a Zettabyte and a Yottabite is 1000 times larger. The 770 Million of which is for baseline minting.\n330 million FIL tokens are released on a 6 year half-life based on time. A 6 year half-life means that 97% of these tokens will be released in aproximately 30 years. This amount is amount is minted to provide counter pressure to shocks.\nAnother 300 million FIL is held back in reserve to incentivize future types of mining. How they are released is up to the Filecoin community.12\n\nFigure 1: Maximum and Minimum Minting from Storage Mining.\n\nFigure 2: Network Storage Baseline for Max Baseline Minting on Log Scale.\nAs of today 29,180,207.338966275 FIL has been slashed.This means that if a network participant misbehaves, part of their FIL collateral or potential FIL rewards is confiscated and burned. FIL is also slashed for various other reasons.\nConsidering the token release we can expect total supply of FIL token to be almost doubled in the next five years. If we look the rate of slashing so far the dilluting process will exceed the tokens burned. This makes specullating Filecoin token price risky, both for the investors and users.13"
  },
  {
    "objectID": "documents/research/posts/ERFC-38.hugo.html#filecoin-and-storj-comparission",
    "href": "documents/research/posts/ERFC-38.hugo.html#filecoin-and-storj-comparission",
    "title": "Building on Filecoin and Filecoin Proofs",
    "section": "Filecoin and Storj comparission",
    "text": "Filecoin and Storj comparission\nWe will compare Filecoin and Storj by: 1. Consensus Algorithms\nFilecoin’s consesus mechanism has been covered in the paragraphs above.\nStorj does not have its own chain: the platform is built on Ethereum(currently built on PoW). The reason they decided to build on the Ethereum network is because of simplicity of using it as a method exchange. From the start Storj was never intented to be a true decentralized storage network.\n\nBlock time\nFilecoin’s block time is thirty seconds on average.\nStorj is the only “decentralized storage” solution to not employ their own chain. Storj decided to use the Ethereum network due to the easy deployment of a coin on it. Because of this decision, the block time is twelve seconds and needs to deal with the consequences of sharing blockchain with other projects.\nEnforcement of data retention\nDue to the PoSt mentioned in the paragraphs above the entire network is designed around data retention. In the case of miner failing to keep his promise , the only backup of the user’s data was irreversibly lost since Filecoin doesn’t use redundancy which is also mentioned in the paragraphs above.\nIn Storj the actual enforcement of data isn’t clearly documented. Each Satellite (the interface between the storage operators and the clients) does the enforcement of data retention all on their own. Each Satellite has its own subset of storage nodes that it knows have a good reputation and it trusts, it uses these hosts to upload data to. Then it, at regular intervals, checks random data segments with “Berlekamp-Welch” error correction to make sure that the data is still there. If they fail to prove they store the data the reputation of that host is changed and data migrates to a new host. There is no chain-level enforcement for data retention.\nContent distribution\nIn Filecoin data has to be sealed to be counted as a provable storage to the chain and because it is computation heavy it isn’t practical for the miners to serve data. 1 MiB file can take 5 to 10 minutes to unseal and 32 GiB file takes 3 hours on minimum hardware requirements mentioned above. To battle this Filecoin introduced a method to store cached and unsealed versions of data while storing the same data as sealed in order to provide proofs. This leads to the issue of miner storing the double amount of data while also posing the problem that unless the data is frequently accessed the miners will not store it because it isn’t profitable for them. That makes creating Google drive equivalent on Filecoin not practical because the data is not frequent enough to makes sense to cache while also being low latency (because of slow sealing and unsealing).\nIn Storj’s case data is being accessible only through the S3 gateway of centralized data data Satellites. Users can transform any data to public data and can send anyone a link to that data.\nSector size\nSector size is the minimum amount of data that can be sent to host and paid for. Filecoin has a fixed 32 GiB sector size. For each 256b stored 2b are proofs. Which means 1% of storage paid for is for proof. Also everything sent must be in a .car file which can be computationally heavy on the client-side.\nIn Storj’s case the sector size is not really clear. Current object fee is $0.0000022 per file stored. Which means that if there is a large amount of small files stored that would bring extra costs to the user.\nDecentralization\nBecause of the Filecoin’s Hardware requirements for hosts that means not everyone can run a storage node. On the other networks basically anyone can run a storage node( most minimal requirements are 2 cores 8GB of RAM and some storage) but in Filecoin only people with sufficient funds for initial investments can run hosts which reduces the spread of the network. Which makes us question the actuall decentralization of the network.\nStorj isn’t decentralized. The blockchain nature of the Storj coin is only designed for for efficient transacting not decentralized enforcement of data retention which means that Storj is actually a distributed storage provider but not fully decentralized.14"
  },
  {
    "objectID": "documents/research/posts/ERFC-38.hugo.html#filecoin-grants",
    "href": "documents/research/posts/ERFC-38.hugo.html#filecoin-grants",
    "title": "Building on Filecoin and Filecoin Proofs",
    "section": "Filecoin Grants",
    "text": "Filecoin Grants\nFilecoin offers various types of grants for building with Filecoin.\n\nNext Step Microgrants\nFilecoin offers grants of 5000$ in FIL to support taking the next step when the initial prototype is created. Their purpose is financing projects in the early stage. Acceptance criteria is simple. Projects must meet these criteria: 1. Applicant has already built something with Filecoin (or closely related technologies such as IPLD, libp2p, or frameworks or services such as NFT.storage, Textile Powergate, etc.), independently or as part of a course or hackathon. 2. Applicant must provide clear description of the Next Step after grant support 3. The project can be completed within 3 months. 4. Project must be open-sourced. 5. The applicant must complete weekly updates and a grant report upon conclusion.\nProjects that qualify for Microgrants: 1. Projects that publish data or files to IPFS 2. Projects that don’t use IPFS directly 3. Projects that save data or retrieve data from the Filecoin Network 4. Non-coding projects, videos, tutorials etc\nThese grants are offered on the quarterly basis. ### Open Grants Filecoin’s focus areas currently are: 1. Core development - core protocol research, specification and implementation work 2. Application Development - applications that utilize Filecoin as a decentralized storage layer 3. Developer tools and libraries - tools and libraries for protocol developers and application developers 4. Integration and adoption - integration into existing app or projects with significant usage 5. Technical design - improvement proposals for the core storage protocol 6. Documentation 7. Community building 8. Metaverse - experiences, applications, communities, tooling, standards, infrastructure, et cetera15 9. Research that explores Filecoin and decentralized storage16"
  },
  {
    "objectID": "documents/research/posts/ERFC-38.hugo.html#conclusion",
    "href": "documents/research/posts/ERFC-38.hugo.html#conclusion",
    "title": "Building on Filecoin and Filecoin Proofs",
    "section": "Conclusion",
    "text": "Conclusion\nWhen it comes to decentralized storage solutions Filecoin draws all the attention compared to its competitors like Storj and Sia. Storj solves Filecoin’s replication problem, on the expense of decentralization but the internet search statistics still shows that Filecoin is the main contender in Decentralized storage. \nFigure 3. Filecoin (blue) vs Storj (red) search interest in the past 12 months.\nEven with unclear tokenomics a nd problematic durability to the writer of this research, building with Filecoin seems like the best solution when building dApps that utilize decentralized storage because of its low cost and grant opportunities, even though storage miners are somewhat centralized. If there is a way to improve current storage miner’s/clients experience on Filecoin both in cost and ease of use that would surely be a great grant opportunity. Also there could potentially be a way to draw them to a protocol that is a cheaper alternative. Ofcourse, these are all assumptions, since much of the data on these protocol is unclear and not available."
  },
  {
    "objectID": "documents/research/posts/ERFC-171.hugo.html",
    "href": "documents/research/posts/ERFC-171.hugo.html",
    "title": "Royalty Contract Standardization - RCS",
    "section": "",
    "text": "This research examines the possibility of hardcoding the royalty logic for the NFT royalty payments. It also explores how are royalties for NFT creators/artists taken care of by two largest NFT marketplaces OpenSea and LooksRare. It examines the EIP-2981 which aims to solve the royalty implementation problem. Small experiment is conducted with a goal to modify the transfer function from the ERC-721 standard.\nAfter explorative research and a short experiment we have come to these conclusions:\n\nThe Marketplaces prefer handling the royalties themselves and only for the trades on their platform\nERC-2981 contains the optional royalty implementation logic. It’s on the platforms to decide whether they will utilize this standard.\nHardcoding royalties without making “a mess” of the NFT smart-contract is currently way too complex and would require altering the ERC-721 heavily."
  },
  {
    "objectID": "documents/research/posts/ERFC-171.hugo.html#opensea",
    "href": "documents/research/posts/ERFC-171.hugo.html#opensea",
    "title": "Royalty Contract Standardization - RCS",
    "section": "OpenSea",
    "text": "OpenSea\nOpenSea offers royalties for Artists and Creator which are usually around 10%. They are also applied to secondary sales and the proceeds after fees go to the seller of the NFT.\nUsers can check the royalty fees with 3 methods:\n\nAttempting to buy an NFT which will then open up a checkout window where the royalty amount is listed under the name of the NFT.\nInstalling the Flava Chrome extension which shows the royalty next to the NFT without needing to open the checkout menu.\nUsing the NFT analytics tools - There are numerous NFT analytics tools that include creator royalties in their stats.1\n\nAs we can see royalties are very important part of NFT space and they are of great importance for both traders and creators.\n\nHow do creators earn their royaties\nOn OpenSea proceeds from the primary sales of the NFT are immediately forwarded to the creators address. Royalties are usually held by OpenSea for 2-4 weeks before paid out to the creator, this includes both primary sales and secondary sales.\nRoyalties are not automatically set on OpenSea and the creator of the collection must set the royalty percentage and the payout address on the collection level.\n\n\nOpenSea royalty on other marketplaces\nOpenSea royalties are enforced on many other platforms. This is a result of various legal agreements between platforms.\nIf we are talking about ERC-721 and ERC-1155 standard tokens there is no royalty support on token or smart contract level. That is why previosly mentioned legal agreements are needed to enforce royalty payments.2"
  },
  {
    "objectID": "documents/research/posts/ERFC-171.hugo.html#looksrare",
    "href": "documents/research/posts/ERFC-171.hugo.html#looksrare",
    "title": "Royalty Contract Standardization - RCS",
    "section": "LooksRare",
    "text": "LooksRare\nLooksRare is a decentralized NFT marketplace which rewards traders, token stakers, creators and collectors for participating on the platform.\nIt was launched in January 22 with an aim to dethrone Opensea from it’s spot as a leader in the NFT market.\nAs a community-first platform all the revenue generated is distributed to the stakers of LOOKS token.\n\nToken\nLOOKS is the native token that powers the LooksRare marketplace, its price is $0.5908 at the time of this writing. It is used for staking and various rewards.\n\n\nRoyalties\nWhenever the NFT is traded on LooksRare there are 2 types of fee the seller is charged:\n\nPlatform fee (2%)\nCreator royalties\n\n“Creator royalties are fees that are decided by the collection creator. Collection owners can specify a percentage of royalties they wish to recieve on their collection management page.”\nRoyalties are on-chain. Whenever a sale is made through the platform the royalties are paid in the same transaction as the sale and the creators instantly recieve their royalty. This is one of the ways LooksRare is different from OpenSea.\nLooksRare also supports EIP-2981 royalty standard which takes precedent over any royalties specified directly on LooksRare."
  },
  {
    "objectID": "documents/research/posts/ERFC-171.hugo.html#eip-2981",
    "href": "documents/research/posts/ERFC-171.hugo.html#eip-2981",
    "title": "Royalty Contract Standardization - RCS",
    "section": "EIP-2981",
    "text": "EIP-2981\nThis standard provides a way to retrieve royalty payment information for NFTs with a goal to enable universal support for royalty payments across all NFT marketplaces and ecosystem participants.\nThis standard enables all marketplaces to retrieve royalty payment information for a given NFT. This enables accurate royalty payments regardless of which marketplace the NFT is sold or re-sold at.\nThis standard only provides a mechanism to fetch the royalty amount and recipients. The actual funds transfer is something the marketplace needs to do.\n“Royalty amounts are always a percentage of the sale price. If a marketplace chooses not to implement this EIP, then no funds will be paid for secondary sales.”\nThat is one of the reasons hardcoding royalties idea was proposed.\nEIP-2981 can also be integrated with other contracts to return royalty payment information. ERC-2981 is a royalty standard for many asset types.\nIt is recomended to read the full specification of the proposal in order to better understand the issue at hand and the way it is handled in the EIP-2981.\nIn the proposal the writers have come to these conclusions:\n\n“It is impossible to know which NFT transfers are the result of sales, and which are merely wallets moving or consolidating their NFTs. Therefore, we cannot force every transfer function, such as transferFrom() in ERC-721, to involve a royalty payment as not every transfer is a sale that would require such payment.”\n“It is impossible to fully know and efficiently implement all possible types of royalty payments logic. With that said, it is on the royalty payment receiver to implement all additional complexity and logic for fee splitting, multiple receivers, taxes, accounting, etc. in their own receiving contract or off-chain processes. Attempting to do this as part of this standard, it would dramatically increase the implementation complexity, increase gas costs, and could not possibly cover every potential use-case.”\n“This EIP mandates a percentage-based royalty fee model. It is likely that the most common case of percentage calculation will be where the royaltyAmount is always calculated from the _salePrice using a fixed percent i.e. if the royalty fee is 10%, then a 10% royalty fee must apply whether _salePrice is 10, 10000 or 1234567890.”\n“This EIP does not specify a currency or token used for sales and royalty payments. The same percentage-based royalty fee must be paid regardless of what currency, or token was used in the sale, paid in the same currency or token. This applies to sales in any location including on-chain sales, over-the-counter (OTC) sales, and off-chain sales using fiat currency such as at auction houses. As royalty payments are voluntary, entities that respect this EIP must pay no matter where the sale occurred - a sale outside of the blockchain is still a sale.”\n\nThey also plan on taking on the mechanism for paying and notifying the recepient in the future EIPs.3\n\nOur experiment\nAfter trying to implement hardcoding the royalties the same issue with the transfer function occured.\n\n// SPDX-License-Identifier: MIT\npragma solidity ^0.8.13;\n\nimport \"@openzeppelin/contracts@4.5.0/token/ERC721/ERC721.sol\";\nimport \"@openzeppelin/contracts@4.5.0/token/ERC721/extensions/ERC721Enumerable.sol\";\nimport \"@openzeppelin/contracts@4.5.0/access/Ownable.sol\";\nimport \"@openzeppelin/contracts@4.5.0/utils/Counters.sol\";\nimport \"@openzeppelin/contracts@4.5.0/interfaces/IERC2981.sol\";\n\ncontract InstantRoyaltyToken is ERC721, ERC721Enumerable, Ownable, IERC2981 {\n    using Counters for Counters.Counter;\n\n    Counters.Counter private _tokenIdCounter;\n\n    address royaltyAddress;\n    uint256 royalty = 10_000;\n\n    constructor(address _royaltyAddress ) ERC721(\"InstantRoyaltyToken\", \"IRT\") {\n            royaltyAddress = _royaltyAddress;\n    }\n\n    function safeMint(address to) public onlyOwner {\n        uint256 tokenId = _tokenIdCounter.current();\n        _tokenIdCounter.increment();\n        _safeMint(to, tokenId);\n    }\n\n    // The following functions are overrides required by Solidity.\n\n    function _beforeTokenTransfer(address from, address to, uint256 tokenId)\n        internal\n        override(ERC721, ERC721Enumerable)\n    {\n        super._beforeTokenTransfer(from, to, tokenId);\n    }\n\n    function supportsInterface(bytes4 interfaceId)\n        public\n        view\n        override(ERC721, ERC721Enumerable, IERC165)\n        returns (bool)\n    {\n        return interfaceId == type(IERC2981).interfaceId || super.supportsInterface(interfaceId);\n    }\n\n    function royaltyInfo(uint256 tokenId, uint256 salePrice) public view override returns(address receiver, uint256 royaltyAmount) {\n        return (royaltyAddress, royalty);\n    }\n    /* \n\n        * The function below from ERC721 is the main issue. Making this a\n        * payable function would add unecessary complexity to the standard\n        * and would make the function payable, thus requiring payment \n        * even when sales are not ocurring.\n\n    */\n    function transferFrom(\n        address from,\n        address to,\n        uint256 tokenId\n    ) public virtual override(ERC721,IERC721) {\n            require(_isApprovedOrOwner(_msgSender(), tokenId), \"ERC721: transfer caller is not owner nor approved\");\n            //royaltyInfo and pay royalty for transfer part would go here\n            //implementing this would make the function payable\n            _transfer(from, to, tokenId);\n    }\n\n}"
  },
  {
    "objectID": "documents/research/posts/ERFC-39.hugo.html",
    "href": "documents/research/posts/ERFC-39.hugo.html",
    "title": "BLS vs Schnorr vs ECDSA digital signatures",
    "section": "",
    "text": "Bitcoin and Ethereum both use elliptic-curve cryptography for generating keys and signing transactions. The algorithm they both use is called Elliptic Curve Digital Signature Algorithm (ECDSA), which represents a secure way of signing a message (a transaction for example) using Elliptic Curve Cryptography (ECC). This is a comparative analysis of BLS, Schnorr, and ECDSA signatures. The goal is to understand why are Ethereum and Bitcoin migrating from ECDSA, why BLS and Schnorr are superior to ECDSA and what are the key differences between them, how to use these signatures in the development and which Elliptic curves they are using and why."
  },
  {
    "objectID": "documents/research/posts/ERFC-39.hugo.html#quick-recap-ecdsa",
    "href": "documents/research/posts/ERFC-39.hugo.html#quick-recap-ecdsa",
    "title": "BLS vs Schnorr vs ECDSA digital signatures",
    "section": "Quick recap: ECDSA",
    "text": "Quick recap: ECDSA\nModern cryptography is founded on the idea that the key that you use to encrypt your data can be made public while the key that is used to to decrypt your data can be kept private. As such, these systems are known as public-key cryptographic systems.\nECDSA stands for Elliptic Curve Digital Signature Algorithm. Elliptic curve cryptography is a form of public key cryptography which is based on the algebraic structure of elliptic curves over finite fields. Used by both Bitcoin and Ethereum.\nElliptic curve: secp256k1\n\nsecp256k1\nThe elliptic curve domain parameters over Fp associated with a Koblitz curve secp256k1 are specified by the sextuple T = (p, a, b, G, n, h) where the finite field Fp is defined by:\n\np = 0xFFFFFFFF FFFFFFFF FFFFFFFF FFFFFFFF FFFFFFFF FFFFFFFF FFFFFFFE FFFFFC2F = 2^256 − 2^32 − 2^9 − 2^8 − 2^7 − 2^6 − 2^4 − 1\n\nThe curve E: y^2 = x^3 + ax + b over Fp is defined by:\n\na = 0x00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000\nb = 0x00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000007\n\nThe base point G in compressed form is:\n\n0x02 79BE667E F9DCBBAC 55A06295 CE870B07 029BFCDB 2DCE28D9 59F2815B 16F81798\n\nand in uncompressed form is:\n\n0x04 79BE667E F9DCBBAC 55A06295 CE870B07 029BFCDB 2DCE28D9 59F2815B 16F81798 483ADA77 26A3C465 5DA4FBFC 0E1108A8 FD17B448 A6855419 9C47D08F FB10D4B8\n\n\n\n\nsecp256k1 curve\n\n\n\n\nPrivate and public keys\nPrivate keys are generated as random 256 bits or 64 random hex characters or 32 random bytes. The public key is derived from the private key using ECDSA. Public key is a point on secp256k1 elliptic curve, generated by formula K = k * G where K is public key, k is private key, G is the constant point on secp256k1 elliptic curve and * is the multiplication operator on secp256k1 elliptic curve. There is no inverse, “/” operator, therefore the relationship between k and K is fixed, but can only be calculated in one direction, from k to K. A private key can be converted into a public key, but a public key cannot be converted back into a private key, because the math only works one way. The multiplication of k * G is equivalent to repeated addition, so G + G + G + …​ + G, repeated k times.\n\n\nSigning and verification\nTo sign and verify ECDSA signature using OpenSSL, do next\n# Generate private key\nopenssl genpkey -algorithm rsa -out privatni.pem\n\n# Generate public key out of private key\nopenssl rsa -pubout -in privatni.pem -out javni.pem\n\n# Test message for signing\necho \"Test\" > message.txt\n\n# Sign the message (with Bitcoin's hashing alghorithm)\nopenssl dgst -sha256 -sign privatni.pem -out signature.bin message.txt\n\n# Verification\nopenssl dgst -sha256 -verify javni.pem -signature signature.bin message.txt\n\n\n\necdsa signing\n\n\n\n\nECDSA verification in Solidity\nimport \"@openzeppelin/contracts/utils/cryptography/ECDSA.sol\";\n\ncontract Example {\n  address public admin;\n\n  constructor() {\n    admin = msg.sender;\n  }\n\n  function verify(bytes32 _digest, bytes calldata _signature) public view returns(bool) {\n    return admin == ECDSA.recover(_digest, _signature);\n  }\n}\n\n\nECDSA verification in Javascript/Typescript\nimport { SignerWithAddress } from '@nomiclabs/hardhat-ethers/signers';\nimport { utils } from 'ethers';\nimport { expect } from 'chai';\n\n(async () => {\n    let admin: SignerWithAddress;\n\n    [admin] = await ethers.getSigners();\n\n    const message: string = 'Hello World';\n    const msgHash: string = utils.hashMessage(message);\n    const digest: Uint8Array = utils.arrayify(msgHash);\n\n    const signature: string = await admin.signMessage(message);\n\n    const address: string = utils.recoverAddress(digest, signature);\n    expect(address).to.equal(admin.address);\n})();"
  },
  {
    "objectID": "documents/research/posts/ERFC-39.hugo.html#bls",
    "href": "documents/research/posts/ERFC-39.hugo.html#bls",
    "title": "BLS vs Schnorr vs ECDSA digital signatures",
    "section": "BLS",
    "text": "BLS\nBLS stands for Boneh-Lynn-Shacham, it’s a signature scheme that is based on bi-linear pairings. A pairing, defined as e(,), is a bilinear map of 2 groups G1 and G2 in some other group, GT. e(,) takes as arguments points in G1 and G2.\nPairings that verifies a signature looks like this:\ne(g1, sig) = e(P, H(m))\n\n# or in expanded form like this\ne(g1, pk*H(m)) = e(pk*g1, H(m)) = e(g1, pk*H(m))\nH(m) is hashing a message to a point on an elliptic curve.\nBLS consists of:\n\nKeyGen — choose a random α. Given generator g1, P=α*g1\nSign — σ = α*H(m) ∈ G2 (in the case of ETH 2.0)\nVerify(P,m, σ) — if e(g1, σ) = e(P, H(m)) return true.\n\nElliptic curve: BLS12-381\n\nBLS12-381\nBLS12-381 is a pairing-friendly elliptic curve construction that is optimal for zk-SNARKs at the 128-bit security level.\nBarreto-Naehrig (BN) curves are a class of pairing-friendly elliptic curve constructions built over a base field Fp of order r, where r≈p. It is possible to construct a new BN curve that targets 128-bit security by selecting a curve closer to p≈2^(384). However, the larger group order r impairs the performance of multi-exponentiation, fast fourier transforms and other cryptographic operations.\nBarreto-Lynn-Scott (BLS) curves are a slightly older class of pairing-friendly curves which now appear to be more useful for targeting this security level. Current research suggests that with p≈2^(384), and with an embedding degree of 12, these curves target the 128-bit security level.\n\n\nPrivate and public keys\nThe private/secret key (to be used for signing) is just a randomly chosen number between 1 and r−1 inclusive. We’ll call it pk.\nThe corresponding public key is P=[pk]g1, where g1 is the chosen generator of G1. That is, g1 multiplied by pk, which is g1 added to itself pk times.\nThe discrete logarithm problem means that it is unfeasible to recover pk given the public key P.\n\n\nSigning\nOne can sign the message by calculating the signature σ=[pk]H(m). That is, by multiplying the hash point by our secret key. But what is H?\nTo calculate a digital signature over a message, we first need to transform an arbitrary message (byte string) to a point on the G2 curve. The initial implementation in Eth2 was “hash-and-check”:\n\nHash your message to an integer modulo q\nCheck if there is a point on the curve with this x-coordinate. If not, add one and repeat\nOnce you have a point on the curve multiply it by the G2 cofactor to convert it into a point in G2.\n\n\n\n\nbls signing\n\n\n\n\nVerification\nGiven a message m, a signature σ, and a public key P, we want to verify that it was signed with the pk corresponding to P. The signature is valid if, and only if, e(g1,σ)=e(P,H(m)).\n\n\n\nbls verification\n\n\n\n\nAggregation\nA really neat property of BLS signatures is that they can be aggregated, so that we need only two pairings to verify a single message signed by n parties, or n - 1 pairings to verify n different messages signed by n parties, rather than 2n pairings you might naively expect to need. Pairings are expensive to compute, so this is very attractive.\nTo aggregate signatures we just have to add up the G2 points they correspond to: σagg=σ1+σ2+...+σn. We also aggregate the corresponding G1 public key points Pagg=P1+P2+...+Pn.\nNow the magic of pairings means that we can just verify that e(g1,σagg)=e(Pagg,H(m)) to verify all the signatures together with just two pairings.\n\n\nBLS verification in Solidity\nBelow shows an example Solidity function that verifies a single signature. EIP-197 defined a pairing precompile contract at address 0x8 and requires input to a multiple of 192. This assembly code calls the precompile contract at address 0x8 with inputs.\n  // Negated genarator of G2\n  uint256 constant nG2x1 = 11559732032986387107991004021392285783925812861821192530917403151452391805634;\n  uint256 constant nG2x0 = 10857046999023057135944570762232829481370756359578518086990519993285655852781;\n  uint256 constant nG2y1 = 17805874995975841540914202342111839520379459829704422454583296818431106115052;\n  uint256 constant nG2y0 = 13392588948715843804641432497768002650278120570034223513918757245338268106653;\n\n\nfunction verifySingle(\n    uint256[2] memory signature, \\\\ small signature\n    uint256[4] memory pubkey, \\\\ big public key: 96 bytes\n    uint256[2] memory message\n) public view returns (bool) {\n    uint256[12] memory input = [\n        signature[0],\n        signature[1],\n        nG2x1,\n        nG2x0,\n        nG2y1,\n        nG2y0,\n        message[0],\n        message[1],\n        pubkey[1],\n        pubkey[0],\n        pubkey[3],\n        pubkey[2]\n    ];\n    uint256[1] memory out;\n    bool success;\n\n    assembly {\n        success := staticcall(sub(gas(), 2000), 8, input, 384, out, 0x20)\n        switch success\n            case 0 {\n                invalid()\n            }\n    }\n\n    require(success, \"\");\n    return out[0] != 0;\n}\n\n\nBLS verification in Javascript/Typescript\nconst bls = require('@noble/bls12-381');\n\n(async () => {\n    // keys, messages & other inputs can be Uint8Arrays or hex strings\n    const privateKey =\n        '67d53f170b908cabb9eb326c3c337762d59289a8fec79f7bc9254b584b73265c';\n    const message = '64726e3da8';\n    const publicKey = bls.getPublicKey(privateKey);\n    const signature = await bls.sign(message, privateKey);\n    const isValid = await bls.verify(signature, message, publicKey);\n    console.log({ publicKey, signature, isValid });\n\n    // Sign 1 msg with 3 keys\n    const privateKeys = [\n        '18f020b98eb798752a50ed0563b079c125b0db5dd0b1060d1c1b47d4a193e1e4',\n        'ed69a8c50cf8c9836be3b67c7eeff416612d45ba39a5c099d48fa668bf558c9c',\n        '16ae669f3be7a2121e17d0c68c05a8f3d6bef21ec0f2315f1d7aec12484e4cf5',\n    ];\n    const messages = ['d2', '0d98', '05caf3'];\n    const publicKeys = privateKeys.map(bls.getPublicKey);\n    const signatures2 = await Promise.all(\n        privateKeys.map((p) => bls.sign(message, p))\n    );\n    const aggPubKey2 = bls.aggregatePublicKeys(publicKeys);\n    const aggSignature2 = bls.aggregateSignatures(signatures2);\n    const isValid2 = await bls.verify(aggSignature2, message, aggPubKey2);\n    console.log({ signatures2, aggSignature2, isValid2 });\n})();"
  },
  {
    "objectID": "documents/research/posts/ERFC-39.hugo.html#schnorr",
    "href": "documents/research/posts/ERFC-39.hugo.html#schnorr",
    "title": "BLS vs Schnorr vs ECDSA digital signatures",
    "section": "Schnorr",
    "text": "Schnorr\nSchnorr signatures are generated slightly differently than ECDSA. Instead of two scalars (r,s) we use a point R and a scalar s. Similar to ECDSA, R is a random point on elliptic curve (R = k×G). Second part of the signature is calculated slightly differently: s = k + hash(P,R,m) ⋅ pk. Here pk is your private key, P = pk×G is your public key, m is the message. Then one can verify this signature by checking that s×G = R + hash(P,R,m)×P.\n\n\n\nschnorr signing\n\n\nThis equation is linear, so equations can be added and subtracted with each other and still stay valid. This brings us to several nice features of Schnorr signatures that we can use.\n\nBatch validation\nTo verify a block in Bitcoin blockchain we need to make sure that all signatures in the block are valid. If one of them is not valid we don’t care which one - we just reject the whole block and that’s it.\nWith ECDSA every signature has to be verified separately. Meaning that if we have 1000 signatures in the block we will need to compute 1000 inversions and 2000 point multiplications. In total ~3000 heavy operations.\nWith Schnorr signatures we can add up all the signature verification equations and save some computational power. In total for a block with 1000 transactions we need to verify that\n(s1+s2+…+s1000)×G=(R1+…+R1000)+(hash(P1,R1,m1)×P1+ hash(P2,R2,m2)×P2+…+hash(P1000,R1000,m1000)×P1000)\nHere we have a bunch of point additions (almost free in sense of computational power) and 1001 point multiplication. This is already a factor of 3 improvement - we need to compute roughly one heavy operation per signature.\n\n\n\nbatch validation\n\n\n\n\nKey aggregation\nWe want to keep our bitcoins safe, so we might want to use at least two different private keys to control bitcoins. One we will use on a laptop or a phone and another one - on a hardware wallet / cold wallet. So when one of them is compromised we still have control over our bitcoins.\nCurrently it is implemented via 2-of-2 multisig script. This requires two separate signatures to be included in the transaction.\nWith Schnorr signatures we can use a pair of private keys (pk1,pk2) and generate a shared signature corresponding to a shared public key P=P1+P2=pk1×G+pk2×G. To generate this signature we need to choose a random number on every device (k1,k2), generate a random point Ri=ki×G, add them up to calculate a common hash(P,R1+R2,m) and then get s1 and s2 from every device (si = ki + hash(P,R,m) ⋅ pki). Then we can add up these signatures and use a pair (R, s) = (R1+R2, s1+s2) as our signature for shared public key P. No one else won’t be able to say if it is an aggregated signature or not - it looks exactly the same as a normal Schnorr signature."
  },
  {
    "objectID": "documents/research/posts/ERFC-90.hugo.html",
    "href": "documents/research/posts/ERFC-90.hugo.html",
    "title": "Detect NFT Wash Trading",
    "section": "",
    "text": "With the monthly trading volume of the Non-Fungible Token (NFT) marketplace OpenSea reaching 5 billion dollars in January 20221 it is clear that NFTs are gaining popularity and with that grows the importance of having a transparent trading activity.\nWash trading is a form of market manipulation where a single entity or a group of colluding entities buy and sell the same asset with the goal of feeding the marketplace misleading information2. There are at least two possible benefits to wash trading, the first being that a single asset can be wash traded multiple times, continually increasing the price, thus making the asset appear more sought after than it actually is. This chain of wash trades is broken when an unsuspecting victim buys the asset. The second potential benefit is that the trading can be incentivized by the platform, with rewards being tied to the volume traded. Trading rewards can, at least for a limited time period, be higher than the fees, which makes this process worthwhile.\nIn the case of NFTs, wash trading is additionally enabled by the associated user anonymity. One single entity can control a large number of addresses without a way of reliably determining who is behind them. Those addresses, however, need to be somehow funded to make them usable, which leads to a money trail that can be followed to detect connections between them and to flag suspicious NFT trades."
  },
  {
    "objectID": "documents/research/posts/ERFC-90.hugo.html#collecting-and-parsing-of-data",
    "href": "documents/research/posts/ERFC-90.hugo.html#collecting-and-parsing-of-data",
    "title": "Detect NFT Wash Trading",
    "section": "Collecting and Parsing of Data",
    "text": "Collecting and Parsing of Data\nThis research takes into account only trades of NFTs that follow the ERC721 standard. The same principles can be applied to the trades involving the ERC1155 standard, with the only difference being the collection and parsing of trades.\nWhen an NFT trade is executed, the ERC721 compliant contracts emit a Transfer event that contains three fields: previous owner, new owner, and the token id. Using the combination of Etherscan and Alchemy APIs, it is possible to get all the events that were emitted by the transaction and to extract the needed event based on its topic along with all of its fields.\nNot every Transfer event corresponds to a trade, so there needs to be an extra processing step that will eliminate all transfers that were not made through a marketplace. To do this, one needs to go through all of the marketplace’s contract’s transactions and select only those that have the right methodID.\nAfter the seller’s (previous owner’s) and buyer’s (new owner’s) addresses are known, the last step is collecting and parsing of all of their transactions searching for Ether transfers and all the addresses they have interacted with - in this paper referred to as “associates”."
  },
  {
    "objectID": "documents/research/posts/ERFC-90.hugo.html#wash-trading-detectors",
    "href": "documents/research/posts/ERFC-90.hugo.html#wash-trading-detectors",
    "title": "Detect NFT Wash Trading",
    "section": "Wash Trading Detectors",
    "text": "Wash Trading Detectors\nHaving a set of all of the buyer’s and seller’s associates enables the creation of Wash Trading Detectors (WTD). This paper proposes and implements two basic algorithms:\n\nWTD0 that detects a direct transfer by checking if the seller’s address belongs to the set of buyer’s associates\nWTD1* that detects a set of common associates that are Externally Owned Accounts (EOAs)\n\n*WTD1 is incomplete because the detected common associate can be a Centralized Exchange’s (CEX) address which would give a false positive. The only way to make the WTD1 fully functional is to manually keep a list of all the addresses that should be ignored."
  },
  {
    "objectID": "documents/research/posts/ERFC-90.hugo.html#implementation",
    "href": "documents/research/posts/ERFC-90.hugo.html#implementation",
    "title": "Detect NFT Wash Trading",
    "section": "Implementation",
    "text": "Implementation\nUsing the proposed ways of getting the data and reasoning on it, it is possible to extract suspicious wash trading patterns, flag those trades, and perform an analysis of the results. The code shown bellow is capable of getting the count of detected wash trades performed through a marketplace in a given block range :\nimport utils\n\ndef run(contract, methodIds, start_block, end_block):\n    '''Detects potential Wash trades for a marketplace's contract'''\n\n    transactions = utils.get_all_transactions(\n        contract,\n        start_block,\n        end_block\n    )\n\n    wtd0_count, wtd1_count, total = 0, 0, 0\n\n    for  tx in transactions:\n\n        if tx['input'][:10] in methodIds:\n\n            status, logs = utils.get_logs(tx['hash'])\n\n            if status != 1: # Reverted transaction\n                continue\n\n            nft_contract, token_id, A, B = utils.parse_logs(logs)\n\n            if A == None or B == None: # not a standard ERC721\n                continue\n\n            associates_A = utils.get_associates(A)\n            associates_B = utils.get_associates(B)\n\n            wtd0_count += int(\n                utils.wtd0(A, B, associates_A, associates_B)\n            )\n            wtd1_count += int(\n                len(utils.wtd1(A, B, associates_A, associates_B)) > 0\n            )\n\n            total += 1\n\n    return (wtd0_count, wtd1_count, total)\nThe run function consists of getting all transaction data for a marketplace’s contract starting from the start_block up to the end_block and considering only those that are in the provided list of methodIds. These transactions are then parsed, and values are extracted that will be passed to the utils.wtd0 and utils.wtd1 functions which will perform detection.\nFor the full implementation of all of the used helper methods from utils module see Appendix A.\n\nExample\nFor example, let us take the OpenSea’s Wyvern V1 contract and pass three different block ranges. The ['0xab834bab'] argument corresponds to the methodID of the contract’s method that gets called when there is a trade.\nWYVERN_V1 = '0x7Be8076f4EA4A4AD08075C2508e481d6C946D12b'\n\nranges = [\n    [6652089, 6652239],\n    [7486211, 7486311],\n    [7704798, 7704898],\n]\n\nfor start_block, end_block in ranges:\n\n    print(run(WYVERN_V1, ['0xab834bab'], start_block, end_block))\nFrom the total of 23 trades that were made during the provided ranges :\n\nWTD0 flags 8 trades\nWTD1 flags 11** trades\n\n**WTD1 returns a list of associate addresses; these lists were manually checked through Etherscan to see if they belong to a CEX. The list of ignored addresses is available in Appendix B.\n\n\nImplications\nThe sample size of 23 is too small to discuss how the reported numbers relate to all of the NFT trades since the marketplace’s contract deployment. The code itself can, however, serve as a starting point for the development of a service capable of extracting the data from all of the NFT marketplaces since their creation. In that data lies the key to answering not only what trades are a wash trade but also who performed them, how many times was an address linked to a wash trade, whether one NFT has been wash traded multiple times, etc. Such a service would need to effectively manage its resources such as the collection of data and the computation needed in the detection - in the example above, set of associates is always computed from scratch (there is no storing of the result and checking if those values have already been computed). The full specification for the development of this service is out of the scope of this paper and should be a topic of a separate research.\n\nComplex Patterns\nDue to current non-negligible transaction fees on Ethereum and the fact that not many people are deeply looking into each trade, it is unlikely that there are complex patterns present in NFT wash trading. As the fees get lower and as the adoption grows, it’s almost certain that they will emerge. On the Figure 3 is shown one pattern that could be used in the process of wash trading.\n\n\n\n\ncomplex-pattern\n\n\nFigure 3 - Complex pattern\n\nThere are two NFT wash trades present (marked by the black and blue colored arrows). The sequence of transfers is the following:\n\nA finances B and C through 3 and 2 intermediaries, respectively\nB finances D through 3 intermediaries\nD buys an NFT from some non-associated address\nD sells the NFT to C\nD sends the funds to B through the same 3 intermediaries that were used before\nB finances E through 2 intermediaries\nE buys the NFT from C\n\nAfter the last step, E can sell the NFT to an unsuspecting victim. It is important to note that addresses used do not have to be discarded after each wash trade - i.e. B can be used just for routing of the funds. Furthermore, a malevolent entity can inflate the prices of not just a single NFT but for a complete collection, making it look like the collection is very popular, which attracts victims."
  },
  {
    "objectID": "documents/research/posts/ERFC-90.hugo.html#appendix-a",
    "href": "documents/research/posts/ERFC-90.hugo.html#appendix-a",
    "title": "Detect NFT Wash Trading",
    "section": "Appendix A",
    "text": "Appendix A\nImplementation of the utils module.\nimport time, json, requests\nfrom web3 import Web3\n\nconfig = {\n    \"alchemy-url\" : \"\",\n    \"etherscan-api-key\": \"\",\n}\n\nweb3 = Web3(Web3.HTTPProvider(config['alchemy-url']))\n\ndef get_all_transactions(address, start_block = 0, end_block = 19999999):\n    '''Gets all transactions using Etherscan API for the provided address'''\n    transactions = []\n\n    while True:\n        time.sleep(5)\n        result = requests.get(\n            'https://api.etherscan.io/api?module=account&action=txlist' +\n            f'&address={address}' +\n            f'&startblock={start_block}' +\n            f'&endblock={end_block}' +\n            f'&offset={1_000}' +\n            f'&sort={\"asc\"}' +\n            f'apikey={config[\"etherscan-api-key\"]}'\n        ).json()['result']\n\n        transactions += result\n\n        if len(result) < 1_000:\n            break\n\n        start_block = int(result[-1][\"blockNumber\"]) + 1\n\n\n    return transactions\n\ndef is_EOA(address):\n    '''Returns true if the address belongs to an Externally Owned Account'''\n\n    try:\n        _address = Web3.toChecksumAddress(address)\n        return web3.eth.getCode(_address) == b\"\"\n    except:\n        return False\n\ndef get_associates(address):\n    '''Returns a set of all account with which the provided addresses interacted with'''\n\n    transactions = get_all_transactions(address)\n\n    associates = set()\n    for tx in transactions:\n        if tx['from'] != address:\n            associates.update([tx['from']])\n        if tx['to'] != address:\n            associates.update([tx['to']])\n\n    return associates\n\ndef get_logs(tx_hash):\n    '''Gets the logs from the transaction receipt of the tx_hash'''\n\n    tx_receipt = web3.eth.get_transaction_receipt(tx_hash)\n\n    return tx_receipt['status'], tx_receipt['logs']\n\ndef parse_logs(logs):\n    '''Returns the NFT contract's address, token id and addresses involved in the trade'''\n\n    TRANSFER_TOPIC = \"0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef\"\n    WRAPPED_ETH = \"0xC02aaA39b223FE8D0A0e5C4F27eAD9083C756Cc2\"\n\n    nft_contracts, token_ids, _from, _to = [], [], None, None\n\n    for ev in logs:\n\n        if TRANSFER_TOPIC == ev[\"topics\"][0].hex() and ev[\"address\"] != WRAPPED_ETH:\n\n            nft_contracts.append(ev[\"address\"])\n\n            bytecode = \"\".join([x.hex() for x in ev[\"topics\"]]) + \"\".join(ev[\"data\"])\n            _from = \"0x\" + bytecode[66 : 66 * 2][-40:]\n            _to = \"0x\" + bytecode[66 * 2 : 66 * 3][-40:]\n            token_ids.append(int(bytecode[66 * 3 : 66 * 4][2:66], base=16))\n\n    return nft_contracts, token_ids, _from, _to\n\n\ndef wtd0(A, B, associates_A, associates_B):\n    '''WTD0 implementation'''\n\n    return (A in associates_B) or (B in associates_A)\n\ndef wtd1(A, B, associates_A, associates_B):\n    '''WTD1 implementation'''\n\n    EOA_associates = []\n\n    common_associates = associates_A.intersection(associates_B)\n    for ca in common_associates:\n        if is_EOA(ca):\n            EOA_associates.append(ca)\n\n    return EOA_associates"
  },
  {
    "objectID": "documents/research/posts/ERFC-90.hugo.html#appendix-b",
    "href": "documents/research/posts/ERFC-90.hugo.html#appendix-b",
    "title": "Detect NFT Wash Trading",
    "section": "Appendix B",
    "text": "Appendix B\nFollowing is the list of all the addresses that were ignored by WTD1 due to the fact that they belong to CEXs\n\n\n\n\nAddress\nCEX\n\n\n\n\n0x564286362092d8e7936f0549571a803b203aaced\nBinance3\n\n\n0x59a5208b32e627891c389ebafc644145224006e8\nHitBTC2\n\n\n0x56eddb7aa87536c09ccc2793473599fd21a8b17f\nBinance17\n\n\n0xeb2629a2734e272bcc07bda959863f316f4bd4cf\nCoinbase6\n\n\n0xd551234ae421e3bcba99a0da6d736074f22192ff\nBinance2\n\n\n0xb5d85cbf7cb3ee0d56b3bb207d5fc4b82f43f511\nCoinbase5\n\n\n0x0681d8db095565fe8a346fa0277bffde9c0edbbf\nBinance4\n\n\n0x3f5ce5fbfe3e9af3971dd833d26ba9b5c936f0be\nBinance"
  },
  {
    "objectID": "documents/research/posts/ERFC-42.hugo.html",
    "href": "documents/research/posts/ERFC-42.hugo.html",
    "title": "Analyze of smart contract fuzzers",
    "section": "",
    "text": "Introduction\nSmart contracts often contain valuable assets, whether in the form of tokens or Ether. Smart contract’s source code is publicly available, every execution happens on a public network. If smart contracts have vulnerabilities that can lead to catastrophic damages, that is potentially measured in millions of dollars. For example, at 2017 attack on Parity Wallet cost ~30 million dollars1 , at the 2016 DAO Hack cost ~150 million dollars2 , at 2020 Harvest Finance was attacked using flash loans and stole ~30 million dollars3 . To prevent disasters like this, it is important to find any vulnerabilities in smart contracts before deployments. Some of the common vulnerabilities are integer overflow/underflow, race conditions, and also we could have logic mistakes that are hard to detect. Security is essential while developing smart contracts. There are some known hacker attacks and good practices to follow.4\nHence, in the development of smart contracts, testing is one of the most important techniques that require special time aside. Mostly we write unit tests, but unit tests are specific to one use case, and often some edge cases are not covered. There is a big research interest in developing testing tools, especially ones that are able to automatically detect as many problems in the code as possible, one such technique is fuzz testing.\n\n\nGoals & Methodology\nThere are many automatic bug-finding tools, and the purpose of this research is to introduce you to smart contracts fuzz testing. Fuzzing is a well-known technique in the security community it generates random, or invalid data as inputs to reveal bugs in the program, in one word it stress the program and causes unexpected behavior or crashes.\nConsidering the research interest in this area, a number of tools have been developed. Some popular open source tools are ContractFuzzer, ContraMaster, ILF, sFuzz, Smartian, and Echidna. Comparison of these tools is not easy task, the main reason is that each tool covers some specific set of bug classes.\nEchidna is a property-based tool, develop with the aim to check if the contract violates some user-defined invariants, while other tool tries to find crashes. Company Trails of Bits extended fuzz technique to the EVM by developing the Echidna tool. Echidna can test both Solidity and Vyper smart contracts, it is written in Haskell, and main design goals are:\n\nEasy to use and configure\nGood contract coverage\nFast and quickly results\n\nContraMaster have been developed to detect irregular transactions due to various types of adversarial exploits, detects 3 classes of bug: Reentrancy, Exception Disorder, Gasless Send and Integer overflow/underflow.\nContractFuzzer detects 7 classes of bug: Reentrancy, Exception Disorder, Gasless Send, Timestamp Dependency, Block Number Dependency, DelegateCall and Freezing Ether Contract.\nsFuzz detects all classes of bug as Contract Fuzzer plus Integer overflow/underflow. It has an extendable architecture which allows to easily support new bug classes as well. Also, sFuzz is effective in achieving high code coverage\nSmartian detects 13 classes of bug: Assertion Failure, Arbitrary Write Block state Dependency, Control Hijack, Ether Leak, Integer Bug, Mishandled Exception, Multiple Send, Reentrancy, Suicidal Contract, Transaction Origin Use, Freezing Ether, Requirement Violation.\nThe way of generating inputs is different, ContractFuzzer and Echidna generate test cases based on a set of predefined parameter values, and fail to cover deeper paths that expose some vulnerabilities. sFuzz has guided input generation based on a genetic algorithm to iteratively improve its branch coverage. ILF generates input based on AI, using neural networks.\nAll in all, only Smartian, ILF, and Echidna at the end show the path how we could reproduce the bug. As Smartian covers more bug classes than ILF, shown in bellow Figure5 , the focus in this research will be on Smartian and Echidna.\n\n\n\nResults & Discussion\nAll examples are run on MacOS Big Sur, version 11.6, processor 2,6 GHz 6-Core Intel Core i7 and 16GB of memory. As there is no upper bound on how long Echidna can run, but the goal is to find a bug in up to 5 minutes.6 Configuration for Smartian test timeout is set up to 5 minutes.\nLet’s first show and discuss few motivating smart contract examples:\ncontract MotivationExample {\n    function f(int256 a, int256 b, int256 c) public pure returns (int256) {\n        int256 d = b + c;\n        if (d < 1) {\n            if (b < 3) {\n                return 1;\n            }\n            if (a == 42) {\n                assert(false);\n                return 2;\n            }\n            return 3;\n        } else {\n            if (c < 42) {\n                return 4;\n            }\n            return 5;\n        }\n    }\n}\nVery fast both Smartian and Echidna find assertion failure in above smart contract, results with counterexample and information how to reproduce transaction are show in next Figures(Figure1 and Figure2):\n\nFigure1 : Smartian replayable test case\n\nFigure2 : Echidna replayable test case\nThe next two examples have some more complex math:\ncontract MotivationExample {\n    bool private value_found;\n\n    function f(uint256 a, uint256 b, uint256 c, uint256 d) public {\n        require(a == 42);\n        require(b == 129);\n        require(c == d+333);\n        value_found = true;\n        assert(value_found == false);\n    }\n}\nAbove one, the inputs must meet three requirements, and to the equality. Smartian and Echidna test it, and at bellow Figures(Figure3 and Figure4) are results:\n\nFigure3 : Smartian replayable test case\n\nFigure4 : Echidna failed to find assertion\nSmartian quickly found assertion failure and counterexample, while Echidna failed to find one. Take a look at hardest motivation example and result from fuzzers:\ncontract MotivationExample {\n    uint256 private stateA;\n    uint256 private stateB;\n    uint256 CONST = 32;\n\n    function f(uint256 x) public {\n      stateA = x;\n    }\n\n    function g(uint256 y) public{\n      if (stateA % CONST == 1) {\n        stateB = y - 10;\n      }\n    }\n\n    function h() public view {\n      if (stateB == 62) { \n        bug(); \n      }\n    }\n\n    function bug() private pure {\n      assert(false);\n    }\n}\n\nFigure5 : Smartian replayable test case\n\nFigure6 : Echidna failed to find assertion\nAgain, Smartian quickly finds assertion failure, while Echidna fails. The reason for failure is due to the way it generates inputs. Echidna is not smart to go in-depth when making input seeds and figure out the values in the deeper branches.\nIndeed, there is a way for Echidna to find assertion failure in the above examples, solution is in the configuration file.\nEchidna has a YAML configuration file, with configurable parameters, that can be turned on or off during the test. If config.yaml is not listed, the default YAML configuration file is called.7 Some of configuration parameters enable to blacklist function, compute maximum gas usage, the maximum number of transactions sequences to generate, number of test sequences to run, prefix for boolean functions that are properties to be checked, contract deployer address. Also it is possible to define set of addresses transactions originate from along with default balance for addresses. In case we have a complex contract, and we need to initialize the blockchain with some data -> tool Etheno helps here8 , after Etheno finishes the initialization JSON file is created, that is set as initialization inside configuration file. Additionally, if our contract uses some framework, for example, Hardhat or Truffle, Echidna then use crytic compile, and build directory of the framework is sent through crytic arguments inside Echidna configuration file.\nBack to the above example, to find assertion failure, it is enough inside configuration file to set corpus directory. After first run, inside the corpus directory we could see the generated input for contract properties, now is enough to modify the input to use suitable parameters that will cause assertion failure.\nAlthough Smartian beat Echidna in the above examples, the logical question, that arises, is what are the advantages of Echidna and why would we use Echidna rather than Smartian?\nEchidna’s advantage are invariants, Invariants are Solidity functions that can represent any incorrect state that contract can have, each invariant must be:\n\nPublic method that has no argument\nReturn true if it is successful\n\nor:\n\nPublic method that can have an argument\nUse assert in function\n\n\nFigure7 : Architecture of Echdina\nArchitecture9 is divided into preprocessing and fuzzing campaigns. In the preprocessing step, the static analyzer tool Slither10 is used with the purpose to find useful constants and functions for effective testing. In the fuzzing campaign step, using contract ABI(Application binary interface) the random transactions are generated, and also any previous transactions from the corpus are included. In case the vulnerability is detected, a counterexample is automatically minimized to the smallest and simplest sequence of transactions that cause failure.\nRunning Echidna:\n$ echidna-test contract.sol --constract TEST --config config.yaml,\nor if Truffle or Hardhat is used:\n$ echidna-test . contract.sol --constract TEST --config config.yaml\nEchidna can be run from the docker, the official image of the current 2.0.0 version is trailofbits/echidna. Inside docker, default version of solidity compiler is 0.5.7, so if we want to test contracts in another version we need to install solc-select. If the preferred method is to not worry about how to install additional tools, there is a docker image trailofbits/eth-security-toolbox, but currently there ecidna version is 1.7.2.\n\n\nConclusion\nAlthough Smartian is better at generating input, what should be noted is that Smartian still doesn’t have support for solc 0.8.x or greater. All examples from the section Results & Discussion have been tested with solc 0.4.25. For an experiment, if you take IB.sol from Smartian benchmark examples,11 and adapt it to work with solidity version 0.8.9, Smartian will fail to find Integer Bug, but Echidna will find it. If you take a look at comparison of fuzz tools in Goals & Methodology that in the previous version Echidna was not able to found these bugs, as the integer overflow/underflow is one of the features in Echidna 2.0.0 for solc 0.8.x or greater. In Appendix at Figure11 and Figure12 is shown Smartian output, and in Figure13 is shown Echidna output.\nIn general, Echidna and Smartian together cover bug classes: Assertion Failure and Integer Bug. Some comparison examples between Echidna 2.0.0, Smartian solc 0.4.25 and Smartian solc 0.8.9 are in research_examples.\nEchidna Assertion allows us to manage what property should test along with the input range value of testing property arguments, in contrary using explicit property we are not sure which function will be checked and which arguments should be used to call test property, explicit property check all method that is not private or internal.\nWhen asked which tool of the two to use, the simple answer is both. Both tools are promising. They cover different classes of problems and the use of both tools for testing smart contracts reduces the chance that our contract has potential flaws. Echidna is under active development, so it is reasonable that it is buggy. But, they work hard to reply & fix each issue.\nThe Echidna coverage report is a little bit confused, one is shown at Figure8. Some suggestion is to add some styled report(example) and the legend table of symbol meaning:\n\n\n\n\n\n\n\nLine marker\nMeaning\n\n\n\n\n‘*’\nIf an execution ended with a STOP\n\n\nr\nIf an execution ended with a REVERT\n\n\no\nIf an execution ended with an out-of-gas error\n\n\ne\nIf an execution ended with any other error (zero division, assertion failure, etc)\n\n\n\n\nFigure8 : Echidna coverage\nUI for both tools should be improved, it would be nice to display emitted events. And, Smartian output could color counterexample and found bugs. The current UI is shown in the bellow Figures.\nFigure9 : Smartian output\n\nFigure10 : Echidna output\nIt would be nice to have integration with Remix IDE, which will help with debugging.\n\n\nAppendices\n\nFigure11 : Smartian found Integer Bug with 0.4.25\n\nFigure12 : Smartian not found Integer Bug with 0.8.9\n\nFigure13 : Echidna found Integer Bug with 0.8.9\n\n\nBibliography\n\n\n‘Building-Secure-Contracts/Workflow.md at Master \\(\\cdot\\) Crytic/Building-Secure-Contracts’, GitHub <https://github.com/crytic/building-secure-contracts> [accessed 14 February 2022]\n\n\nChoi, Jaeseung, Doyeon Kim, Soomin Kim, Gustavo Grieco, Alex Groce, and Sang Kil Cha, ‘SMARTIAN: Enhancing Smart Contract Fuzzing with Static and Dynamic Data-Flow Analyses’, in 2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE) (Melbourne, Australia: IEEE, 2021), pp. 227–39 <https://doi.org/10.1109/ASE51524.2021.9678888>\n\n\n‘Crytic/Echidna: Ethereum Smart Contract Fuzzer’ <https://github.com/crytic/echidna> [accessed 28 February 2022]\n\n\n‘Etheno’ (Crytic, 2022) <https://github.com/crytic/etheno> [accessed 28 February 2022]\n\n\nFoxley, William, ‘Harvest Finance: $24m Attack Triggers $570m ’Bank Run’ in Latest DeFi Exploit’, 2020 <https://www.coindesk.com/tech/2020/10/26/harvest-finance-24m-attack-triggers-570m-bank-run-in-latest-defi-exploit/> [accessed 11 February 2022]\n\n\nGrieco, Gustavo, Will Song, Artur Cygan, Josselin Feist, and Alex Groce, ‘Echidna: Effective, Usable, and Fast Fuzzing for Smart Contracts’, in ISSTA 2020, 2020\n\n\nSiegel, David, ‘Understanding The DAO Attack’, 2016 <https://www.coindesk.com/learn/2016/06/25/understanding-the-dao-attack/> [accessed 11 February 2022]\n\n\n‘Slither, the Solidity Source Analyzer’ (Crytic, 2022) <https://github.com/crytic/slither> [accessed 11 February 2022]\n\n\n‘Smartian’ (SoftSec Lab, 2022) <https://github.com/SoftSec-KAIST/Smartian> [accessed 28 February 2022]\n\n\n‘The Parity Wallet Hack Explained’, OpenZeppelin Blog, 2017 <https://blog.openzeppelin.com/on-the-parity-wallet-multisig-hack-405a8c12e8f7/> [accessed 11 February 2022]\n\n\n\n\n\n\n\nFootnotes\n\n\n‘The Parity Wallet Hack Explained’, OpenZeppelin Blog, 2017 <<https://blog.openzeppelin.com/on-the-parity-wallet-multisig-hack-405a8c12e8f7/>> [accessed 11 February 2022].↩︎\nDavid Siegel, ‘Understanding The DAO Attack’, 2016 <<https://www.coindesk.com/learn/2016/06/25/understanding-the-dao-attack/>> [accessed 11 February 2022].↩︎\nWilliam Foxley, ‘Harvest Finance: $24m Attack Triggers $570m ’Bank Run’ in Latest DeFi Exploit’, 2020 <<https://www.coindesk.com/tech/2020/10/26/harvest-finance-24m-attack-triggers-570m-bank-run-in-latest-defi-exploit/>> [accessed 11 February 2022].↩︎\n‘Building-Secure-Contracts/Workflow.md at Master \\(\\cdot\\) Crytic/Building-Secure-Contracts’, GitHub <<https://github.com/crytic/building-secure-contracts>> [accessed 14 February 2022].↩︎\nJaeseung Choi and others, ‘SMARTIAN: Enhancing Smart Contract Fuzzing with Static and Dynamic Data-Flow Analyses’, in 2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE) (Melbourne, Australia: IEEE, 2021), pp. 227–39 <https://doi.org/[10.1109/ASE51524.2021.9678888](https://doi.org/10.1109/ASE51524.2021.9678888)>.↩︎\nGustavo Grieco and others, ‘Echidna: Effective, Usable, and Fast Fuzzing for Smart Contracts’, in ISSTA 2020, 2020.↩︎\n‘Crytic/Echidna: Ethereum Smart Contract Fuzzer’ <<https://github.com/crytic/echidna>> [accessed 28 February 2022].↩︎\n‘Etheno’ (Crytic, 2022) <<https://github.com/crytic/etheno>> [accessed 28 February 2022].↩︎\nGrieco and others.↩︎\n‘Slither, the Solidity Source Analyzer’ (Crytic, 2022) <<https://github.com/crytic/slither>> [accessed 11 February 2022].↩︎\n‘Smartian’ (SoftSec Lab, 2022) <<https://github.com/SoftSec-KAIST/Smartian>> [accessed 28 February 2022].↩︎"
  },
  {
    "objectID": "documents/research/posts/ERFC-315.hugo.html",
    "href": "documents/research/posts/ERFC-315.hugo.html",
    "title": "[ERFC - 315] Reaching Towards Realtime in Blockchain",
    "section": "",
    "text": "The concept of Real-Time can be best explained as a set of guarantees that are given and that need to be met within a predetermined time period. Usually, though not necessarily, this is a short time period. In the case the guarantees are not met before the deadline has been reached, then the performance of the real-time system either degrades or the system has failed completely.\nFor a blockchain to achieve real-time operations, it has to have low latency and indirectly a high throughput, hopefully without sacrificing decentralization and security.\nThis is a problem of scaling a blockchain, and several approaches exist, each with its advantages and limitations. These approaches can be grouped into on-chain and off-chain scaling solutions, with the main difference being whether there exist a need for changing the blockchain’s protocol in some way.\nThe main focus of this research is put on the off-chain scaling in the Ethereum Ecosystem and how can a Decentralized Application (DApp) potentially achieve something that resembles real-time operations.\nThe research proposes a framework consisting of an off-chain and on-chain part where the on-chain part would be used to enforce rules that the off-chain code needs to honor.\nThe ideas proposed here would need to be expanded upon and thoroughly tested in real-world conditions to completely assess their practical significance.\nFurther research should also take into consideration on-chain scaling, namely ETH2.0, as well as an exciting concept of creating a network of private sovereign blockchains built specifically for the DApp’s needs."
  },
  {
    "objectID": "documents/research/posts/ERFC-315.hugo.html#ethereums-layer-1",
    "href": "documents/research/posts/ERFC-315.hugo.html#ethereums-layer-1",
    "title": "[ERFC - 315] Reaching Towards Realtime in Blockchain",
    "section": "Ethereum’s Layer 1",
    "text": "Ethereum’s Layer 1\nEthereum blockchain, as defined in,1 can be viewed as a “transaction-based state machine” that groups transactions into blocks which are then sequentially executed in the order that was set by the block’s creator - miner.\nBefore being included inside a block, the transactions reside in the pending transaction pool. Miners have complete control over what transactions get included and in what order. Each transaction advances the chain’s state for which the miners are rewarded fees paid by the Externally-Owned-Account(EOA)* that initiated the transaction.\nIn essence, the more the EOA is willing to pay for the service of including their transaction, the more likely it is will be included in the next block. This creates the problem in the situation of blockchain’s network congestion - as the transactions are being created at a faster rate than they can be processed, fees drastically increase, and it becomes a competition between different EOAs, which leads to a bad user experience.\nTo ease the load put on the Ethereum Mainnet (Layer 1 - L1) chain and effectively perform scaling, multiple approaches exist: Sidechains and schemes commonly referred to as Layer 2 (L2) solutions. L2 solutions include State Channels, Plasma, and Rollups (Optimistic and Zero-Knowledge based).\n* Ethereum account can be defined as a private-public keypair mapped to an address. EOA is a type of an Ethereum account where the private key is known and “externally” controlled, in contrast to smart contract accounts where the private key is not known, and only the address exists."
  },
  {
    "objectID": "documents/research/posts/ERFC-315.hugo.html#off-chain-scaling-approaches",
    "href": "documents/research/posts/ERFC-315.hugo.html#off-chain-scaling-approaches",
    "title": "[ERFC - 315] Reaching Towards Realtime in Blockchain",
    "section": "Off-chain Scaling Approaches",
    "text": "Off-chain Scaling Approaches\nWhen considering approaches to Ethereum’s scaling problem, this paper considers only Ethereum Virtual Machine (EVM) compatible sidechains and optimistic rollups.\n\nSidechains\nSidechains are completely independent parallel blockchains to the Ethereum’s L1, with which they can communicate over two-way bridges. They introduce their own set of parameters and operate on different rules.\nFor example, Polygon, a popular Ethereum sidechain, uses Proof-of-Stake for its consensus mechanism, with block times being around 2.3 seconds2 (Ethereum’s average block time is between 12 and 14 seconds)3\n\n\nOptimistic Rollups\nUnlike Sidechains, Optimistic Rollups have their security rooted in the L1 chain.\nAs Vitalik Buterin, the founder of Ethereum discusses in4 :\n\n“Instead of putting all activity on the blockchain directly, users perform the bulk of their activity off-chain in a”layer 2” protocol. There is a smart contract on-chain, which only has two tasks: processing deposits and withdrawals and verifying proofs that everything happening off-chain is following the rules. There are multiple ways to do these proofs, but they all share the property that verifying the proofs on-chain is much cheaper than doing the original computation off-chain.”\n\nIn Optimistic Rollups, after a state is proposed, there exists a period where it can be disputed, after which it cannot be longer challenged.\nWhile anyone can propose a state, one of the optimistic rollups - Arbitrum One chain has used a concept of a “sequencer”5:\n\nThe sequencer is a specially designated full node, which is given limited power to control the ordering of transactions. This allows the sequencer to guarantee the results of user transactions immediately, without needing to wait for anything to happen on Ethereum. So no need to wait five minutes or so for block confirmations–and no need to even wait 15 seconds for Ethereum to make a block.\n\nThe company behind Arbitrum has recently introduced another L2 concept chain - AnyTrust Chains6, which is operated by a “committee” of nodes signing the state that will later be put on-chain. If nodes go offline or refuse to cooperate, the chain reverts to a standard protocol."
  },
  {
    "objectID": "documents/research/posts/ERFC-315.hugo.html#bridges-and-cross-chain-applications",
    "href": "documents/research/posts/ERFC-315.hugo.html#bridges-and-cross-chain-applications",
    "title": "[ERFC - 315] Reaching Towards Realtime in Blockchain",
    "section": "Bridges and Cross-chain Applications",
    "text": "Bridges and Cross-chain Applications\nTo enable cross-chain communication, there exist Bridges with the purpose of transferring assets and passing messages. The benefit of bridges is that users can move to a different chain if their needs require it. For example, when either transaction cost is too high or when there is more of a specific user activity going on the target chain - i.e., Non-Fungible Token (NFT) trading.\nBridges operate between two extremes - trusted (there is a centralized entity that is being trusted) and trustless (there is no need for the centralized entity).7\nThe security of bridges, however, remains troublesome. As Buterin has pointed out in,8 in the case of a 51% attack of the chain from which funds are being moved, attacker can broadcast a transaction that deposits some funds to the target chain and then revert it, as soon the assets are minted on the target chain. He states that:\n\n“… while I am optimistic about a multi-chain blockchain ecosystem (there really are a few separate communities with different values and it’s better for them to live separately than all fight over influence on the same thing), I am pessimistic about cross-chain applications.”\n\n\nApplication-specific Blockchains\nInstead of having all applications competing for the same resources, a DApp can run its own blockchain. There can exist inter-communication between those private and public chains through bridges.\nEach chain operates with a unique set of rules and validators - making them sovereign but less decentralized."
  },
  {
    "objectID": "documents/research/posts/ERFC-315.hugo.html#concept-of-time-in-blockchains",
    "href": "documents/research/posts/ERFC-315.hugo.html#concept-of-time-in-blockchains",
    "title": "[ERFC - 315] Reaching Towards Realtime in Blockchain",
    "section": "Concept of Time in Blockchains",
    "text": "Concept of Time in Blockchains\nBlockchains operate in discrete time as the state is advanced only when a block is created and validated. It is worth noting that Ethereum’s Yellow Paper[2] states only that the current block’s timestamp should be strictly greater than its predecessor’s timestamp - not what’s the maximum difference between those two values.\nThis leads to blocks being produced at non-constant time intervals and the concept of time being distorted. When periods and deadlines are mentioned in this paper, it is meant in terms of block numbers."
  },
  {
    "objectID": "documents/research/posts/ERFC-315.hugo.html#on-chain-code",
    "href": "documents/research/posts/ERFC-315.hugo.html#on-chain-code",
    "title": "[ERFC - 315] Reaching Towards Realtime in Blockchain",
    "section": "On-chain code",
    "text": "On-chain code\n\nParallezization of Processes\nRather than separating DApp’s functionality into different contract methods with only one method being called per transaction, it is possible to have one method capable of executing different sequences of method calls and accomplishing more.\nIn the example shown below, there exist three separate processes(P0, P1, P2) that perform operations on the same set of data. Processes P0 and P1 are independent of each other and have no other dependencies (if EVM allowed concurrency, they could be executed in parallel). P2 is, however dependent on P0 and so before calling P2 we need to call P0.\ncontract Example_0 {\n\n    ...\n\n    function execute (uint[] operation, uint[] data_0) public onlyAdmin returns (bool) {\n\n        for (uint i = 0; i < operation.length; ++i){\n\n            if (operation[i] == 0){\n\n                //P0 : calculate the sum of numbers in `data_0` and put in the `sum` storage variable\n\n            } else if (operation[i] == 1){\n\n                //P1 : store the entire `data_0` into the storage variable `buffer`\n\n            } else if (operation[i] == 2){\n\n                //P2 : if the `sum` even returns 'true'\n\n            }\n        }\n        return false;\n    }\n\n    ...\n}\nThis setup allows us to create “programmable” sequences of operations and a higher degree of freedom. We can call the execute function with its operation argument set to [0, 2], [1], [0, 2, 1], [1, 0, 2] or [0, 1, 2]. We could also execute a single process multiple times by repeating a number corresponding to the process, though in the example above, that would not make much sense.\n\n\nPersistence across Transactions\nIn the previous example, everything happens in the context of one single transaction, but it doesn’t have to. There can exist another process - P3 that would read from the buffer variable and perform a different operation on its data. Now the order of operations becomes even more important. Because P3 reads from the buffer, we could first call P1 that stores the data_0 into the buffer and then P3 or first call P3 and after P1, which would exhibit a completely different behavior.\n\n\nPrioritization of Processes\nIt is obvious that there are limitations to this approach - transaction size and the transaction cost. This is where prioritization comes in, as not all processes are created equal - some need to be executed more frequently in a shorter period of time while others are less important and can have higher latency. It is up to the DApp’s off-chain code to monitor, decide and optimize for the right moment when a process should be executed.\n\n\nProviding Guarantees: Committing to Promises\nProcesses can depend upon each other; that is, they depend on the result(state) produced by previously executed processes. As there is so much that can fit into a single transaction, DApp doesn’t have to execute a process and produce a result right away - it can make a claim about that result and simultaneously commit to the promise that it will justify the claim later.\n\n\nHonoring Commitments\nCommitments made obviously need to be honored. Otherwise, there is no point. If the deadlines are not met, DApp should be penalized, and the users affected should be compensated.\nHowever, there is no direct reason to halt the execution of the processes - they can operate on claims. The issue now becomes that they effectively act on promises - if it is found later on that the claim has not been honored, the processes and all of their results should be affected as well. An example of this dependency is shown in Figure 1.\n\n\nFigure 1: Operating on Claims\n\nFor simplicity, states (S0, S1, S2, S3, S4) are just uint variables, while P0 and P1 are addition and multiplication operations, respectively. The yellow color indicates that a claim has been made for that variable, while green says that the claim has been honored. We see that claiming S0 affects S2 and S4.\nIt gets even more complicated when a process operates on and produces an update to the same variable. For instance, in Figure 1, if the S2 is fixed to always be equal to S0. The on-chain code would need to keep track of what is affected if the specific claim will not be honored.\nHow to effectively implement mechanisms that would take into account all of these concerns should be part of separate research."
  },
  {
    "objectID": "documents/research/posts/ERFC-315.hugo.html#off-chain-code",
    "href": "documents/research/posts/ERFC-315.hugo.html#off-chain-code",
    "title": "[ERFC - 315] Reaching Towards Realtime in Blockchain",
    "section": "Off-chain code",
    "text": "Off-chain code\nOff-chain code’s responsibility can be summed up to:\n\nmonitor the state of the contract and all of the commitments that need to be honored\ndecide what to put inside the transaction by predicting when is the best time to execute a process\n\n\nMonitoring the on-chain activity\nIn the case where only the admin account can alter the contract’s state, off-chain code can maintain a separate record offline that will periodically be checked with the actual state read from the chain. If it is possible for users to interact directly with the contract, then the chain needs to be read more often - one approach would be to parse the blocks as soon as they are validated.\n\n\nOptimization of action sequences\nDeciding on how to structure a transaction is a more complex problem, and its main part resides in the scheduling of processes. When should a specific process be scheduled depends on what claims need to be honored and when, as well as the state of the network. If the demand for the network’s resources is high, the execution cost is high as well, so it is better to issue a transaction that will use a lot of resources when the demand becomes lower. However, there is no formal guarantee when a transaction will be mined; there would need to exist models that would try to predict and speculate how the demand will change over time and how much to incentivize the miners.\n\nBursts of transcatiions\nAnother important concept is that an EOA can create multiple transactions that can be included in the same block. The miner needs to honor only the order in which they are issued.9 This can be used to enable “bursts” of transactions in order to catch up with the claims that are pending. If there are multiple accounts interacting with the contract, then the order is not guaranteed so there would need to exist some form of synchronization across transactions."
  },
  {
    "objectID": "documents/research/posts/ERFC-146.hugo.html",
    "href": "documents/research/posts/ERFC-146.hugo.html",
    "title": "Data Analytics in Blockchain",
    "section": "",
    "text": "Data Analytics is the science of analyzing raw data related to a specific problem and extracting all of the necessary information in order to make conclusions about as well as derive approaches for solving it.\nIn the context of Blockchain, Data Analytics revolves around the process of collecting and parsing of raw transaction data thus transforming it into usable and actionable data. Parsing of those transactions requires knowledge about the chain specifics as well as internal workings of Smart Contract that that are of interest which is an extremely time consuming process - all of the data on the Blockchain, while it may be public and unchangable, is unstructured.\nBlockchain data, however, holds all of the chain’s history since its inception, making it possible to see past interactions between addresses and/or Smart Contracts. This data can then be segmented to include only a wanted subset for which the analysis will be performed. With this aggregated information it is then possible to gain insight in the past trends for a set of Non-Fungible-Tokens(NFTs) and Decentralized Finance(DeFi) related applications as well as general Crypto related trends and potentially predict future ones.\nData Analytics platforms in the Blockchain space are gaining users as the whole crypto ecosystem evolves and is becoming harder to navigate. With substantial recent investments in this area,12 it is clear that the investors are showing interest in what these platforms can do and the potential they have in shaping the future of Blockchain."
  },
  {
    "objectID": "documents/research/posts/ERFC-146.hugo.html#general-purpose",
    "href": "documents/research/posts/ERFC-146.hugo.html#general-purpose",
    "title": "Data Analytics in Blockchain",
    "section": "General Purpose",
    "text": "General Purpose\n\nTools\nTools in this category query information about the complete transaction activity of a specific address (both Externally-Owned-Accounts (EOA) and Smart Contracts) and groups the extracted information into human readable metrics.\n\nContract interactions\nFor any choosen contract it is possible to extract general information about the:\n\ntransaction count\nunique addresses\ntoken inflow/outflow\n\nfor a specific timeframe. These values can then be organized and monitored over a larger time periods to provide information about the latest trends and changes in the number of users, who these users are, etc.\nThey can also be used to detect contracts that were recently deployed that are gaining popularity as to investigate the project with which those contracts are associated with.\nMore valuable information would be tied to how the contract is being used - what methods are being called, their sequence, etc. To extract meaningfull data, as it was discussed above, there would need to exist a parser with a specific domain knowledge.\n\n\nAddress Profiler\nFor any user address of interest, it is possible to extract the information about the :\n\nportfolio (all of the assets that the address holds)\nestimated portfolio value (sum of values of NFT* and ERC20 holdings)\nrecent token trades (both ERC20 and NFTs)\naddresses that the user has interacted with\n\nAll of these values can also be monitored since the beginning of the chain’s history and addresses can be grouped together to provide some form of live feed for those that are most interesting either to the user or to the platform itself.\n*Estimated value of an NFT is platform specific (i.e. see3)\n\n\nAlerts\nAlerts are delivered to the user, via a communication channel of choice (Telegram, Text Message, Discord, …), when a certain customly defined condition is met - some address buys an NFT, collection’s floor price has increased/decreased by some margin, …\n\n\n\nPlatforms\nTwo of the top plaftforms in this category are Nansen and Dune Analytics which can be used to gather and analyze similar information but take two drastically different approaches - user oriented and business oriented, respectively.\n\nNansen.ai\nThis paid platform doesn’t require or demand from user to have technical knowledge and it provides detailed non-customizable dashboards for General purpose, NFT and DeFi specific tools. Almost all of the tools (from the three categories) listed in this paper, in one form or another, are supported by Nansen making it the most comprehensive and beginner friendly platform.\nAn interesting additional feature that Nansen provides is labeling of some addresses as being “Smart Money” (addresses that were early adopters and/or have made smart decisions in the NFT and/or DeFi space)*. There exist specific dashboards/sections where it is shown what the “Smart Money” is doing - what are they minting/buying/selling, with whom they are interacting, etc. This information can be used by the user to decide what they think is a good strategy for them when investing and can be combined with custom alerts when a certain condition is met.\n*See4 for more details on the labeling of these addresses.\n\n\nDuneAnalytics.com\nDune Analytics translates raw on-chain transaction data into SQL databases such that the information can be requested using SQL queries. Custom vizualizations (charts, graphs, …) and dashboards can be created from those queries which can then be embedded into other websites.\nAdditional benefit of the platform is that there exists an active community of members who can create dashboards for which both the visualizations and the SQL queries are publicly avaliable. This enables them to build upon on another’s work, making a powerfull snowball effect.\nThere exist, however, two drawbacks to the platform:\n\nonly the platfrom itself can perfom the parsing of smart contracts (users can request a contract to be parsed)\ndoesn’t provide an API (though paid users can export results as a CSV file)"
  },
  {
    "objectID": "documents/research/posts/ERFC-146.hugo.html#nft-specific",
    "href": "documents/research/posts/ERFC-146.hugo.html#nft-specific",
    "title": "Data Analytics in Blockchain",
    "section": "NFT specific",
    "text": "NFT specific\n\nTools\n\nMarket Overview/Trends\nContains information about the whole NFT market and specific marketplaces, such as:\n\nnumber of distinct users (minters/buyers/sellers)\ntrading volume\naverage price of all NFTs sold\nfloor price (taking into account all NFTs listed)\ntrending collections\n\nThese values are then used to analyze the percentage share of a marketplace compared to the whole market which is useful to determine the top marketplaces and capture the moment when there is a drastic shift in the leaderboard.\nThe tool also helps in discovering new trending NFT collections and on which marketplace is the most of the trading activity for that collection happening thus answering the question where to go to when considering to invest in it.\n\n\nCollection Breakdown\nContains information tied to a specific collection and involves:\n\nbasic information (number of distinct holders, average price, volume, price range, number of trades…)\nbalance changes (how many NFTs were bought/sold/minted by an address)\nrarity stats - what traits are the rarest and thus more valuable\nrecent mints/trades\nsimilar collections\n\nThis data can be used to determine whether the majority of NFTs from the collection are held by few addresses which is a bad position for other holders as those addresses can quickly unload the NFTs, selling them at lower prices and so driving the floor price for the entire collection down.\nIt can also be used to assess the confidence of NFT holders in the collection by seeing if long term holders are suddenly started selling or if the collection is gaining momentum (for example, a lot of trades by different addresses in a short period of time).\nAll of this can be used by the users to develop unique NFT trading strategies, making this tool extremely informative.\n\n\nNFT Breakdown\nThis tool focuses on a specific NFT from a collection and displays\n\nhistory of trades - changes in ownership and price\nsimilar NFTs (based on traits)\n\nOne of the obvious use case is tracking the price movement of that NFT but another one is tracking at the same time the price movement of similar NFTs and buying those that seem undervalued (ofter reffered as “sniping”).\n\n\n\nPlatforms\nAll of the previously listed tools are supported by Nansen but there are specialized alternatives that perform limited subset of those functionalities in a same/slightly different way. Some of the popular ones are icy.tools, moby.gg and NFTNerds.ai."
  },
  {
    "objectID": "documents/research/posts/ERFC-146.hugo.html#defi-specific",
    "href": "documents/research/posts/ERFC-146.hugo.html#defi-specific",
    "title": "Data Analytics in Blockchain",
    "section": "DeFi specific",
    "text": "DeFi specific\n\nTools\n\nTotal Value Locked (TVL) Tracker\nTVL s the overall value of crypto assets deposited in a specific DeFi protocol – or in DeFi protocols generally. It is often analyzed to determine the oportunities across chains and protocols. When analyzed over longer periods of time it can help in discovering new project trends.\n\n\nRecent Activity Tracker\nThis tool gathers in real time the latest transactions that happened on Decentralized Exchanges (DEXs), Lending/borrowing and Derivatives platforms. Using it, it is possible to detect and take into account large funds movement by an address that is of interest.\n\n\nStaking/Lending/Liquidity Metrics\nThese metrics revolve around the number of lenders/borrowers/stakers of a DeFi platform, their current and past balances (including deposits/withdrawals/liquidations) as well as the distribution of token holders.\n\n\n\nPlatforms\nDune Analytics is very useful in this area as the most useful information is DeFi platform specific and needs to be analyzed in a different way. For general, comparable information there are DeFi Pulse and DeFi Llama."
  },
  {
    "objectID": "documents/research/posts/ERFC-172.hugo.html",
    "href": "documents/research/posts/ERFC-172.hugo.html",
    "title": "Developing with Ape",
    "section": "",
    "text": "Introduction\nApe is a new tool for creating and exploring on Ethereum and other blockchains. This framework is written in python with a goal of onboarding more python developers to Web3 thus providing much needed inclusivity in the space.\nTheir goal is to make development smoother with their modular approach. Ape is centered around their open-source plugins written in python; some of them are:\n\nApe-hardhat - Hardhat network provider for Ape\nApe-infura - Infura provider plugins for Ethereum-based networks\nApe-solidity - Support for Solidity smart contracts\nApe-ledger - Ledger Nano S and X plugin for Ape\nApe-alchemy - Alchemy Provider plugins for Ethereum-based networks\n\nThere are over 20 plugins Ape offers. Considering the open-source nature of the project a lot of new plugins are on the way.\nCurrent version of Ape is v0.2.1 and some of the new interesting features offered are:\n\nPolygon, Binance Smart Chain, and Fantom support. Developers can build multi-chain applications with ape’s network switching feature.\nImpersonated account. This let’s the developers test their project and interact with the contract on a fork network pretending to be any account. If you want to impersonate Vitalik, Ape makes that possible.\n\nThey are also working on Ape Project Templates which should increase productivity and enhance developers’ experience when using this framework. Some of the templates Ape is currently developing are:\n\nNFT template\nToken template\nVarious other templates for airdrops,minting\nTemplates for different ERC standards\n\nApe is also set out to be the “first professional-grade smart contract development framework to support multi-chain application development, including non-EVM chains like StarkWare” .[^1]\nAnother sign that Ape is growing is that a Yearn.Finance repo has officially migrated over from Brownie to Ape.\nAs previosly mentioned, this is a new framework and we are expecting more adoption and improvements in the coming months, especially as more developers “take it for a spin”.\nAnother interesting thing is that there is a possibility of developers switching to Ape from Brownie framework, as Brownie updates have slowed down.\n\n\nGoals & Methodology\nThe goal of this research paper is to explore this new player in the smart contract frameworks market, this is an opportunity to explore a new framework that is python oriented.\nThis will be done by writing a simple smart contract, deployment script for rinkeby and a couple tests for said contract and examining the documentation and tutorials present. That way we can research the ease of use for both beginners and experienced developers, and see what is the approach to development process this open-source framework is taking.\nAs a result Web3 Tech Radar location for this framework will be suggested.\n\n\nResults & Discussion\nBeginner friendly?\nAfter initial testing of this framework and considering the state of the documentation at this stage I would recommend this framework to experienced python developers venturing into Web3. Documentation is well written, still in the works and continuosly updated with contributions from the community around this framework. Apeworx Team and Apeworx community is currently working on workshops to get developers up to speed and tutorials are in the works.\nCurrently there is little materials for newcomers. Considering this is a new open-source project this is understandable. However, for absolute beginners, going through the Brownie framework first is recomended at this stage of Ape’s development. The reason for that is abundance of tutorials, workshops and well written documentation. After Brownie, switch to Ape and its plugin system is smooth.\nPerformance:\nApe framework performs well. Smart contract was developed and deployed to Rinkeby test network using a python script without any problems. Verification of the contract on Etherscan via a python script is not yet possible but is in the works in the Etherscan plugin.\nTesting works well both locally and when using network forks, which makes exhaustive testing possible. Currently Ape doesn’t include built-in smart contract fuzz-testing tool.\nCurrently the speed of the framework is satisfactory and more improvements are on the way.\nPlugins:\nOpen-source modular plugins are definitely the highlight of this framework. It allows developers to easily install and remove the functionality they need in their development process and I could see this being a way to onboard new developers from the python world and a way to incentivize developers to develop their own plugins. Some of the interesting Ape plugins are:\nape-tokens is an interesting plugin which allows developers to get token contracts without putting in the addresses themselves.\nExample:\nfrom ape_tokens import tokens\n\nlink = tokens[\"LINK\"]\n\nprint(link.address)\nThis will print out the eth adress of the LINK token. “link” can now be used in various python scripts, be it testing or development.\nape-ledger is a plugin for Ape Framework which integrates with Ledger devices to load and create accounts, sign messages, and sign transactions.\nRequirements\n\nhave the Ledger USB device connected\nhave the Ledger USB device unlocked (by entering the passcode)\nhave the Ethereum app open.\n\nLedger accounts have the following capabilities in ape:\n\nCan sign transactions\nCan sign messages using the default EIP-191 specification\nCan sign messages using the EIP-712 specification\n\nape-trezor is a plugin for Ape Framework which integrates Trezorlib ethereum.py to load and create accounts, sign messages, and sign transactions.\nYou can load the account like any other account in Ape console and then use it to sign transactions like this:\nape trezor sign-message [YOUR TREZOR ALIAS] \"hello world\"\nape trezor verify \"hello world\nThe output of verify should be the same address as the account $account_name.\nApe Polygon Ecosystem Plugin - Ecosystem Plugin for Polygon support in Ape\nApe Fantom Ecosystem Plugin - Ecosystem Plugin for Fantom support in Ape\nape-addressbook is plugin that allows tracking addresses and contracts in projects and globally. This is an interesting way to improve developers user experience and is currently in development.\n…And many more.\n\n\nConclusion\nTech Radar Proposal:\nRecommended location is the Assess ring at this stage. The reason for that is the shere novelty of this framework. However the development team is great, community is growing and we are seeing new projects emerging using Ape. This framework is with its simplicity aiming to become the industry standard in Ethereum development for python developers and is on a great way to do so.\n\n\nBibliography"
  },
  {
    "objectID": "documents/research/posts/ERFC-259.hugo.html",
    "href": "documents/research/posts/ERFC-259.hugo.html",
    "title": "Approaches to Testing Of Smart Contracts",
    "section": "",
    "text": "Smart Contracts are applications built on blockchain that, once deployed, cannot be altered or updated. With that in mind, their testing is crucial, even more so than in traditional software development.\nSeveral different techniques exist in the testing of Smart Contracts, and it is up to the developers to choose when a technique should be used with the goal of creating tests that will perform sufficient validation. This is a non-standardized, individualistic approach as there is no established methodology for doing this, and the developers’ skill plays an essential part in it.\nThis research focuses on testing techniques that are most widely used and showcases them in order to give a sense of what kind of testing is possible and where it makes sense.\nIn testing, there is always the question of whether the collection of tests (test suite) covers all of the cases - “Who will guard the guards themselves?”*.\nTo answer this question, to a certain degree, the paper elaborates on evaluation tools that indicate whether or not more tests should be written or if there’s a case that is overlooked.\nAs the techniques and tools mature and increase in complexity, we may see the introduction of standardized methodologies that provide a thinking framework on how code should be written and/or tested, as well as a separation of roles between developers and testers.\n*Quis custodiet ipsos custodes? - a Latin phrase found in the work of the Roman poet Juvenal (Satire VI, lines 347–348)"
  },
  {
    "objectID": "documents/research/posts/ERFC-259.hugo.html#contract-example",
    "href": "documents/research/posts/ERFC-259.hugo.html#contract-example",
    "title": "Approaches to Testing Of Smart Contracts",
    "section": "Contract example",
    "text": "Contract example\nThe example contract DummyToken can wrap/unwrap Ether through deposit and withdraw functions and transfer the tokens between two addresses using a function of the same name - transfer. During the execution of those functions, a corresponding event is emitted.\nThe implementation details are purposefully hidden with the intention of starting the thinking process of how those functions should behave both when called in intended and non-intended ways.\n/**\n * @dev Implementation of the Dummy Token.\n */\ncontract DummyToken {\n\n    /**\n     * @dev Emitted when tokens are moved from one account (`from`) to\n     * another (`to`) of the `value` amount.\n     */\n    event Transfer(address indexed from, address indexed to, uint value);\n\n    /**\n     * @dev Emitted when a new Deposit is made\n     */\n    event Deposit(address indexed to, uint value);\n\n    /**\n     * @dev Emitted when new Withdrawal is made\n     */\n    event Withdrawal(address indexed to, uint value);\n\n    ...\n\n    /**\n     * @dev Mints `value` tokens to `msg.sender` that corresponds to `msg.value` .\n     *\n     * Returns a boolean value indicating whether the operation succeeded.\n     *\n     * Emits a {Deposit} event.\n     */\n    function deposit () public payable returns (bool) {...}\n\n    /**\n     * @dev Burns `value` tokens if the `msg.sender` balance can cover it.\n     *\n     * Returns a boolean value indicating whether the operation succeeded.\n     *\n     * Emits a {Withdraw} event.\n     */\n    function withdraw (uint value) public returns (bool) {...}\n\n    /**\n     * @dev Moves `value` tokens from the caller's account to `to`.\n     *\n     * Returns a boolean value indicating whether the operation succeeded.\n     *\n     * Emits a {Transfer} event.\n     */\n    function transfer (address to, uint value) public returns (bool) {...}\n\n    /**\n     * @dev Returns the number of tokens owned by `account`.\n     */\n    function balanceOf(address account) public view returns (uint) {...}\n\n    /**\n     * @dev Returns the total amount of tokens in existence.\n     */\n    function totalSupply() public view returns (uint) {...}\n\n}\n\nSpecification of the transfer function\nTo understand the forms of testing that can be performed, let us write a specification on what one of the functions needs to accomplish, namely the transfer function.\n\nHigh level specification of the transfer function\nThis function transfers the amount of tokens (value) from the msg.sender‘s balance to the to address’ balance.\n\n\nLow level specification of the transfer function\n\nAfter successful transfer, the balance of to address is incremented by the value amount and the msg.sender’s balance is decremented by it.\nIf the msg.sender’s balance is smaller than the value, the transaction should revert with the \"Transfer amount exceeds balance\" message.\nIf the transfer is successful, the function returns true - otherwise, it returns false\nIf the transfer is successful, the Transfer event should be emitted with the corresponding fields:\n\nfrom : msg.sender\nto : value of the to argument\nvalue : value of the value argument"
  },
  {
    "objectID": "documents/research/posts/ERFC-259.hugo.html#forms-of-testing",
    "href": "documents/research/posts/ERFC-259.hugo.html#forms-of-testing",
    "title": "Approaches to Testing Of Smart Contracts",
    "section": "Forms of testing",
    "text": "Forms of testing\n\nUnit Testing\nUnit Testing relies on keeping the tests separate from each other and as simple as possible, with each unit test being responsible for testing a single module(“unit”).\nThese tests follow a common pattern referred to as Arrange-Act-Assert(AAA). First, the “arrangments” are made to put the system in the desired state, then the “act” is performed (function call most often) that leads the system to the next state, after which that state is “asserted” for correctness.\nIn an individual unit test, most often, only one assertion is made, which increases the number of tests. This, however, has the benefits of having a clear indication of why a test has failed and increasing the code readability.\nWhen thinking about unit testing the DummyToken contract, we will take only the transfer function as an example. Following is an incomplete list of test scenarios for this functionality that should serve as a starting point.\n\nTest Scenarios:\nTo form a part of a test suite, let us divide the test scenarios into two sections (generalized and edge cases) and write some examples of tests for each of them.\n\nGeneralized:\n\nValid* Transfer amount** of DummyToken from address0 to address1 where address0 != address1\n\nTests:\n\naddress0’s balance is decremented by the amount\naddress1’s balance is incremented by the amount\nbalances of other adresses has not changed\nTransfer event was emitted with the corresponding fields\n\n\nInvalid* Transfer amount of DummyToken from address0 to address1 where address0 != address1\n\nTests:\n\ntransaction was reverted with the right message (“Transfer amount exceeds balance”)\n\n\n…\n\nEdge Cases:\n\nValid/Invalid Transfer amount of DummyToken from address0 to address1 where address0 == address1\nValid Transfer 0/1 of DummyToken from address0 to address1 where address0 != address1\nValid Transfer 0/1 of DummyToken from address0 to address1 where address0 == address1\n…\n\n\n*Term “Valid/Invalid” refers to the fact of whether this transfer should be possible (due to balance amounts).\n**amount can be any uint (including the value being greater than the total supply)\nWe can notice that for the first scenario of the generalized section, four tests need to be written, with each of them being a unit test that checks a specific thing (i.e., the sender’s balance has been decremented by the right amount).\nIt is important to note that a “Property-based Testing” technique was used in the above list, which is a form of an automated process called “fuzzing” that is used to find bugs by feeding randomized data into the system. This technique focuses on the “properties” of the code that should always hold. The tests are not concerned with the actual values of amount, address0, and address1, which can be anything in the allowed range of possibilities. Rather, they aim to say whether the properties around the balances hold in the test scenario - i.e., if an account transfers some tokens to another account, only those two balances should be affected.\n\n\n\nIntegration Testing\nIn the context of Smart Contract testing, integration tests validate interactions between different components of a single contract or across multiple different contracts and are more complex when compared to unit tests.\nOne form of integration testing is Stateful testing, an advanced method of property-based testing, where a single test is defined by:\n\nan initial state that can, after deployment, be kept as it is or be created by some fixed sequence of actions\nactions - transactions that lead to a transition of state\ninvariants which are properties that should always hold true\n\nStarting from the initial state, a randomized sequence of actions is carried out, where after each action, all of the invariants are tested.\nFor example, when writing a “stateful” test for the DummyToken contract :\n\ninitial state can be created such that each test account calls a deposit function with a random amount of Ether provided\nactions can be kept basic (deposit , transfer and withdraw) or more complex (nested - i.e. one action can be [deposit, withdraw, withdraw,…])\none of the invariants can be that sum of account balances of the DummyToken must always be equal to the Ether amount that the contract holds\n\nBesides being more complex, integration tests require more resources and execution time.\n\n\nStatic (code) analysis\nBoth of the above-mentioned forms of testing are considered a type of “dynamic code analysis” that searches for bugs during the execution of the program, and they are the main topic of this research.\nIt is worth mentioning its counterpart - Static code analysis or just Static analysis, which is a debugging method that examines the source code before a program is run. This is done by analyzing the code against a set of detection rules that include: timestamp dependency, integer underflow/overflow, re-entrancy issues, use of tx.origin instead of msg.sender, … It remains up to the developer to implement or reject the recommendations of these rules.\n\n\nGeneral Considerations\nSmart Contracts operate in an extremely hostile environment, and this should always be taken into account. During development and testing, the most valuable guiding principle is that everything that can go wrong will eventually go wrong, especially if someone stands to benefit from it.\nA set of principles can be adopted to make the functionality of a contract and its complexity more manageable as to reduce the probability of bugs or exploits happening. Some of those include that:\n\ncode should be modularized and kept simple (KISS and DRY principles*** should be followed)\nclarity should be preferred over performance (if possible)\nlatest versions of battle-tested tools and frameworks should be used\nthe blockchain characteristics should be considered\nthe latest security developments should always be incorporated\ndeployment and testing should be done on Testnet before moving to Mainnet\n\n*** KISS (Keep It Simple, Stupid) and DRY (Don’t Repeat Yourself) are software programming principles where KISS states that the most simple solutions often work the best, while DRY follows the reasoning that same/similar code sections should not be replicated across the code base."
  },
  {
    "objectID": "documents/research/posts/ERFC-259.hugo.html#evaluation",
    "href": "documents/research/posts/ERFC-259.hugo.html#evaluation",
    "title": "Approaches to Testing Of Smart Contracts",
    "section": "Evaluation",
    "text": "Evaluation\nThe purpose of tests is to verify the correctness of the implementation, which poses the question of whether or not the test suite is sufficient for the implementation requirements. To address this and to have a sanity check for a developer’s thought process, evaluation tools have been created.\n\nCode Coverage\nThe term code coverage refers to the set of evaluation metrics that are used to determine how much of the program has been tested by the test suite - how many functions have been called, how many statements have been executed, etc.\nFor example, in the code below, to reach a 100% coverage for the function fcn, at least one of the tests would need to call with parameters that pass all of the three if statements (i.e. fcn(32, 300, 500)).\nfunction fcn (uint a, uint b, uint c) {\n\n    if(a < 100) {\n        if(b > 200) {\n            if(c > 300 && c < 600) {\n                ...\n            }\n        }\n    }\n}\nWhile a high coverage doesn’t generally equal good tests, low coverage helps in identifying gaps in the test suite that can be filled by adding new, carefully designed tests.\n\nCoverage-guided Fuzzing\nDuring testing, feeding purely randomized values is often wasteful and time-consuming. In the example above, parameter a is of type uint, which means it can hold any value in the range [0, 2**64-1], but the condition a < 100 will hold true only for a small portion of time.\nCoverage-guided Fuzzing takes into account code coverage information for each random value it tries, and if that value executes a new code, it is put in the set of promising values. For example, if a = 32 has been generated, fuzzer will keep note of it, as it opens the door to new code - it can then keep a fixed and randomize parameters b and c, thus reducing the search space.\n\n\n\nMutation Testing (Mutation analysis)\nMutation testing is a technique used to evaluate the effectiveness of a test suite by introducing minor modifications, called “mutations”, in the code, thus producing “mutants”.\nThese modifications are performed using a fixed set of mutation operators like operand replacement, expression modification, statement modification, etc.\nListed below is an example of an original code as well as one potential mutant that can be generated from it.\nOriginal Code\nfunction fcn (uint a, uint b) returns (bool) {\n\n    if(a > b){\n\n        return true;\n    }\n\n    return false;\n}\nMutant #1 - produced by using an expression modification operator (replaced > with <)\nfunction fcn (uint a, uint b) returns (bool) {\n\n    if(a < b){\n\n        return true;\n    }\n\n    return false;\n}\nThese mutants are then tested, and, ideally, all of them would need to get caught (killed) by at least one of the tests. The percentage of killed mutants is referred to as the mutation score.\nThese techniques can give insight into what are the tests missing and where are the blind spots as well as what tests are rarely killing mutants - both of which is valuable when improving the test suite.\nIf a mutant cannot be compiled (i.e., mutation produced a syntax error), it is called stillborn and is not taken into consideration. Sometimes, mutants can have the same behavior as the original code, in which case, they are referred to as equivalent mutants. These mutants will not get killed by the test suite and will lower the mutation score. Detecting and taking them out of consideration is not an easy task and is the biggest obstacle to the widespread application of mutation testing."
  },
  {
    "objectID": "documents/research/posts/ERFC-40.hugo.html",
    "href": "documents/research/posts/ERFC-40.hugo.html",
    "title": "Access NFT Tools - Overview And Space For Improvements",
    "section": "",
    "text": "NFT Access tokens are primarily used to join private Discord and Telegram communities. Still, a significantly larger opportunity lies in many online and offline communities and events that might utilize this technology.1\nOn the other side, there is a privacy concern with some tools that might be a deal-breaker. More precisely, tools used to identify whether a specific user holds an NFT might connect a user’s identity (e.g., Discord/Telegram name, SM account, etc.) with their wallet address and the amount of money they possess. Yet, we believe that ZK technology can be used to bridge that gap between privacy and utility.\nHowever, before we deep-dive into creating a tech spec and building a PoC solution, we wanted to research whether our presumptions are correct, whether current collusion works as we assumed and whether there is space for technical improvement (innovation)."
  },
  {
    "objectID": "documents/research/posts/ERFC-40.hugo.html#goals",
    "href": "documents/research/posts/ERFC-40.hugo.html#goals",
    "title": "Access NFT Tools - Overview And Space For Improvements",
    "section": "Goals",
    "text": "Goals\n\nIdentifying content gating tools (that works).\nUnderstanding how they work.\nRecognizing use cases that they are used for, the problems they aim to solve.\n\nIt is essential to overview current solutions in the field we aim to dive into. Both, short-term, for this research and experiment, ut also long-term, for being able to actively track all the innovation in this field from now on.\n\nVerify our presumption about how current solutions are used to gate Telegram and Discord communities: Through using centralized services and storing the user’s private information (wallet address and username), existing solutions periodically verify the user’s possession of specific NFT.\n\nThe main goal of this research is to verify our presumption that there is privacy concern and space for tech improvement within the Access NFT tool niche. If it is accurate, and if the current solution keeps track and connection between private users’ data, we can conclude that there are reasons to move forward with this idea and start working on technical specifications and research how it can be developed."
  },
  {
    "objectID": "documents/research/posts/ERFC-40.hugo.html#methodology",
    "href": "documents/research/posts/ERFC-40.hugo.html#methodology",
    "title": "Access NFT Tools - Overview And Space For Improvements",
    "section": "Methodology",
    "text": "Methodology\n\nTrying out and using existing solutions. We aimed to try each solution available on the market as we believe it is the only way to understand how it works and what problems it solves.\nConsult and discuss with team and community members via contact forms or community servers We want to confirm that our understanding of how a specific tool works and what lies under the hood is correct. Hence, the best way is to receive confirmation from the people building and using it, especially when those tools are not open-sourced.\nAnalyze other reviews There are blog posts, websites, and apps that have already analyzed and gathered information about the current state of Access NFT tools. Thus, we do not need to do all of this work again, as we can benefit from their insights and overviews. Still, this does not mean that we will take it for granted. We want to do our research as well.\n\n*We were not looking under the hood (going through the code). Most of the solutions are not open-sourced. Also, we could understand how they work by consulting with the community and team members behind these solutions."
  },
  {
    "objectID": "documents/research/posts/ERFC-40.hugo.html#collab.land",
    "href": "documents/research/posts/ERFC-40.hugo.html#collab.land",
    "title": "Access NFT Tools - Overview And Space For Improvements",
    "section": "Collab.Land",
    "text": "Collab.Land\nCollab.Land is a sovereign ruler and tool that is used by almost all Discord and Telegram Access communities.\nThe Collab.Land documentation is scarce and it is focused on explaining how to connect their bot rather than explaining how it works. Also, we could not find any other useful information and as their solution was not open-sourced, we reached out directly to them looking for answers that will help us test our hypothesis.\nBased on the answers we received, Collab.Land does store connections between users personal information and their wallet address.\nHowever, before we could conclude that our hypothesis was correct, we had to research Swordy-bot."
  },
  {
    "objectID": "documents/research/posts/ERFC-40.hugo.html#swordy-bot",
    "href": "documents/research/posts/ERFC-40.hugo.html#swordy-bot",
    "title": "Access NFT Tools - Overview And Space For Improvements",
    "section": "Swordy Bot",
    "text": "Swordy Bot\nSwordy-bot is a Discord bot used to verify and grant access to a specific Discord channel (server) if a user has required token(s).\nCompared to Collab.Land, Swordy-bot is built on top of decentralized Unlock Protocol. However, as well as Collab.Land it does store and keep a connection between user’s personal information (Discord id) and wallet address in their centralized database. We came to that conclusion (again) based on the information provided by their team members. .\nAnother common thing with Collab.Land is that Sowrdy-bot is not open-sourced as well.\nWorth mentioning is that, even though Collab.Land is sovereign ruler, Swordy-bot is used as a gate keeper by more than 100 Discord communities.\nFinally, we could confirm that our presumption was correct based on the fact that both of these solutions do store and keep track of the user’s wallet address and username centrally.\nThereafter we went a step further and analyzed other solutions used for content gating. All of them can be divided into 3 categories:\n\nProtocols\nPlatforms\nApps/Tools\n\nAlso each of them is either centralized or decentralized.\nWe have already covered Swordy-bot and Collab.land which are centralized tools."
  },
  {
    "objectID": "documents/research/posts/ERFC-40.hugo.html#unlock-protocol-decentralized-protocol",
    "href": "documents/research/posts/ERFC-40.hugo.html#unlock-protocol-decentralized-protocol",
    "title": "Access NFT Tools - Overview And Space For Improvements",
    "section": "Unlock Protocol [Decentralized Protocol]",
    "text": "Unlock Protocol [Decentralized Protocol]\nUnlock Protocol is an open-source protocol, not a centralized platform, used to create an underlying infrastructure for token gating content (communities, application, websites pages, and sections, images, videos, etc.)\nIt enables:\n\nuser (admin) to deploy a set of smart contracts (on Mainnet, xDAI, Polygon, BSC, or Optimism) and define gating details (number of keys, key price and key duration).\nregular users to purchase key (an NFT) and access content.\n\nUP is the underlying layer that allows other tools to build on top of it and utilize its functionalities. Bellow are listed use cases enabled through community-developed integrations (apps and plugins) build on top of Unlock Protocol:\n\nSwordy-bot - gating Discord communities (servers and channels).\n\nDiscourse plugin - gated content on Discourse.\nWP and Webflow plugins - gated website pages or section.\nDurap module - gating content on Durap.\nSlack plugin - gated Slack servers.\nShopify app - allows merchants to offer special memberships to their customers.\n\nPlugins and other integration tools are the ones that connect blockchain with specific apps (Discord, Slack, etc.). Hence, they are centralized solutions that monitor what’s happening on a blockchain (whether the wallet address still holds an NFT) and inform apps about that. Something like reverse oracles.\nAll in all, Unlock Protocol is a customized set of smart contracts with the following functionalities:\n\nMinting and sending NFTs (locks) to users.\nCollecting and withdrawing crypto on behalf of admin.\n\nAlso, if needed, it can support more traditional (web2) authentication methods by storing private keys on behalf of a user. Also, UP support CC payments.\nYou are right if you think that all of the above may be done without UP. However, it is much easier and faster to implement all of these functionalities by using Unlock Protocol than developing all of the smart contracts and features from scratch."
  },
  {
    "objectID": "documents/research/posts/ERFC-40.hugo.html#whale-room-centralized-platform",
    "href": "documents/research/posts/ERFC-40.hugo.html#whale-room-centralized-platform",
    "title": "Access NFT Tools - Overview And Space For Improvements",
    "section": "Whale room [Centralized Platform]",
    "text": "Whale room [Centralized Platform]\nWhale room is a centralized platform that enables users to create their chat rooms within the platform and gate them by setting up the access requirements (required tokens). Also, it supports token mining and distribution (to the community members).\nWhale room is an alternative for using Discord (or Telegram) in combination with Collab.Land (or Swordy-bot) as it offers both, chat rooms (as Telegram and Discord) and token gating functionality (as Collab.Land and Swordy-bot)."
  },
  {
    "objectID": "documents/research/posts/ERFC-40.hugo.html#mintgate-centralized-tool-platform",
    "href": "documents/research/posts/ERFC-40.hugo.html#mintgate-centralized-tool-platform",
    "title": "Access NFT Tools - Overview And Space For Improvements",
    "section": "MintGate [Centralized Tool & Platform]",
    "text": "MintGate [Centralized Tool & Platform]\nMintGate is a tool that allows users to hide content (URL) and present it only to the ones that possess specific NFT. Users can use MintGate to deploy new NFTs required to access content, but it also supports using other NFTs, minted in the other way.\nIn addition, MintGate recently created a centralized platform that offers the creation of your store and gating it with MintGate technology."
  },
  {
    "objectID": "documents/research/posts/ERFC-40.hugo.html#guild-centralized-platform",
    "href": "documents/research/posts/ERFC-40.hugo.html#guild-centralized-platform",
    "title": "Access NFT Tools - Overview And Space For Improvements",
    "section": "Guild [Centralized Platform]",
    "text": "Guild [Centralized Platform]\nIn combination with its Medusa bot, Guild gate the access to Discord private channels rather than the whole discord server. Compared to other solutions, it supports more complex requirements logic by combining the following requirement options: possession of an NFT, amount of an ERC20 token, and opportunity to whitelist wallet addresses.\nBesides this feature and difference, it offers the same options as using some of the previously mentioned bots combined with Discord or Telegram (that you also need if you want to use Guild)."
  },
  {
    "objectID": "documents/research/posts/ERFC-154.hugo.html",
    "href": "documents/research/posts/ERFC-154.hugo.html",
    "title": "Blacklisting Platform based on untransferable NFTs",
    "section": "",
    "text": "The main topic of this research is exploring the possibility and the need to create the “Authority” protocol that would handle blacklisting based on community voting and assessment. This protocol would mint the untransferable NFT to the blacklisted address. Creating this solutions (both the platform and the NFT) would not be challenging but the need for these solutions is questionable to the author. This paper explored the current blacklist cases and the possibility of creating the said NFTs. The Tether case is in the main focus since the solutions are similar and simple. The main takeaway is that creating this solution is not necessary per se, but the issue is open for discussion."
  },
  {
    "objectID": "documents/research/posts/ERFC-154.hugo.html#blacklisting",
    "href": "documents/research/posts/ERFC-154.hugo.html#blacklisting",
    "title": "Blacklisting Platform based on untransferable NFTs",
    "section": "Blacklisting",
    "text": "Blacklisting\nAs we pointed above we did a deep-dive into contracts that utilize blacklisting. In the beginning of the research we have searched for contracts that previously used blacklisting and we have come across the Tether case as it is the biggest one. There are various contracts that use the blacklist contracts but it is the exact same approach. When it comes to individual sites and addresses MetaMask already blocks sites that are known to steal funds.2\nStablecoin issuer Tether froze the ethereum address holding over $715,000 worth of USDT. This address is the address of hackers who stole $3 million on the Multichain bridge. This means that they cannot move the funds.\n\nThe question here is how did Tether manage to do that?\n\nThey managed that by importing the Blacklist contract in their main contract. We will try to explain the contract in the code-box comments below:\ncontract BlackList is Ownable, BasicToken {\n\n    // Getters to allow the same blacklist to be used also by other contracts.(including upgraded Tether) \n    function getBlackListStatus(address _maker) external constant returns (bool) {\n        return isBlackListed[_maker];\n    }\n\n    // Returns the owner of the contract address.\n    function getOwner() external constant returns (address) {\n        return owner;\n    }\n\n    // Puts the blacklisted addresses in the mapping for checking and later use.\n    mapping (address => bool) public isBlackListed;\n\n    // Self explanatory, adds the address to the isBlacklisted mapping. Only owner of the contract can call the function\n    function addBlackList (address _evilUser) public onlyOwner {\n        isBlackListed[_evilUser] = true;\n        AddedBlackList(_evilUser);\n    }\n\n    // Removes the address from the blacklist and \"unfreezes the assets\". Only owner of the contract can call the function.\n    function removeBlackList (address _clearedUser) public onlyOwner {\n        isBlackListed[_clearedUser] = false;\n        RemovedBlackList(_clearedUser);\n    }\n\n    // Destroys the funds of the blacklisted address and reduces the total suply by the same amount. Only owner of the contract can call this function.\n    function destroyBlackFunds (address _blackListedUser) public onlyOwner {\n        require(isBlackListed[_blackListedUser]);\n        uint dirtyFunds = balanceOf(_blackListedUser);\n        balances[_blackListedUser] = 0;\n        _totalSupply -= dirtyFunds;\n        DestroyedBlackFunds(_blackListedUser, dirtyFunds);\n    }\n    // Simple events for transaction logs.\n    event DestroyedBlackFunds(address _blackListedUser, uint _balance);\n\n    event AddedBlackList(address _user);\n\n    event RemovedBlackList(address _user);\n\n}\nThen they simply put the require statement in all their main contract functions(except for the ones with the “Only Owner modifier”) for example:\n// Require statement above makes sure the blacklisted address can't access the function.\nfunction transferFrom(address _from, address _to, uint _value) public whenNotPaused {\n        require(!isBlackListed[_from]);\n        if (deprecated) {\n            return UpgradedStandardToken(upgradedAddress).transferFromByLegacy(msg.sender, _from, _to, _value);\n        } else {\n            return super.transferFrom(_from, _to, _value);\n        }\n    }\nThis approach to blacklisting gives the Tether the absolute control in what addresses it blacklists and for how long.\nAs we can see in the getBlacklistStatus the other contracts can use the same Blacklist to limit their usage as Tether thus leaning on their decision making.\nIn theory a blacklist protocol can be created where the voting what address to blacklist could be done by the community. The said addresses would be stored in the contract and those addresses could be whitelisted by voting again. Then, other contracts could lean on the “list” and block the addresses from using their functions. This would also make our solution the major point of centralization and considering how easy it is to set up blacklisting individually this proposes a question is the such solution needed?\nImplementing a separate blacklist functions is not challenging and any contract can include them and have the complete control in what addresses it freezes and for how long."
  },
  {
    "objectID": "documents/research/posts/ERFC-154.hugo.html#untransferable-nfts",
    "href": "documents/research/posts/ERFC-154.hugo.html#untransferable-nfts",
    "title": "Blacklisting Platform based on untransferable NFTs",
    "section": "Untransferable NFTs",
    "text": "Untransferable NFTs\nUntrasferable NFTs have been a topic of interest for a while in Web3 and there are various use cases that have been explored. Such as:\n\nidentity\nbadges\nachievements, etc\n\nVitalik Buterin in his blog post showed his interest in “soulbound NFTs”. If we want these NFTs to be truly soulbound (untrasferable) we would need to block transferability thus limiting them to only one address. When it comes to badges and achievements there are already POAPs. POAP has made the decision not to block transferability of POAPs themselves since the owners might want to change addresses and migrate assets for various reasons. There are various cases where they have been sold or even given out for free and after that sold for the highest bidder.3\nWhen it comes to creating a “soulbound NFT” we think that it is possible and that it can be done by modifying the transfer function from the ERC-721 standard.\nThe main issue here is utilizing them in the Blacklisting sense, the mint function from the ERC-721 interface can be included in the addBlacklist() function which would mint the said token to the said address. But so far we haven’t come to the use-case except for “flagging” the addresses for the world to see, so it seems unnecessary at the moment."
  },
  {
    "objectID": "documents/research/posts/ERFC-101.hugo.html",
    "href": "documents/research/posts/ERFC-101.hugo.html",
    "title": "NFT that is bound by time",
    "section": "",
    "text": "Introduction\nNFTs (Non-Fungible Tokens) reached incredible popularity in 2021 Ryan Browne1. Most of created NFTs are static. We collect it, and we hope its market value will increase. In the case of static NFT, it’s characteristic that its properties and data are immutable and recorded on the blockchain, so such NFTs can’t be changed. Otherwise, there are dynamic NFT for which it’s characteristic that properties and data are mutable, often through oracles that trigger events off-chain system or by interaction with on-chain components, for example, smart contracts other NFTs.\n\n“Dynamic NFTs are the logical next step for the NFT space, allowing unique items to evolve, and sometimes decay. This replicates the ephemeral nature of the real world and potentially gives exceptional value to a collected item because of its current state. NFTs transitioning from being ‘only’ static to being dynamic can be thought of as progressing from 2D to 3D, it enables an immense possibility of use cases.” — Adrien Berthou, Head of Crypto-Native Comms at DoinGud\n\n\n\nGoals & Methodology\nThe goals of this research:\n\nIntroduce with dynamic NFT\nSearch for project that make evolvable NFTs\nResearch how they work, find some leak\nSuggest improvements\n\nMethodology for accomplishing those goals:\n\nGetting under the hood of open source solutions\nTesting existing approach\nSolidity\n\n\n\nResults & Discussion\nFirst of all, let’s briefly review some existing projects:\n\nEtherCards\n\nEtherCards is a dynamic NFT platform that allows anyone to give a base set of traits and requirements and launch their NFT collection so that the EtherCards team can create unique NFTs. Traits can be discounts, special access rights, connections to real-world events, airdrops, upgrades, and other benefits. The ability to have traits allows the creator to maximize the value of their art. Ether Cards is an integrated ecosystem composed of two major parts. Those are the platform and the Ether Cards (an advanced membership NFT card). Anybody can use the EtherCards platform, but the owner of the EtherCards card has certain privileges. Under the hood, EtherCards integrate Chainlink VRF to provide verifiable randomness on-chain. Chainlink allows developers to read data from any external API and blockchain network and perform off-chain computation. That will enable NFTs to be connected to the external world to trigger real-world events, in a word, to be dynamic.\nEtherCards have supported and worked with LaMelo Ball, Mike Tyson, Steve Aoki and DirtyRobot. In the above collections, all NFTs metadata are stored https://client-metadata.ether.cards on the central server. Within that metadata is a link that points to NFTs image on IPFS. So, inside the smart contract, we store points to metadata JSON URIs to all variants of one NFT. Later, inside tokenURI function with the support of Chainlink return dynamically created URI of NFT, only one variant.\n\nLoopheads\n\nLoopheads is a Loopring ‘Loopring - zkRollup Layer2 for Trading and Payment’2 Moody Brains NFT collection, minted on Looprings Layer 2. There are 25 variants for one Loophead avatar(5 different backgrounds and 5 different brain sizes), which one will be displayed depending on the LRC token price using Uniswap Oracles.\nAll NFT metadata are stored on decentralized storage - IPFS, within that metadata, is a link that sends to NFT’s image on IPFS. So, inside the smart contract, we store points to metadata JSON URIs to all of the Loophead’s variations. When a Loophead NFT is accessed because Loopheads use ERC1155 standard, the Loophead NFT runs the uri function, the start point of dynamic calculation, to show the loophead avatar. The calculation is done with the support of Uniswwap V3 Oracles. That changes parts of the metadata link based on LRC price and returns only one variant.\n\nUniswap LP NFT\n\nOn Uniswap V3 liquidity provider(LP) position is represented as NFT. This NFT shows information about liquidity position. Based on the pool and your parameters selected on the liquidity providing interface. The unique NFT will be minted, representing your position in that specific pool. As the owner of this NFT, you can modify or save the position. The best part of this project is that NFT is SVG generated entirely on-chain. Because of that, it is secure as an image not rely on any other service that is not on the blockchain, and it affects the price of that NFT.\nAll liquidity parameters for NFT are stored on-chain. Interesting is that SVG generation is done inside a pure function, and it returns base64 encoded metadata from the view function.\nWhen a Uniswap V3 LP NFT is accessed because it uses the ERC721 standard, it runs the tokenURI function, which is the start point that generates SVG from liquidity parameters and returns base64 encoded metadata.\n\nAavegotchi\n\nAavegotchi is a crypto collectible game. It was developed to provide users with a new blockchain-based game powered by dynamic NFTs. Aavegotchi information such as Aavegotchi name, traits, and SVG files themselves are saved as contract calldata because it is less gas cost than store in contract storage. The fundamental element of Aavegotchi’s game is randomness. Because of that, they use Chainlink VRF. The main idea behind the game is that the more you love your Aavegotchi character, the more rewards it will give you.\nTo store SVG, we pass one or more SVG images as a string, along with the information of SVG category type(aavegotchi, collaterals, eyeShapes, wearables) and size of passed SVG images. So inside tokenURI we have all NFTs prepared to return only one determined based on real-life events.\n\n\nYou can quickly determine where your NFT is by calling the tokenURI or uri function on the contract, which returns a URI that points to metadata that shows where NFT lives. Above project for NFT storage use:\n\nCentralized server\nDecentralized storage (IPFS, Filecoin, Arweave)\nOn-chain storage\n\nThe problem with the central server is that the possibility for manipulation is vast. The server owner can change the JSON scheme of your NFT whenever he wants.\nThe problem with IPFS is that there is no defined way of data replication. It just happens depending on the relevance of the content. In addition, the IPFS node can become offline. The problem is that if the relevance of our data is minor, the bigger is chance to lose data. To resolve the issue, Filecoin and Arweave come into play. Filecoin is a solution where we pay some price to store data for a set time. The problem is that we are limited by time, so our data are not stored permanently. Arweave is a solution that incentivizes the nodes to hold data permanently by paying only one fee in the Arweave token and keeping data forever. The most significant leak here is that it all comes down to having one storage layer that is separate from the blockchain and from the NFT itself on which it is located.\nWhen it comes to on-chain storage, the SVG is scalable because it does not rely on pixels to display the image. SVG is used for vector graphics where we can describe shapes and lines mathematically. But, if we want to present a more complex image in this format, we get a massive SVG file, which will lead to a considerable gas cost. Additionally, the worth of mention is EIP-2569.\n\nEIP-2569 is an Ethereum improvement proposal to allow a smart contract to save and retrieve an SVG image. Based on that, the two methods contract must have: getTokenImageSvg(uint256) view returns (string memory) and setTokenImageSvg(uint256 tokenId, string memory imagesvg) internal. As we can see, the potential flaw of function setTokenImageSvg as input parameter accepts SVG image string, which can lead to a considerable gas cost in case of complex SVG.\n\nThere is no obstacle to the realization of any project that would require the evolution of NFT. Everything needed is that our contract overrides the tokenURI or uri function from ERC721 or ERC1155. Therefore, the precondition is that we have prepared all parameters variants for the dynamic generation of potential variants. The project specification itself decides which variants to return within it.\n  function tokenURI(uint256 tokenId) public view override(ERC721) returns (string memory) {\n    require(_exists(tokenId));\n    return generateDynamicNFT(tokenId);\n  }\n  function uri(uint256 tokenId) public view override(ERC1155) returns (string memory) {\n    require(_exists(tokenId));\n    return generateDynamicNFT(tokenId);\n  }\nInside generateDynamicNFT, the user defines how and under what conditions NFT to generate, usually using oracles.\n\n\nConclusion\nThis research has shown that it is possible to change the data and properties of NFTs, and the next evaluation in NFT is moving from static NFTs to dynamic NFTs. With dynamic NTFs, some use cases could be:\n\nAn NFT ticket that could retain a value after the event is finished can turn as a discount for a related event or, as some bonus, gifts.\nSports NFT cards can evolve, such as updating their player’s stats or having a limited edition of sports event cards if the player got a super score in a match.\nSport NFT cards that receive bonuses or losses based on wins/losses.\nArtist NFT cards that change on a daily/monthly based.\nNFTs affected by social media/real-life events.\nNFTs that affect the real world, where a user can receive a physical item in exchange for NFT.\nKata NFTs were on the users’ progress the Kata NFT will change.\nGeometric shape that change as price change\n\nThe only related problem is about on-chain storage assets. Most NFTs do not store their assets directly on the blockchain because the cost of keeping them on-chain is expensive, as every action and every byte of information we hold on the blockchain has a fee. In addition, the Ethereum blockchain is designed to keep a record of transactions and not to serve as a data warehouse. Second, on-chain geometrical arts can quickly present, but for the more complex SVG files, it is possible to use the bottom-top approach. The idea is to have one zero-based image as a base and then add traits dynamically to the base image. Where will store only characteristics on-chain, and the NFT image is created dynamically. On-chain storage reduces external dependence, increasing reliability, durability, ownership, and decentralization. Keeping assets on-chain has excellent value. Users can rely on the same guarantees of immutability they use to secure property ownership, and the value of such art is more significant. When the asset is being followed on the Ethereum, we also want that asset to be placed on the Ethereum somehow.\n\n\nBibliography\n\n\nBrowne, Ryan, ‘Trading in NFTs Spiked 21,000% to More Than $17 Billion in 2021, Report Says’, CNBC, 2022 <https://www.cnbc.com/2022/03/10/trading-in-nfts-spiked-21000percent-to-top-17-billion-in-2021-report.html> [accessed 24 March 2022]\n\n\n‘Loopring - zkRollup Layer2 for Trading and Payment’, Loopring <https://loopring.org/#/> [accessed 24 March 2022]\n\n\n\n\n\n\n\nFootnotes\n\n\n‘Trading in NFTs Spiked 21,000% to More Than $17 Billion in 2021, Report Says’, CNBC, 2022 <<https://www.cnbc.com/2022/03/10/trading-in-nfts-spiked-21000percent-to-top-17-billion-in-2021-report.html>> [accessed 24 March 2022].↩︎\nLoopring <<https://loopring.org/#/>> [accessed 24 March 2022].↩︎"
  },
  {
    "objectID": "documents/research/posts/ERFC-248.hugo.html",
    "href": "documents/research/posts/ERFC-248.hugo.html",
    "title": "[ERFC - 248] Crosschain Identity",
    "section": "",
    "text": "When we look at the current Web2 user identity solutions, we can see that they are mostly linked to centralized corporate organizations. Although convenient for the end-user, this type of online identity is not owned by the user, and the issuers (Google, Facebook, Twitter, etc.) have complete control of them. In Web3, the focus is on decentralization; however, the problem of cross-chain interoperability of identities is a big one to overcome.\nThis paper examines the current state of cross-chain identity solutions and the technology behind them to understand how they operate and tackle the previously mentioned interoperability problem. Those solutions are:\n\nLitentry - a decentralized cross-chain identity aggregator which enables linking user identities across multiple networks. Litentry collects, indexes, and distributes DIDs to blockchains in a decentralized way.\nORE - a cross-chain global identity registry where users have control over their own identity.\nAccumulate (mainnet launch planned for September 2022) - an identity-based, delegated proof-of-stake blockchain solution.\n\nIt also shortly covers Decentralized Identifiers (DIDs), Accumulate Digital Identifiers (ADIs), tests and analyzes the project’s solutions and current place in the Web3 market.\nThis paper does not cover Web3 identity solutions that are not cross-chain, as that is not the main focus of this research."
  },
  {
    "objectID": "documents/research/posts/ERFC-248.hugo.html#litentry---identity-aggregator",
    "href": "documents/research/posts/ERFC-248.hugo.html#litentry---identity-aggregator",
    "title": "[ERFC - 248] Crosschain Identity",
    "section": "Litentry - identity aggregator",
    "text": "Litentry - identity aggregator\nLitentry is a decentralized cross-chain identity aggregator which enables linking user identities across multiple networks. Litentry collects, indexes, and distributes DIDs to blockchains in a decentralized way.\nAggregation is the process of integrating a wide range of digital identities from multiple networks.\n“Decentralized identifiers (DIDs) are a new type of identifier that enables verifiable, decentralized digital identity. A DID refers to any subject (e.g., a person, organization, thing, data model, abstract entity, etc.) as determined by the controller of the DID.” DIDs, by design, allow the controller of a DID to prove control over it without requiring permission from any other party. ‘Decentralized Identifiers (DIDs) V1.0’3\nLitentry’s main selling point is its decentralized identity and user activity data aggregation infrastructure. It is built on Substrate network.\nProtocols could use identity aggregating service for collateralized lending, DeFi insurance rate, and DAO voting power calculations, preventing bots from getting airdrops and various other uses in dApps.\nMain features of Litentry:\n\nIdentity management - The primary focus of this platform are identities. Litentry provides anonymous and independent identities from applications and services used by the user.\nIdentity Staking - As well as staking tokens and earning, users can “stake” their identity and get rewards.\nDistributed storage of identity data.\nUsers do not need to create multiple accounts to use different platforms of services. They can use one identity to interact with various services.\n\nUsing Litentry, blockchain projects can “offer” special services to users based on their identity’s quantified data.\n“For example, if a new project knows that an account is a Polkadot validator, and it spends hundreds of DOTs on another Parachain for half a year, then the project could directly gift this specific user some token to start to play with, or send him/her an attractive offer of the new DeFi product, or accredit him to be a validated voter.” Litentry4\nIt features an identity matching and identity staking mechanism, which are at the very core of the Litentry model. But what do they represent exactly?\nIdentity Matching is blind matchmaking where random anonymous identities are picked from the on-chain pool of identities, and the substrate off-chain worker processes candidate identity data. The network sends the matchwinners DID as a matching opportunity back to the matching buyer (for example, dApp that wants to do an airdrop). The buyer only has access to the matchwinner’s DID; thus match buyer pays the LIT token in exchange for a matching opportunity.\nWhen it comes to identity staking, that is the process in which an identity owner sends the snapshot of their identity document and DID to the identities pool of blockchain and authorizes the read permissions to the validator node. The owner gets staking and matching rewards in the following blocks. The document is encrypted and stored on IPFS or on the on-chain key-value store. DID is stored on-chain.\nLitentry’s use cases and platforms that cover these use cases:\n\nTaskFi & Airdrops Whitelisting - Not yet implemented (Drop3 Platform)\n\nLitentry identity verification system enables Web3 projects to identify target users and filter out bots. It also has a mechanism where users need to complete the task and then get rewarded after the task is complete.\n\nSocial interaction (My Crypto Profile)\n\nUsing Litentry’s cross-chain capabilities, this platform enables users to generate proof of ownership of various accounts and create a unique identity graph that connects accounts and addresses from Ethereum, Binance, Polkadot, Kusama, Phala, Twitter, and Github into a unique Web3 identity. This identity can grant access to various dApps and Airdrops. ‘My Crypto Profile | 0x4d52f8d989796ffde311da9ad696258d5a7b3cc5’5\n\n\n\nMCP\n\n\nPicture 1: My Crypto Profile Identity Graph\n\nIdentity Data Analytics (Web3Go)\n\nWeb3 go is a multi-chain platform that uses Litentry’s DID aggregated identity data to analyze and provide insights into the activity of a particular cross-chain identity or identities.\n\nPolkadot Name system\n\nLitentry currently acts as the main registrar entry for PNS (Polkadot Name System ). Public data is indexed into the domain name with the private name reserved in Litentry’s TEE side chain. A Trusted Execution Environment (TEE) is an environment for executing code; it guarantees code and data loaded inside to be protected with respect to confidentiality and integrity. ‘Polkadot Name System’6\nLitentry products technical overview:\n\nLitentry Graph\n\nLitentry Graph serves clients with aggregated identity data from Substrate and EVM-based networks. Data is taken directly from the blockchain using APIs such as Polkadot API or using blockchain indexers.\n\n\n\nGraph\n\n\nPicture 2: Litentry Graph overview\nLitentry Graph - An Express GraphQL server using schema stitching to aggregate a collection of remote schemas and subschemas.\nSubstrate Indexer - takes the data from the Substrate Archives and transforms it into a schema designed for easy querying. Postgres is used to store the data, and a GraphQL query node is used to serve the data to the Litentry graph as a remote schema. This indexer is hosted by Litentry.\nSubstrate Archive - Is a Postgres database with a real-time feed of raw events and extrinsic data directly from the blockchain. A GraphQL query node is used to serve the data to the Substrate Indexer.\nEthereum & BSC Indedxers - Litentry uses The Graph.\nSubstrate Chain - The Substrate Chain component queries data directly from blockchain nodes via web sockets using the Polkadot API.\n\nDrop 3\n\n\n\n\nDrop3\n\n\nPicture 3: Drop3 Overview\nPallets, Ethereum Verify Bot and Web3 go Analysis are not yet implemented\nDrop3 is still in the works. However, they have implemented some tasks into the product like getting verified as a human, connecting polkadot wallet etc.\n\nGovernance mobile App - We will not go into details of this product as mobile apps are not the topic of the research.\nMy Crypto Profile\n\nIdentity Graph:\n{\n  \"version\": \"v0.1\",\n  \"did\": \"did:mcp:0xd8ebc2be207451ff9eafb3ef7fada06d64d05059\",\n  \"main\": {\n    \"address\": \"0x8ad12345c3bc8598d2f602d63e927f5995dcf5d0\",\n    \"chain\": \"ethereum\"\n  },\n  \"web2List\": [\n    {\n      \"cid\": \"bafybeid5fo6ig6ilobawudqwgsu7so5guaedgvkdwyo6k5hnvlqiqjlhaq\",\n      \"account\": \"x3\",\n      \"socialType\": \"github\",\n      \"validator\": {\n        \"name\": \"Litentry Technologies GmbH\",\n        \"validatedAt\": \"1645810190\",\n        \"signature\": \"0x0c4c08460e651c6d6949af21c5b290bed39ffcae6cfdb5ef374760758c9387f3643992be7d5b6f5373fb78540b116750471a9fbcc68e5e5ccb5330250158bfab1c\"\n      }\n    }\n  ],\n  \"web3List\": [\n    {\n      \"cid\": \"bafybeihj43osgpx3u5zo567lod25rzfxoqrbspb27v2akdwrgwugziddf4\",\n      \"proofs\": [\"bafybeidcihuc74xxgatyag2q6iiv7tjwn6vjgc4dk6jravthcryrazzkru\"],\n      \"address\": \"5DXZdKSFTEx5rE25dnxamebAoSsA4fqgGT9VPFiiouxP2xM1\",\n      \"chain\": \"polkadot\",\n      \"validator\": {\n        \"name\": \"Litentry Technologies GmbH\",\n        \"validatedAt\": \"1645810190\",\n        \"signature\": \"0x0c4c08460e651c6d6949af21c5b290bed39ffcae6cfdb5ef374760758c9387f3643992be7d5b6f5373fb78540b116750471a9fbcc68e5e5ccb5330250158bfab1c\"\n      }\n    },\n    {\n      \"cid\": \"bafybeibic6syxpgnvyp5udyrhhuosm3thbslrfm4ivaauqxtus6vbjghsm\",\n      \"proofs\": [\"bafybeibbsripq672skohuiy6ruztjr5hwclkwoswj6yilt5rkezgmn4f3u\"],\n      \"address\": \"5GgmqtSXGuh2d3LxRiYo691bL4iTYQNjLCAYNakj4xfmLjnm\",\n      \"chain\": \"polkadot\",\n      \"validator\": {\n        \"name\": \"Litentry Technologies GmbH\",\n        \"validatedAt\": \"1645810356\",\n        \"signature\": \"0x0c5eb79402646ab7390355404243d73c01cde9ccabd0c26cb1803599c44de8621f69c05848c0e254bdf5849b3ff69fd43b82f3e346cdac3175d052cedf107c3c1c\"\n      }\n    }\n  ],\n  \"createdAt\": \"1646836239\",\n  \"updatedAt\": \"1646836239\"\n}\n“Each ID graph is extracted by ID pairs. An ID pair is made of two decentralized verifiable ownership claims. Each ID pair claims the joint ownership of two accounts, it can be a pairing of two web3 addresses, or a pairing of a web3 address and a web2 account. Everyone can verify and trust the ID pair. Each ID graph is represented in its unique DID. For ID graphs that have a common crypto address, one ID graph will be merged into the other ID graphs and keep only one MCP DID.” - Litentry ‘Identity Graph’7\nToken economy:\nSuppose we look at the price of LIT token, which is $0.7227 at the time of writing. Litentry reached an all-time high of $14.79 on Feb 16, 2021. Currently, it’s down -95.11% since its record high.\nFrom the market cap of just $26,979,670 we can see that Litentry hasn’t gained much attention or traction as a solution."
  },
  {
    "objectID": "documents/research/posts/ERFC-248.hugo.html#ore-id---identity-registry",
    "href": "documents/research/posts/ERFC-248.hugo.html#ore-id---identity-registry",
    "title": "[ERFC - 248] Crosschain Identity",
    "section": "ORE ID - identity registry",
    "text": "ORE ID - identity registry\nDeveloped by Aikon, ORE is a cross-chain global identity registry where users have control over their own identity. This registry is stored on the ORE blockchain. Users can use ORE ID as a single sign-on to manage their wallets on multiple public blockchains. Single ORE ID account can be used as a wallet on multiple chains because ORE ID accounts hold public and private keypairs for ED 25519, SR25519, and SECP256K1 and SECP256R1 encryption curves.\n\n\n\nDrop3\n\n\nPicture 4: ORE ID account creation process\n“Given how sensitive this data is, we use a Trustless Signing Service that allows a user to decrypt their keys and send a signed transaction to various blockchains using ChainJS library. Both of these modules are open source, so any developer can audit the code.” - ORE network’s whitepaper\nORE ID allows users to access various dApps with a one-click sign-up experience. Users can use social logins of their choosing. ORE ID creates blockchain accounts for the end-user when they sign up and encrypts and stores the user’s private key with their chosen PIN. ORE ID accounts are currently exportable to Scatter Wallet.\nWhen developing an app that uses ORE ID accounts, developers need to register the app and the app logo in order to get their APP-ID and API-key. Documentation is well written and can be found here.\nORE ID operates differently from Litentry as it creates an account that works on multiple chains - it does not aggregate the existing addresses.\nToken economy and future of ORE ID:\nThis project, although an interesting concept, with its market cap of only $ 319,175 doesn’t look like a solution for cross-chain identities and onboarding new users. The current price is $ 0.007984, while its all-time-high was $ 0.320. After testing the ORE ID, it seems that the project has been neglected."
  },
  {
    "objectID": "documents/research/posts/ERFC-248.hugo.html#accumulate---mainnet-launch-planned-for-september-2022",
    "href": "documents/research/posts/ERFC-248.hugo.html#accumulate---mainnet-launch-planned-for-september-2022",
    "title": "[ERFC - 248] Crosschain Identity",
    "section": "Accumulate - mainnet launch planned for September 2022",
    "text": "Accumulate - mainnet launch planned for September 2022\nAccumulate is an identity-based, delegated proof-of-stake blockchain solution. It plans on creating a universal communication and audit layer for individuals, entities, and blockchains to transact with each other using their version of identifiers that adhere to W3C standards: Accumulate Digital Identifiers (ADIs).\nADIs are human-readable addresses that users choose to represent their presence on the blockchain. Using ADIs accumulate can serve as a communication and audit layer between blockchains, enabling the transfer of tokens between different chains, no matter the consensus mechanism.\nADIs are made of a collection of independent sub-chains. They are managed by:\n\nToken accounts - Issuing tokens and tracking deposits and withdrawals from a token account.\nData accounts - Tracking and organizing data.\nStaking accounts - Staking ACME tokens to participate in consensus.\nScratch accounts - Accruing data that is needed to build consensus.\n\nAccumulate Innovations:\n\nIdentity - Accumulate is centered around ADIs where each ADI defines its own state that is independent of other ADIs. Each ADI has its own state and set of accounts and chains. They can be updated independently. They are distributed over a set of Tendermint networks.\nSynthetic Transactions - Because each ADI has its state, transactions that are routed to an ADI must be processed independently of all other ADIs. Accumulate generates another transaction that performs settlements within an ADI. These transactions are called synthetic since the protocol generates them in response to the transactions initiated by the user.\n\nScratch Accounts - Accumulate provides scratch accounts, which reduce the cost of using the blockchain for consensus building. “Scratch accounts allow processes to provide cryptographic proof of validation and process without overburdening the blockchain” - Accumulate Whitepaper ‘Whitepaper - Accumulate’8\n\nIntegrations:\nAccumulate protocol supports various smart contract roll-ups. This allows Accumulate to track the state and validity of contracts on third-party chains. Using Accumulate, organizations can process smart contracts across various layer one protocols (Solana, Ethereum, Tezus).\nAccumulate also plans on integrating with Layer-0 protocols, for example, Cosmos and Polkadot. In that case, Accumulate can be utilized to manage the transferred asset under the identity (ADI) of a buyer and to continue tracking the assets across multiple chains.\nTechnical overview:\n\n\n\nAccumulate overview\n\n\nPicture 5: Accumulate system overview\nIn contrast to the traditional blockchain, where architecture is centered around blocks, Accumulate is centered around accounts. Each account is treated as an independent chain and managed as a growing Merkle tree, and blocks are treated as a synchronization point for all chains in the networks.\nInside the Directory Network and Block Validator Networks is the interconnected network of chains responsible for collecting signatures, communicating with each other, and anchoring roots to other blockchains.\nThese chains are:\n\nSignature chain, which collects signatures for a period of 2 weeks\nMain chain which records transactions in the origin account and accounts that are modified by the transactions\nSynthetic Transaction chain, which is used to store cryptographic proof that a synthetic transaction was actually produced by a particular BVN.\nBinary Patricia Trie, which collects hashes of the current state and history of accounts in BVN and DN.\nRoot Anchor Chain - collects an anchor once per block from every account and system chain updated during the block.\nIntermediate Anchor Chain: Within the Directory Network, this chain collects anchors from the Root Anchor chain of every Block Validator Network once per block.\n\nAs our topic is identity we will focus more on the account and identity architecture of Accumulate, which is actually at the very core of this protocol:\nAccumulate supports these accounts:\n\nLite Token Account - Traditional address whose URL contains a public key hash and human-readable suffix denoting the token or a data type held by the account. When tokens are sent to this account, the account is created if it doesn’t exist. Users can create a key and have a trusted party send tokens to their URL.\nLite Data Account - It is used for collaboration with Factom protocol. Lite Data Accounts are similar to token accounts, but they are limited to writing data.\nAccumulate Digital Identifier (ADI) - Primary unit of organization within Accumulate. ADIs can issue their tokens.\nKey book and Key Page - Belongs to an ADI and is used for key management.\nADI Token and Data Accounts are explained at the beginning of the topic\n\nIdentity architecture:\nUsers can participate in the network through ADIs and Lite Accounts. ADIs give users access to smart contracts, off-chain consensus building, and dynamic key management. Lite Token and Data Accounts are just a “lite” version of ADIs.\nADIs can only be created through the spending of Credits issued through the Accumulate protocol. Users can also use their ADI to sponsor the creation of other ADIs for themselves or others. These identities can govern token issuance, off-chain consensus building, and multisig signatures.\n“ADI Data and Token Account URLs have the general format acc://<ADI>/<directory>/<account> where the prefix acc : // specifies the Accumulate blockchain, ADI specifies the toplevel identity in control of the URL, directory specifies a particular type of account, and account specifies data or tokens.” - Accumulate Whitepaper.\nWe will not venture deeper into the protocol architecture as it is reasonably complex and goes well beyond this research topic. The main takeaway is that it aims to create cross-chain transaction-compatible accounts using ADIs. The details of a way how this will work in practice are not covered in the whitepaper.\nThis project is still in development, which means the practical implementations of the concepts presented in the whitepaper are still under the big question mark sign."
  },
  {
    "objectID": "documents/research/index.html",
    "href": "documents/research/index.html",
    "title": "Research",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n         \n          File Name\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJun 28, 2022\n\n\nAleksandar Damjanovic\n\n\n16 min\n\n\nERFC-246.hugo.md\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJun 19, 2022\n\n\nMilos Bojinovic\n\n\n14 min\n\n\nERFC-315.hugo.md\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJun 14, 2022\n\n\nAleksandar Damjanovic\n\n\n18 min\n\n\nERFC-248.hugo.md\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJun 5, 2022\n\n\nAleksandar Veljković\n\n\n11 min\n\n\nERFC-270.hugo.md\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJun 4, 2022\n\n\nMilos Bojinovic\n\n\n15 min\n\n\nERFC-259.hugo.md\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMay 31, 2022\n\n\nAleksandar Damjanovic\n\n\n15 min\n\n\nERFC-261.hugo.md\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMay 12, 2022\n\n\nAleksandar Damjanovic\n\n\n9 min\n\n\nERFC-171.hugo.md\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMay 7, 2022\n\n\nMilos Bojinovic\n\n\n10 min\n\n\nERFC-146.hugo.md\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 30, 2022\n\n\nAleksandar Damjanovic\n\n\n4 min\n\n\nERFC-172.hugo.md\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 6, 2022\n\n\nAleksandar Veljković\n\n\n25 min\n\n\nERFC-147.hugo.md\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMar 31, 2022\n\n\nAleksandar Damjanovic\n\n\n8 min\n\n\nERFC-154.hugo.md\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMar 14, 2022\n\n\nMarija Mijailovic\n\n\n9 min\n\n\nERFC-101.hugo.md\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMar 12, 2022\n\n\nMilos Bojinovic\n\n\n15 min\n\n\nERFC-90.hugo.md\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMar 9, 2022\n\n\nAleksandar Damjanovic\n\n\n38 min\n\n\nERFC-91.hugo.md\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMar 8, 2022\n\n\nAndrej\n\n\n5 min\n\n\nERFC-105.hugo.md\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMar 4, 2022\n\n\nAndrej\n\n\n13 min\n\n\nERFC-39.hugo.md\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMar 3, 2022\n\n\nAleksandar Veljković\n\n\n10 min\n\n\nERFC-103.hugo.md\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 25, 2022\n\n\nAleksandar Damjanovic\n\n\n19 min\n\n\nERFC-38.hugo.md\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 24, 2022\n\n\nMilos Bojinovic\n\n\n32 min\n\n\nERFC-37.hugo.md\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 24, 2022\n\n\nAleksandar Veljković\n\n\n6 min\n\n\nERFC-57.hugo.md\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 21, 2022\n\n\nMilos Novitovic\n\n\n10 min\n\n\nERFC-40.hugo.md\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 17, 2022\n\n\nMarija Mijailovic\n\n\n14 min\n\n\nERFC-42.hugo.md\n\n\n\n\n\n\nNo matching items"
  }
]